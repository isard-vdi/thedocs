{
    "docs": [
        {
            "location": "/", 
            "text": "Isard\nVDI\n Tech \n Prac info\n\n\nAs we deployed IsardVDI platform we build and configured our own storage to get maximum performance at a low price.\n\n\nIn this repo you'll find our experience in virtualization, storage, networking and clusters.\n\n\nAuthors\n\n\n\n\nJosep Maria Vi\u00f1olas Auquer\n\n\nAlberto Larraz Dalmases\n\n\nN\u00e9fix Estrada Campa\u00f1\u00e1\n\n\n\n\nAcknowledgements\n\n\n\n\nInstitut Escola del Treball de Barcelona\n\n\n\n\nVideos \n conferences\n\n\n\n\nRedIRIS 2017\n\n\nClaves para la optimizaci\u00f3n de recursos sobre escritorios virtuales con hipervisores KVM/Linux sobre Isard VDI\n\n\n\n\n\n\nBarcelona Free Software 2017\n\n\nIsard VDI: Orchestration and deployment of virtual desktops with KVM and Linux\n\n\n\n\n\n\nFosdem 2018\n\n\nKeys to deploy affordable virtual desktops with IsardVDI\n\n\n\n\n\n\nRedIris 2018\n\n\nDRBD9 Y PACEMAKER: ALMACENAMIENTO DE ALTO RENDIMIENTO Y ALTA DISPONIBILIDAD CON SOFTWARE LIBRE\n\n\nDESPLIEGUE RA\u0301PIDO DE INFRAESTRUCTURAS DE ESCRITORIOS VIRTUALES CON DOCKER-COMPOSE E ISARDVDI\n\n\nESCRITORIOS VIRTUALES CON SOFTWARE 3D: MXGPU EN HYPERVISORES LINUX KVM\n\n\n\n\n\n\n\n\nSupport/Contact\n\n\nPlease send us an email to \ninfo@isardvdi.com\n if you have any questions", 
            "title": "Introduction"
        }, 
        {
            "location": "/#isardvdi-tech-prac-info", 
            "text": "As we deployed IsardVDI platform we build and configured our own storage to get maximum performance at a low price.  In this repo you'll find our experience in virtualization, storage, networking and clusters.", 
            "title": "IsardVDI Tech &amp; Prac info"
        }, 
        {
            "location": "/#authors", 
            "text": "Josep Maria Vi\u00f1olas Auquer  Alberto Larraz Dalmases  N\u00e9fix Estrada Campa\u00f1\u00e1", 
            "title": "Authors"
        }, 
        {
            "location": "/#acknowledgements", 
            "text": "Institut Escola del Treball de Barcelona", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/#videos-conferences", 
            "text": "RedIRIS 2017  Claves para la optimizaci\u00f3n de recursos sobre escritorios virtuales con hipervisores KVM/Linux sobre Isard VDI    Barcelona Free Software 2017  Isard VDI: Orchestration and deployment of virtual desktops with KVM and Linux    Fosdem 2018  Keys to deploy affordable virtual desktops with IsardVDI    RedIris 2018  DRBD9 Y PACEMAKER: ALMACENAMIENTO DE ALTO RENDIMIENTO Y ALTA DISPONIBILIDAD CON SOFTWARE LIBRE  DESPLIEGUE RA\u0301PIDO DE INFRAESTRUCTURAS DE ESCRITORIOS VIRTUALES CON DOCKER-COMPOSE E ISARDVDI  ESCRITORIOS VIRTUALES CON SOFTWARE 3D: MXGPU EN HYPERVISORES LINUX KVM", 
            "title": "Videos &amp; conferences"
        }, 
        {
            "location": "/#supportcontact", 
            "text": "Please send us an email to  info@isardvdi.com  if you have any questions", 
            "title": "Support/Contact"
        }, 
        {
            "location": "/storage/concepts/", 
            "text": "Disk utilities\n\n\nPartition alignment\n\n\nUnaligned partitions and disk sectors will result in two disk writes for an OS write. \nThat will dramatically reduce disk io throughput. To avoid that you can use parted to\ncreate and align partitions.\n\n\nFirst we should get the optimal sector where to start and aligned partition with disk\nsectors.\n\n\ninitial sector=(optimal_io_size+alignment_offset)/physical_block_size\n\n\n\n\n\nAnd to get the parameters for sdd disk:\n\n\ncat /sys/block/sdd/queue/optimal_io_size\n\n\n\n\n\ncat /sys/block/sdd/alignment_offset\n\n\n\n\n\ncat /sys/block/sdd/queue/physical_block_size", 
            "title": "Concepts"
        }, 
        {
            "location": "/storage/concepts/#disk-utilities", 
            "text": "", 
            "title": "Disk utilities"
        }, 
        {
            "location": "/storage/concepts/#partition-alignment", 
            "text": "Unaligned partitions and disk sectors will result in two disk writes for an OS write. \nThat will dramatically reduce disk io throughput. To avoid that you can use parted to\ncreate and align partitions.  First we should get the optimal sector where to start and aligned partition with disk\nsectors.  initial sector=(optimal_io_size+alignment_offset)/physical_block_size  And to get the parameters for sdd disk:  cat /sys/block/sdd/queue/optimal_io_size  cat /sys/block/sdd/alignment_offset  cat /sys/block/sdd/queue/physical_block_size", 
            "title": "Partition alignment"
        }, 
        {
            "location": "/storage/tools/", 
            "text": "", 
            "title": "Tools"
        }, 
        {
            "location": "/storage/raid/", 
            "text": "RAID\n\n\nRaid allows for data to be sparsed over multiple disks and give security to your\ndata as one (or more) disks can fail and data integrity will still remain.\n\n\nInitialize disks\n\n\nmdadm --zero-superblock /dev/sd[b-e]\n\n\n\n\n\nor\n\n\nwipefs -a /dev/sd[b-e]\n\n\n\n\n\nCreate raid\n\n\nYou can set level to get more performance or more redundancy\n\n\nThis command will create a raid level 10.\n\n\nmdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/sd[b-e] \n\n\n\n\n\nCheck raid types and benefits here: http://www.raid-calculator.com/\n\n\nSave your newly created raid configuration for next reboots:\n\n\nmdadm --detail --scan \n /etc/mdadm/mdadm.conf\n\n\n\n\n\nUtils\n\n\n\n\ncat /proc/mdstat: shows raid status and progress\n\n\n\n\nConsiderations\n\n\nmdadm software raid will trigger a complete check every sunday at 1am.\nBe aware of that as it will lower diks io speed a lot.", 
            "title": "Raid"
        }, 
        {
            "location": "/storage/raid/#raid", 
            "text": "Raid allows for data to be sparsed over multiple disks and give security to your\ndata as one (or more) disks can fail and data integrity will still remain.", 
            "title": "RAID"
        }, 
        {
            "location": "/storage/raid/#initialize-disks", 
            "text": "mdadm --zero-superblock /dev/sd[b-e]  or  wipefs -a /dev/sd[b-e]", 
            "title": "Initialize disks"
        }, 
        {
            "location": "/storage/raid/#create-raid", 
            "text": "You can set level to get more performance or more redundancy  This command will create a raid level 10.  mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/sd[b-e]   Check raid types and benefits here: http://www.raid-calculator.com/  Save your newly created raid configuration for next reboots:  mdadm --detail --scan   /etc/mdadm/mdadm.conf", 
            "title": "Create raid"
        }, 
        {
            "location": "/storage/raid/#utils", 
            "text": "cat /proc/mdstat: shows raid status and progress", 
            "title": "Utils"
        }, 
        {
            "location": "/storage/raid/#considerations", 
            "text": "mdadm software raid will trigger a complete check every sunday at 1am.\nBe aware of that as it will lower diks io speed a lot.", 
            "title": "Considerations"
        }, 
        {
            "location": "/storage/drbd/", 
            "text": "DRBD 9\n\n\nInstall required packages\n\n\napt install drbd-dkms drbd-utils python-drbdmanage\n\n\n\n\n\nCheck module install\n\n\nmodprobe drbd\nmodinfo drbd\n\n\n\n\n\nEnable and start drbd cluster manager service\n\n\nsystemctl enable drbdmanaged\nsystemctl start drbdmanaged\n\n\n\n\n\nConfigure drbd9 cluster\n\n\nCreate drbdpool volume group on desired PVs\n\n\nvgcreate drbdpool /dev/nvme0n1 /dev/mapper/disks\n\n\n\n\n\nInitialize the drbdmanage in the master node with own parameters\n\n\ndrbdmanage init \nnode-name\n \nip_drbd\n\n\n\n\n\n\nAdd volume and resources\n\n\ndrbdmanage add-volume \nvolume-name\n \ncapacity\n\n\n\n\n\n\nNote: It will create and associated resource with the same volume-name.\n\n\nDeploy resources to nodes\n\n\ndrbdmanage deploy \nresource-name\n \nn\u00ba nodes\n\n\n\n\n\n\nJust if you want to allocate volumes on desired PV\n\n\npvmove \nresource-name\n \npv\n\n\n\n\n\n\nCreate filesystem and mount\n\n\nmkfs.ext4 /dev/\nresource-name\n\nmount /dev/\nresource-name\n /mnt\n\n\n\n\n\nIn drbd9 mount action will automatically trigger a Secondary -\n Primary change to allow rw mount.\n\n\nTypes of nodes\n\n\nIn all types what it is shared is a block device, also in diskless nodes.\n\n\nControl node\n\n\n\n\nControl volume (.drbdctrl): local (could be pri/sec)\n\n\nResources: local (could be pri/sec)\n\n\n\n\ndrbdmanage add-node drbd1 192.168.0.11\n\n\n\n\n\nThe cluster startup is done with drbdmanage init and it will become also a control node.\n\n\nPure controller node\n\n\n\n\nControl volume (.drbdctrl): local (could be pri/sec)\n\n\nResources: local (could be pri/sec)\n\n\n\n\ndrbdmanage add-node --no-storage drbd2-controller 192.168.0.12\n\n\n\n\n\nIt is like a control node but as it won't have storage, will only act a a control node or a satellite.\n\n\nSatellite node\n\n\n\n\nControl volume (.drbdctrl): remote, via TCP. (could be pri/sec)\n\n\nResources: local (could be pri/sec)\n\n\n\n\ndrbdmanage add-node --satellite drbd3-satellite 192.168.0.13\n\n\n\n\n\nPure client node\n\n\n\n\nControl volume (.drbdctrl): remote, via TCP. (could be pri/sec)\n\n\nResources: remote, via TCP. (could be pri/sec)\n\n\n\n\ndrbdmanage add-node --satellite --no-storage drbd4-client 192.168.0.14\n\n\n\n\n\nUtils\n\n\n\n\ndrbdmanage nodes: Show nodes in cluster with available space\n\n\ndrbdmanage resources: show resources\n\n\ndrbdmanage volumes: Show volumes\n\n\ndrbdmanage uninit \n: Remove node from cluster. All resources on that node will be lost.\n\n\ndrbdmanage peer-disk-options --common \n: Modify disk options.\n\n\ndrbdmanage net-options --common \n: Modify net options\n\n\ndrbdsetup status: Show resource status and synchronization progress. Use --verbose for details.", 
            "title": "DRBD"
        }, 
        {
            "location": "/storage/drbd/#drbd-9", 
            "text": "", 
            "title": "DRBD 9"
        }, 
        {
            "location": "/storage/drbd/#install-required-packages", 
            "text": "apt install drbd-dkms drbd-utils python-drbdmanage  Check module install  modprobe drbd\nmodinfo drbd  Enable and start drbd cluster manager service  systemctl enable drbdmanaged\nsystemctl start drbdmanaged", 
            "title": "Install required packages"
        }, 
        {
            "location": "/storage/drbd/#configure-drbd9-cluster", 
            "text": "Create drbdpool volume group on desired PVs  vgcreate drbdpool /dev/nvme0n1 /dev/mapper/disks  Initialize the drbdmanage in the master node with own parameters  drbdmanage init  node-name   ip_drbd   Add volume and resources  drbdmanage add-volume  volume-name   capacity   Note: It will create and associated resource with the same volume-name.  Deploy resources to nodes  drbdmanage deploy  resource-name   n\u00ba nodes   Just if you want to allocate volumes on desired PV  pvmove  resource-name   pv   Create filesystem and mount  mkfs.ext4 /dev/ resource-name \nmount /dev/ resource-name  /mnt  In drbd9 mount action will automatically trigger a Secondary -  Primary change to allow rw mount.", 
            "title": "Configure drbd9 cluster"
        }, 
        {
            "location": "/storage/drbd/#types-of-nodes", 
            "text": "In all types what it is shared is a block device, also in diskless nodes.", 
            "title": "Types of nodes"
        }, 
        {
            "location": "/storage/drbd/#control-node", 
            "text": "Control volume (.drbdctrl): local (could be pri/sec)  Resources: local (could be pri/sec)   drbdmanage add-node drbd1 192.168.0.11  The cluster startup is done with drbdmanage init and it will become also a control node.", 
            "title": "Control node"
        }, 
        {
            "location": "/storage/drbd/#pure-controller-node", 
            "text": "Control volume (.drbdctrl): local (could be pri/sec)  Resources: local (could be pri/sec)   drbdmanage add-node --no-storage drbd2-controller 192.168.0.12  It is like a control node but as it won't have storage, will only act a a control node or a satellite.", 
            "title": "Pure controller node"
        }, 
        {
            "location": "/storage/drbd/#satellite-node", 
            "text": "Control volume (.drbdctrl): remote, via TCP. (could be pri/sec)  Resources: local (could be pri/sec)   drbdmanage add-node --satellite drbd3-satellite 192.168.0.13", 
            "title": "Satellite node"
        }, 
        {
            "location": "/storage/drbd/#pure-client-node", 
            "text": "Control volume (.drbdctrl): remote, via TCP. (could be pri/sec)  Resources: remote, via TCP. (could be pri/sec)   drbdmanage add-node --satellite --no-storage drbd4-client 192.168.0.14", 
            "title": "Pure client node"
        }, 
        {
            "location": "/storage/drbd/#utils", 
            "text": "drbdmanage nodes: Show nodes in cluster with available space  drbdmanage resources: show resources  drbdmanage volumes: Show volumes  drbdmanage uninit  : Remove node from cluster. All resources on that node will be lost.  drbdmanage peer-disk-options --common  : Modify disk options.  drbdmanage net-options --common  : Modify net options  drbdsetup status: Show resource status and synchronization progress. Use --verbose for details.", 
            "title": "Utils"
        }, 
        {
            "location": "/storage/cache/eio/", 
            "text": "EnhanceIO disk cache\n\n\nFirst disable kernel upgrades as it will break cache (and lost of data!)\n\n\nnano /etc/yum.conf\n    \nexclude\n=\nkernel*\n\n\n\n\n\nAfter compiling EiO cache install modules:\n\n\nmodprobe enhanceio\nmodprobe enhanceio_lru\n\n\n\n\n\nInstallation on CentOS 7.2 (kernel 3.9)\n\n\nKernel source repository:\n\n\nRef.: https://wiki.centos.org/HowTos/I_need_the_Kernel_Source\n\n\n\n\n\ncd\n /home/user\ngit clone https://github.com/stec-inc/EnhanceIO.git\nyum install kernel-devel gcc\n\n\n\n\n\nmkdir -p ~/rpmbuild/\n{\nBUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS\n}\n\n\necho\n \n%_topdir %(echo $HOME)/rpmbuild\n \n ~/.rpmmacros\nyum install rpm-build redhat-rpm-config asciidoc hmaccalc perl-ExtUtils-Embed pesign xmlto \nyum install audit-libs-devel binutils-devel elfutils-devel elfutils-libelf-devel\nyum install ncurses-devel newt-devel numactl-devel pciutils-devel python-devel zlib-devel\nyum install bison\nyum install net-tools bc\n\n\n\n\n\nDownload and install kernel source:\n\n\nBuscar la versi\u00f3 de centos: rpm --query centos-release\nrpm -i http://vault.centos.org/7.2.1511/updates/Source/SPackages/kernel-3.10.0-327.10.1.el7.src.rpm \n2\n1\n \n|\n grep -v exist\n\ncd\n ~/rpmbuild/SPECS\nrpmbuild -bp --target\n=\n$(\nuname -m\n)\n kernel.spec\n\n\n\n\n\nKernel source is in:\n\n\nrpmbuild/BUILD/kernel-3.10.0-327.10.1.el7/linux-3.10.0-327.10.1.el7.x86_64/\n\n\n\n\n\nRef.: https://wiki.centos.org/HowTos/BuildingKernelModules\n\n\n\n\n\ncd\n rpmbuild/BUILD/kernel-3.10.0-327.10.1.el7/linux-3.10.0-327.10.1.el7.x86_64/\nmake oldconfig\nmake menuconfig  \n(\nno canviem res en realitat\n)\n\nmake prepare\nmake modules_prepare\n\n\n\n\n\nWe can't build it on current folder. So, we could do an ln or build it on /usr/src/kernels:\n\n\ncd\n /usr/src/kernels/3.10.0-327.10.1.el7.x86_64/fs\n \n(\nor ln rpmbuild/BUILD/ker... to /usr/src/ker...\n)\n\nmkdir enhanceio\n\ncd\n enhanceio\ncp /home/user/EnhanceIO/Driver/enhanceio/* .\n\ncd\n /usr/src/kernels/3.10.0-327.10.1.el7.x86_64/\n\n\n\n\n\nmake -C /lib/modules/\n`\nuname -r\n`\n/build \nM\n=\nfs/enhanceio\nstrip --strip-debug fs/enhanceio/*.ko\ncp fs/enhanceio/*.ko /lib/modules/\n`\nuname -r\n`\n/extra\ndepmod -a\n\n\n\n\n\nFedora (if LINUX_VERSION_CODE \n KERNEL_VERSION(4,3,0))\n\n\nyum install kernel-devel gcc\ngit clone https://github.com/stec-inc/EnhanceIO.git\nuname -a\n\ncd\n EnhanceIO/Driver/enhanceio/\nmake\nmake install\n\n\n\n\n\nFedora (if LINUX_VERSION_CODE \n= KERNEL_VERSION(4,3,0))\n\n\nNOT TESTED\n\nWith this fork it does compile on kernels 4.3.\n\n\nhttps://github.com/elmystico/EnhanceIO\n\n\n\n\n\nNOT TESTED", 
            "title": "EiO"
        }, 
        {
            "location": "/storage/cache/eio/#enhanceio-disk-cache", 
            "text": "First disable kernel upgrades as it will break cache (and lost of data!)  nano /etc/yum.conf\n     exclude = kernel*  After compiling EiO cache install modules:  modprobe enhanceio\nmodprobe enhanceio_lru", 
            "title": "EnhanceIO disk cache"
        }, 
        {
            "location": "/storage/cache/eio/#installation-on-centos-72-kernel-39", 
            "text": "Kernel source repository:  Ref.: https://wiki.centos.org/HowTos/I_need_the_Kernel_Source  cd  /home/user\ngit clone https://github.com/stec-inc/EnhanceIO.git\nyum install kernel-devel gcc  mkdir -p ~/rpmbuild/ { BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS }  echo   %_topdir %(echo $HOME)/rpmbuild    ~/.rpmmacros\nyum install rpm-build redhat-rpm-config asciidoc hmaccalc perl-ExtUtils-Embed pesign xmlto \nyum install audit-libs-devel binutils-devel elfutils-devel elfutils-libelf-devel\nyum install ncurses-devel newt-devel numactl-devel pciutils-devel python-devel zlib-devel\nyum install bison\nyum install net-tools bc  Download and install kernel source:  Buscar la versi\u00f3 de centos: rpm --query centos-release\nrpm -i http://vault.centos.org/7.2.1511/updates/Source/SPackages/kernel-3.10.0-327.10.1.el7.src.rpm  2 1   |  grep -v exist cd  ~/rpmbuild/SPECS\nrpmbuild -bp --target = $( uname -m )  kernel.spec  Kernel source is in:  rpmbuild/BUILD/kernel-3.10.0-327.10.1.el7/linux-3.10.0-327.10.1.el7.x86_64/  Ref.: https://wiki.centos.org/HowTos/BuildingKernelModules  cd  rpmbuild/BUILD/kernel-3.10.0-327.10.1.el7/linux-3.10.0-327.10.1.el7.x86_64/\nmake oldconfig\nmake menuconfig   ( no canviem res en realitat ) \nmake prepare\nmake modules_prepare  We can't build it on current folder. So, we could do an ln or build it on /usr/src/kernels:  cd  /usr/src/kernels/3.10.0-327.10.1.el7.x86_64/fs\n  ( or ln rpmbuild/BUILD/ker... to /usr/src/ker... ) \nmkdir enhanceio cd  enhanceio\ncp /home/user/EnhanceIO/Driver/enhanceio/* . cd  /usr/src/kernels/3.10.0-327.10.1.el7.x86_64/  make -C /lib/modules/ ` uname -r ` /build  M = fs/enhanceio\nstrip --strip-debug fs/enhanceio/*.ko\ncp fs/enhanceio/*.ko /lib/modules/ ` uname -r ` /extra\ndepmod -a", 
            "title": "Installation on CentOS 7.2 (kernel 3.9)"
        }, 
        {
            "location": "/storage/cache/eio/#fedora-if-linux_version_code-kernel_version430", 
            "text": "yum install kernel-devel gcc\ngit clone https://github.com/stec-inc/EnhanceIO.git\nuname -a cd  EnhanceIO/Driver/enhanceio/\nmake\nmake install", 
            "title": "Fedora (if LINUX_VERSION_CODE &lt; KERNEL_VERSION(4,3,0))"
        }, 
        {
            "location": "/storage/cache/eio/#fedora-if-linux_version_code-kernel_version430_1", 
            "text": "NOT TESTED \nWith this fork it does compile on kernels 4.3.  https://github.com/elmystico/EnhanceIO  NOT TESTED", 
            "title": "Fedora (if LINUX_VERSION_CODE &gt;= KERNEL_VERSION(4,3,0))"
        }, 
        {
            "location": "/storage/cache/writeboost/", 
            "text": "dm-writeboost\n\n\n\n\nhttps://github.com/akiradeveloper/dm-writeboost\n\n\n\n\nThis sample configuration does not use DRBD, it's just a simple dm-writeboost setup.\n\n\nInstallation on Centos 7\n\n\nDKMS\n\n\nyum install dkms -y\n\n\n\n\n\nWriteboost DKMS module\n\n\nClone repository:\n\n\ngit clone https://github.com/akiradeveloper/dm-writeboost\ncd dm-writeboost \n make \n make install\n\n\n\n\n\nCheck that Writeboost DKMS is installed:\n\n\ndkms status\nlsmod | grep dm_writeboost\n\n\n\n\n\nWriteboost tools\n\n\ngit clone https://github.com/akiradeveloper/dm-writeboost-tools\nyum install cargo\ncargo install\ncp /root/.cargo/bin/* /usr/sbin/\n\n\n\n\n\nReboot and check module install\n\n\nCreate new Writeboost Cache\n\n\nwbcreate --reformat --read_cache_threshold=127  --writeback_threshold=80 storage /dev/md0 /dev/nvme0n1\n\n\n\n\n\n\n\nstorage: new cache device name\n\n\nmd0: slow disks\n\n\nnvme0n1: fast disks (cache device)\n\n\n\n\nCrete writeboosttab and service\n\n\nvi /etc/writeboosttab\n\n\n## dm-writeboost \ntab\n (mappings) file, see writeboosttab(5).\n##{DM target name}    {cached block device e.g. HDD}    {caching block device e.g. SSD}    [options]\n##\n## wb_hdd     /dev/disk/by-uuid/2e8260bc-024c-4252-a695-a73898c974c7     /dev/disk/by-partuuid/43372b68-3407-45fa-9b2f-61afe9c26a68    writeback_threshold=70,sync_data_interval=3600\n##\nstorage     /dev/md0     /dev/nvme0n1    writeback_threshold=80,read_cache_threshold=127\n\n\n\n\n\nThis is a sample writeboost systemd unit file:\n\n\nhttps://gitlab.com/onlyjob/writeboost/blob/master/writeboost.service\n\n\n\n\n\nsystemctl daemon-reload\nsystemctl enable writeboost\n\n\nDRBD9 over Writeboost\n\n\nTo use writeboost in a drbd setup we should create a writeboost cache device on each drbd node and then use that cache device as a pv for drbd9 drbdpool volume group. So drbd9 will be run over a mounted cache device on each node.\nAnd this is the modified writeboost service file that we use when drbd9 is used over writeboost:\n\n\n[Unit]\nDescription=(dm-)writeboost mapper\nDocumentation=man:writeboost\n#DefaultDependencies=false\n#Conflicts=shutdown.target\n\n## \nBefore=local-fs-pre\n is significant as it influences correct order\n## of stopping (after unmount).\n#Before=shutdown.target drbd.service cryptsetup.target local-fs-pre.target\n#Before=shutdown.target cryptsetup.target local-fs-pre.target\n\nBefore=shutdown.target drbdmanaged.service\n\n[Service]\nType=oneshot\n\n## Must remain after exit to prevent stopping right after start\n## and to stop on shutdown.\nRemainAfterExit=yes\n\n## Scannong caching devices may take long time after unclean shutdown.\nTimeoutStartSec=3600\n\nExecStart=/usr/bin/bash -c \n/usr/sbin/writeboost; lvscan; lvchange -ay /dev/drbdpool/data_00\n\nExecStop=/usr/sbin/writeboost -u\n\n## Long \nTimeoutStop\n is essential as deadlock may happen if writeboost\n## is killed during flushing of caches on shutdown, etc.\nTimeoutStopSec=3600\n\nStandardOutput=syslog+console\n\n[Install]\n#WantedBy=cryptsetup.target\n#WantedBy=local-fs.target\nWantedBy=drbdmanaged.service\n#Alias=dm-writeboost.service", 
            "title": "Writeboost"
        }, 
        {
            "location": "/storage/cache/writeboost/#dm-writeboost", 
            "text": "https://github.com/akiradeveloper/dm-writeboost   This sample configuration does not use DRBD, it's just a simple dm-writeboost setup.", 
            "title": "dm-writeboost"
        }, 
        {
            "location": "/storage/cache/writeboost/#installation-on-centos-7", 
            "text": "", 
            "title": "Installation on Centos 7"
        }, 
        {
            "location": "/storage/cache/writeboost/#dkms", 
            "text": "yum install dkms -y", 
            "title": "DKMS"
        }, 
        {
            "location": "/storage/cache/writeboost/#writeboost-dkms-module", 
            "text": "Clone repository:  git clone https://github.com/akiradeveloper/dm-writeboost\ncd dm-writeboost   make   make install  Check that Writeboost DKMS is installed:  dkms status\nlsmod | grep dm_writeboost", 
            "title": "Writeboost DKMS module"
        }, 
        {
            "location": "/storage/cache/writeboost/#writeboost-tools", 
            "text": "git clone https://github.com/akiradeveloper/dm-writeboost-tools\nyum install cargo\ncargo install\ncp /root/.cargo/bin/* /usr/sbin/  Reboot and check module install", 
            "title": "Writeboost tools"
        }, 
        {
            "location": "/storage/cache/writeboost/#create-new-writeboost-cache", 
            "text": "wbcreate --reformat --read_cache_threshold=127  --writeback_threshold=80 storage /dev/md0 /dev/nvme0n1   storage: new cache device name  md0: slow disks  nvme0n1: fast disks (cache device)", 
            "title": "Create new Writeboost Cache"
        }, 
        {
            "location": "/storage/cache/writeboost/#crete-writeboosttab-and-service", 
            "text": "vi /etc/writeboosttab  ## dm-writeboost  tab  (mappings) file, see writeboosttab(5).\n##{DM target name}    {cached block device e.g. HDD}    {caching block device e.g. SSD}    [options]\n##\n## wb_hdd     /dev/disk/by-uuid/2e8260bc-024c-4252-a695-a73898c974c7     /dev/disk/by-partuuid/43372b68-3407-45fa-9b2f-61afe9c26a68    writeback_threshold=70,sync_data_interval=3600\n##\nstorage     /dev/md0     /dev/nvme0n1    writeback_threshold=80,read_cache_threshold=127  This is a sample writeboost systemd unit file:  https://gitlab.com/onlyjob/writeboost/blob/master/writeboost.service  systemctl daemon-reload\nsystemctl enable writeboost", 
            "title": "Crete writeboosttab and service"
        }, 
        {
            "location": "/storage/cache/writeboost/#drbd9-over-writeboost", 
            "text": "To use writeboost in a drbd setup we should create a writeboost cache device on each drbd node and then use that cache device as a pv for drbd9 drbdpool volume group. So drbd9 will be run over a mounted cache device on each node.\nAnd this is the modified writeboost service file that we use when drbd9 is used over writeboost:  [Unit]\nDescription=(dm-)writeboost mapper\nDocumentation=man:writeboost\n#DefaultDependencies=false\n#Conflicts=shutdown.target\n\n##  Before=local-fs-pre  is significant as it influences correct order\n## of stopping (after unmount).\n#Before=shutdown.target drbd.service cryptsetup.target local-fs-pre.target\n#Before=shutdown.target cryptsetup.target local-fs-pre.target\n\nBefore=shutdown.target drbdmanaged.service\n\n[Service]\nType=oneshot\n\n## Must remain after exit to prevent stopping right after start\n## and to stop on shutdown.\nRemainAfterExit=yes\n\n## Scannong caching devices may take long time after unclean shutdown.\nTimeoutStartSec=3600\n\nExecStart=/usr/bin/bash -c  /usr/sbin/writeboost; lvscan; lvchange -ay /dev/drbdpool/data_00 \nExecStop=/usr/sbin/writeboost -u\n\n## Long  TimeoutStop  is essential as deadlock may happen if writeboost\n## is killed during flushing of caches on shutdown, etc.\nTimeoutStopSec=3600\n\nStandardOutput=syslog+console\n\n[Install]\n#WantedBy=cryptsetup.target\n#WantedBy=local-fs.target\nWantedBy=drbdmanaged.service\n#Alias=dm-writeboost.service", 
            "title": "DRBD9 over Writeboost"
        }, 
        {
            "location": "/networking/concepts/", 
            "text": "Network linux commands\n\n\nLink layer\n\n\nShow interfaces:\n\n\nip link show\n\n\n\n\n\nEnable/disable an interface:\n\n\nip link set eth0 down\nip link set eth0 up\n\n\n\n\n\nChange name or address to interface manually:\n\n\nip link set eth0 down\nip link set eth0 address 00:11:22:33:44:55\nip link set eth0 name lan\nip link set eth0 up\n\n\n\n\n\nShow interface options physical options\n\n\nethtool eth0\n\n\n\n\n\nShow speed of interface:\n\n\nethtool eth0 |grep Speed\n\n\n\n\n\nKernel module that use the interface:\n\n\nethtool -i eth0\n\n\n\n\n\nShow statistics, error packets, collisions:\n\n\nethtool -S eth0\n\n\n\n\n\nVlans\n\n\nAdd virtual interface with tagged vlan 101 to eth0:\n\n\nip link add link eth0 name vlan101 type vlan id 101\nip link set vlan101 up\n\n\n\n\n\nBonding\n\n\nip link set eth1 down\nip link set eth2 down\n\n\nmodprobe -r bonding\nmodprobe bonding mode=4\n\n# default xmit_hash_policy is 0 (hash with mac adress)\n# modprobe bonding mode=4 xmit_hash_policy=2\n\nip link set eth1 master bond0\nip link set eth2 master bond0\n\nip link set eth1 up \nip link set eth2 up\n\nip link set bond0 up\n\ncat /proc/net/bonding/bond0\n\n\n\n\n\nTest network speed with iperf\n\n\nTraditional tool is \niperf\n, with speeds greater than 1Gbps new tool with more options \niperf3\n is better\n\n\n#Server\niperf -s \n#Client\niperf -c SERVER_IP\n\n\n\n\n\nSome useful options:\n\n\n#Server\niperf -s -i 2 -p 5002\n#Client\niperf -p 5002 -c SERVER_IP\n\n\n\n\n\n-t to increase duration of test\n\n\n#Server\niperf -s -i 2\n#Client\niperf -t 30 -c SERVER_IP\n\n\n\n\n\nTest network speed with netperf\n\n\nRealistic test with tcp both directions throughput.\n\n\n\n\nhttps://github.com/HewlettPackard/netperf\n\n\n\n\nStart netserver on one host and execute script on client:\n\n\nvi netperf-test.sh\n\n\n for i in 1\n     do\n      netperf -H 10.1.2.102 -t TCP_STREAM -B \noutbound\n -i 10 -P 0 -v 0 \\\n        -- -s 256K -S 256K \n\n      netperf -H 10.1.2.102 -t TCP_MAERTS -B \ninbound\n  -i 10 -P 0 -v 0 \\\n        -- -s 256K -S 256K \n\n     done\n\n\n\n\n\nIt will run on background and output to console results when it finishes.\n\n\nYou can check live network throughput during test by using iftop:\n\n\niftop -i eth0\n\n\n\n\n\nBridges\n\n\nCreate bridge:\n\n\nbrctl addif br0 usb0\n\n\n\n\n\nAssign interfaces to bridge:\n\n\nbrctl addif br0 eth0\nbrctl addif br0 usb0\nbrctl addif br0 usb1\nbrctl delif br0 usb1\n\n\n\n\n\nTo show interfaces members of a bridge and table of ports and mac address:\n\n\nbrctl show\nbrctl show br0\nbrctl showmacs br0\n\n\n\n\n\nCapture packets\n\n\ntcpdump\n: herramienta cl\u00e1sica para hacer capturas\n\n\ntcpdump -w 08232010.pcap -i eth0\n\n\n\n\n\ntshark\n: wireshark in command line, capture filters show [https://wiki.wireshark.org/CaptureFilters]\n\n\n\n\n[ -i ]\n Input interface\n\n\n[ -w ]\n Output file\n\n\n[ -r ]\n Read options from file\n\n\n[ -c ]\n  limit packets captured\n\n\n[ -a duration:X]\n seconds when stop capture\n\n\n[ -Y ]\n Wireshark filters\n\n\n\n\nExamples:\n\n\ntshark -i enp3s0 -w /tmp/out.pcap -c 10\ntshark -i enp3s0 -w /tmp/out.pcap -a duration:4\ntshark -r /tmp/out.pcap -w /tmp/web.pcap -Y \nhttp\n\n\ntshark -i enp3s0 -w /tmp/ping.pcap \nhost 8.8.8.8\n\ntshark -i enp3s0 -w /tmp/web.pcap \nport 80\n\n\n\n\n\n\nTo show in screen the packets with extended version (-x)\n\n\ntshark -r /tmp/web.pcap\ntshark -r /tmp/web.pcap -x \ntshark -r /tmp/web.pcap -x -Y \nframe.number==10\n\n\n\n\n\n\nNetwork Layer\n\n\nShow ip address, short command of \nip address show\n:\n\n\nip a\n\n\n\n\n\nShow stats (tx,rx,error,dropped)\n\n\nip -statistics a\n\n\n\n\n\nFixed ip address:\n\n\nDelete all address of an interface (flush):\n\n\nip a f dev eth0\n\n\n\n\n\nAsociar una ip a una interfaz (no olvidar la m\u00e1scara):\n\n\nip a a 192.168.100.10/24 dev eth0\n\n\n\n\n\nSe pueden asociar m\u00e1s de una ip a una interfaz y eliminar una en concreto:\n\n\nip a a 192.168.100.10/24 dev eth0\nip a a 192.168.200.11/27 dev eth0\n\nip a d 192.168.200.11/27 dev eth0\n\n\n\n\n\nIps din\u00e1micas:\n\n\nLiberar la ip actual:\n\n\ndhclient -r\n\n\n\n\n\nEsto deber\u00eda haber liberado la ip actual y el daemon deber\u00eda haber finalizado. En caso de que no podamos pedir una nueva ip no nos queda otra m\u00e1s que matar ese proceso con:\n\n\nkillall dhclient\n\n\n\n\n\nPara pedir una nueva ip en cualquier interface:\n\n\ndhclient\n\n\n\n\n\nY en una interface en concreto:\n\n\ndhclient eth0\n\n\n\n\n\nSi quieres ver m\u00e1s detalle de la concesi\u00f3n de ip:\n\n\ndhclient -v eth0\ndhclient -v -lf /tmp/eth0.lease\ncat /tmp/eth0.lease\n\n\n\n\n\nTabla ARP\n\n\nLa orden tradicional arp ha quedado centralizada en la utilidad ip con la opci\u00f3n \nip neigh\n \n\n\nPara ve la tabla de arp actual:\n\n\nip neigh\n\n\n\n\n\nBorrar tabla de arp:\n\n\nip neigh flush all\n\n\n\n\n\nA\u00f1adir entrada arp permanente:\n\n\nip neigh add 192.168.100.1 lladdr 00:11:22:33:44:55 dev enp3s0\n\n\n\n\n\nVer s\u00f3lo las entradas reachable:\n\n\nip neigh show nud reachable\n\n\n\n\n\nRouting\n\n\nVer las rutas con \nip route show\n:\n\n\nip r \n\n\n\n\n\nPara eliminar todas las rutas \"ip route flush\":\n\n\nip r f all\n\n\n\n\n\nAl a\u00f1adir una direcci\u00f3n ip a una interfaz, se a\u00f1ade una ruta directa\nque informa que para ir a la red de esa direcci\u00f3n ip se va directamente\na trav\u00e9s de la interfaz sin necesidad de pasar por ning\u00fan router:\n\n\n$ ip r f all\n$ ip a a 172.16.0.10/16 dev eth0\n$ ip r s \n172.16.0.10/16  dev eth0 [...]\n\n\n\n\n\nA\u00f1adir puerta de enlace por defecto:\n\n\nip r a default via 192.168.1.1\n\n\n\n\n\nEliminar puerta de enlace por defecto:\n\n\nip r d default\n\n\n\n\n\nA\u00f1adir ruta est\u00e1tica:\n\n\nip r a 192.168.200.0/24 via 192.168.100.1\n\n\n\n\n\nActivar bit de forwarding para que trabaje como router:\n\n\necho 1 \n /proc/sys/net/ipv4/ip_forward\n\n\n\n\n\nResoluci\u00f3n de nombres DNS\n\n\nSe consulta la resoluci\u00f3n en el fichero \n/etc/hosts\n, que contiene\nuna l\u00ednea para el nombre localhost, se pueden a\u00f1adir l\u00edneas para\nresolver nombres sin utilizar un servidor dns:\n\n\n$ cat /etc/hosts\n127.0.0.1       localhost.localdomain localhost\n\n\n\n\n\nPara resolver con servidores dns linux consulta el contenido del fichero /etc/resolv.conf\n\n\n$ cat /etc/resolv.conf \n[...]\nnameserver 192.168.1.1\n\n\n\n\n\nSe consulta la l\u00ednea que empieza con nameserver seguida de la ip del servidor dns que queremos usar.\nSi al renovar la ip din\u00e1mica el servidor dhcp ofrece un servidor dns se sobreescribe este fichero\n\n\nPara forzar la resoluci\u00f3n de nombres a trav\u00e9s de un servidor dns de forma manual:\n\n\necho \nnameserver 8.8.8.8\n \n /etc/resolv.conf\n\n\n\n\n\nEnmascaramiento de ips\n\n\nEnmascarar todo el tr\u00e1fico que sale hacia internet por eth0\n\n\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\n\n\n\n\n\nConsultar iptables:\n\n\niptables-save\niptables -t filter -L\niptables -t nat -L\n\n\n\n\n\nPruebas de conectividad\n\n\nComprabar respuestas protocolo ARP:\n\n\narping 192.168.100.1\n\n\n\n\n\nHacer ping indefinidamente (para terminar ctrl + C)\n\n\nping 8.8.8.8\n\n\n\n\n\nLimitar n\u00famero de pings\n\n\nping -c 2 8.8.8.8\n\n\n\n\n\nfping\n: Hacer pings a varios equipos de una red:\n\n\nfping -g 192.168.100.100 192.168.100.110 -a -q\nfping -g 192.168.100.0/24 -a -q\n\n\n\n\n\nnmap\n \n\n\nSondeo ping:\n\n\nnmap -sP 192.168.100.0/24\n\n\n\n\n\nSondeo de puerto 22\n\n\nTraceroute y geolocalizar ips\n\n\ntraceroute\n: Herramienta hist\u00f3rica para descubrir rutas\n\n\ntraceroute 8.8.8.8\n\n\n\n\n\nmtr\n: Si quieres m\u00e1s informaci\u00f3n, estad\u00edsticas y detalle de geolocalizaci\u00f3n de ips\n\n\ndnf -y install mtr GeoIP GeoIP-GeoLite-data GeoIP-GeoLite-data-extra\nmtr -n --report www.gencat.cat\ngeoiplookup 72.52.92.222\n\n\n\n\n\nCapa de transporte\n\n\nnmap y sondeo de puertos\n\n\nSondeo de puertos TCP t\u00edpicos:\n\n\nnmap -sS 192.168.100.1\n\n\n\n\n\nSondeo de un puerto concreto\n\n\nnmap -sS -p 80 192.168.100.1\n\n\n\n\n\nSondeo de rango de puertos\n\n\nnmap -sS -p 8080-8090 192.168.100.1\n\n\n\n\n\nSondeo de puertos aunque el ping no conteste, a\u00f1adiendo la opci\u00f3n -PN\n\n\nnmap -sS -PN -p 80 192.168.100.1\n\n\n\n\n\nSondeo de puerto UDP:\n\n\nnmap -sU -p 53 192.168.100.1\n\n\n\n\n\nnetstat\n\n\nn\u00famero de puertos tcp y udp escuchando con el programa asociado:\n\n\nnetstat -utlnp\n\n\n\n\n\nLas opciones se pueden combinar sin importar el orden en que van las letras,\nlas t\u00edpicas de netstat son:\n\n -u =\n puertos udp\n\n -t =\n puertos tcp\n\n -p =\n programa que est\u00e1 usando esa conexi\u00f3n y pid (s\u00f3lo lo puedes ver si eres root)\n\n -n =\n muestra el n\u00famero de puerto\n\n -a =\n todas las conexiones\n\n -l =\n puertos en escucha\n\n\nTodas las conexiones tcp:\n\n\nnetstat -tanp \n\n\n\n\n\nprobar puertos con netcat", 
            "title": "Concepts"
        }, 
        {
            "location": "/networking/concepts/#network-linux-commands", 
            "text": "", 
            "title": "Network linux commands"
        }, 
        {
            "location": "/networking/concepts/#link-layer", 
            "text": "Show interfaces:  ip link show  Enable/disable an interface:  ip link set eth0 down\nip link set eth0 up  Change name or address to interface manually:  ip link set eth0 down\nip link set eth0 address 00:11:22:33:44:55\nip link set eth0 name lan\nip link set eth0 up  Show interface options physical options  ethtool eth0  Show speed of interface:  ethtool eth0 |grep Speed  Kernel module that use the interface:  ethtool -i eth0  Show statistics, error packets, collisions:  ethtool -S eth0", 
            "title": "Link layer"
        }, 
        {
            "location": "/networking/concepts/#vlans", 
            "text": "Add virtual interface with tagged vlan 101 to eth0:  ip link add link eth0 name vlan101 type vlan id 101\nip link set vlan101 up", 
            "title": "Vlans"
        }, 
        {
            "location": "/networking/concepts/#bonding", 
            "text": "ip link set eth1 down\nip link set eth2 down\n\n\nmodprobe -r bonding\nmodprobe bonding mode=4\n\n# default xmit_hash_policy is 0 (hash with mac adress)\n# modprobe bonding mode=4 xmit_hash_policy=2\n\nip link set eth1 master bond0\nip link set eth2 master bond0\n\nip link set eth1 up \nip link set eth2 up\n\nip link set bond0 up\n\ncat /proc/net/bonding/bond0", 
            "title": "Bonding"
        }, 
        {
            "location": "/networking/concepts/#test-network-speed-with-iperf", 
            "text": "Traditional tool is  iperf , with speeds greater than 1Gbps new tool with more options  iperf3  is better  #Server\niperf -s \n#Client\niperf -c SERVER_IP  Some useful options:  #Server\niperf -s -i 2 -p 5002\n#Client\niperf -p 5002 -c SERVER_IP  -t to increase duration of test  #Server\niperf -s -i 2\n#Client\niperf -t 30 -c SERVER_IP", 
            "title": "Test network speed with iperf"
        }, 
        {
            "location": "/networking/concepts/#test-network-speed-with-netperf", 
            "text": "Realistic test with tcp both directions throughput.   https://github.com/HewlettPackard/netperf   Start netserver on one host and execute script on client:  vi netperf-test.sh   for i in 1\n     do\n      netperf -H 10.1.2.102 -t TCP_STREAM -B  outbound  -i 10 -P 0 -v 0 \\\n        -- -s 256K -S 256K  \n      netperf -H 10.1.2.102 -t TCP_MAERTS -B  inbound   -i 10 -P 0 -v 0 \\\n        -- -s 256K -S 256K  \n     done  It will run on background and output to console results when it finishes.  You can check live network throughput during test by using iftop:  iftop -i eth0", 
            "title": "Test network speed with netperf"
        }, 
        {
            "location": "/networking/concepts/#bridges", 
            "text": "Create bridge:  brctl addif br0 usb0  Assign interfaces to bridge:  brctl addif br0 eth0\nbrctl addif br0 usb0\nbrctl addif br0 usb1\nbrctl delif br0 usb1  To show interfaces members of a bridge and table of ports and mac address:  brctl show\nbrctl show br0\nbrctl showmacs br0", 
            "title": "Bridges"
        }, 
        {
            "location": "/networking/concepts/#capture-packets", 
            "text": "tcpdump : herramienta cl\u00e1sica para hacer capturas  tcpdump -w 08232010.pcap -i eth0  tshark : wireshark in command line, capture filters show [https://wiki.wireshark.org/CaptureFilters]   [ -i ]  Input interface  [ -w ]  Output file  [ -r ]  Read options from file  [ -c ]   limit packets captured  [ -a duration:X]  seconds when stop capture  [ -Y ]  Wireshark filters   Examples:  tshark -i enp3s0 -w /tmp/out.pcap -c 10\ntshark -i enp3s0 -w /tmp/out.pcap -a duration:4\ntshark -r /tmp/out.pcap -w /tmp/web.pcap -Y  http \n\ntshark -i enp3s0 -w /tmp/ping.pcap  host 8.8.8.8 \ntshark -i enp3s0 -w /tmp/web.pcap  port 80   To show in screen the packets with extended version (-x)  tshark -r /tmp/web.pcap\ntshark -r /tmp/web.pcap -x \ntshark -r /tmp/web.pcap -x -Y  frame.number==10", 
            "title": "Capture packets"
        }, 
        {
            "location": "/networking/concepts/#network-layer", 
            "text": "Show ip address, short command of  ip address show :  ip a  Show stats (tx,rx,error,dropped)  ip -statistics a", 
            "title": "Network Layer"
        }, 
        {
            "location": "/networking/concepts/#fixed-ip-address", 
            "text": "Delete all address of an interface (flush):  ip a f dev eth0  Asociar una ip a una interfaz (no olvidar la m\u00e1scara):  ip a a 192.168.100.10/24 dev eth0  Se pueden asociar m\u00e1s de una ip a una interfaz y eliminar una en concreto:  ip a a 192.168.100.10/24 dev eth0\nip a a 192.168.200.11/27 dev eth0\n\nip a d 192.168.200.11/27 dev eth0", 
            "title": "Fixed ip address:"
        }, 
        {
            "location": "/networking/concepts/#ips-dinamicas", 
            "text": "Liberar la ip actual:  dhclient -r  Esto deber\u00eda haber liberado la ip actual y el daemon deber\u00eda haber finalizado. En caso de que no podamos pedir una nueva ip no nos queda otra m\u00e1s que matar ese proceso con:  killall dhclient  Para pedir una nueva ip en cualquier interface:  dhclient  Y en una interface en concreto:  dhclient eth0  Si quieres ver m\u00e1s detalle de la concesi\u00f3n de ip:  dhclient -v eth0\ndhclient -v -lf /tmp/eth0.lease\ncat /tmp/eth0.lease", 
            "title": "Ips din\u00e1micas:"
        }, 
        {
            "location": "/networking/concepts/#tabla-arp", 
            "text": "La orden tradicional arp ha quedado centralizada en la utilidad ip con la opci\u00f3n  ip neigh    Para ve la tabla de arp actual:  ip neigh  Borrar tabla de arp:  ip neigh flush all  A\u00f1adir entrada arp permanente:  ip neigh add 192.168.100.1 lladdr 00:11:22:33:44:55 dev enp3s0  Ver s\u00f3lo las entradas reachable:  ip neigh show nud reachable", 
            "title": "Tabla ARP"
        }, 
        {
            "location": "/networking/concepts/#routing", 
            "text": "Ver las rutas con  ip route show :  ip r   Para eliminar todas las rutas \"ip route flush\":  ip r f all  Al a\u00f1adir una direcci\u00f3n ip a una interfaz, se a\u00f1ade una ruta directa\nque informa que para ir a la red de esa direcci\u00f3n ip se va directamente\na trav\u00e9s de la interfaz sin necesidad de pasar por ning\u00fan router:  $ ip r f all\n$ ip a a 172.16.0.10/16 dev eth0\n$ ip r s \n172.16.0.10/16  dev eth0 [...]  A\u00f1adir puerta de enlace por defecto:  ip r a default via 192.168.1.1  Eliminar puerta de enlace por defecto:  ip r d default  A\u00f1adir ruta est\u00e1tica:  ip r a 192.168.200.0/24 via 192.168.100.1  Activar bit de forwarding para que trabaje como router:  echo 1   /proc/sys/net/ipv4/ip_forward", 
            "title": "Routing"
        }, 
        {
            "location": "/networking/concepts/#resolucion-de-nombres-dns", 
            "text": "Se consulta la resoluci\u00f3n en el fichero  /etc/hosts , que contiene\nuna l\u00ednea para el nombre localhost, se pueden a\u00f1adir l\u00edneas para\nresolver nombres sin utilizar un servidor dns:  $ cat /etc/hosts\n127.0.0.1       localhost.localdomain localhost  Para resolver con servidores dns linux consulta el contenido del fichero /etc/resolv.conf  $ cat /etc/resolv.conf \n[...]\nnameserver 192.168.1.1  Se consulta la l\u00ednea que empieza con nameserver seguida de la ip del servidor dns que queremos usar.\nSi al renovar la ip din\u00e1mica el servidor dhcp ofrece un servidor dns se sobreescribe este fichero  Para forzar la resoluci\u00f3n de nombres a trav\u00e9s de un servidor dns de forma manual:  echo  nameserver 8.8.8.8    /etc/resolv.conf", 
            "title": "Resoluci\u00f3n de nombres DNS"
        }, 
        {
            "location": "/networking/concepts/#enmascaramiento-de-ips", 
            "text": "Enmascarar todo el tr\u00e1fico que sale hacia internet por eth0  iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE  Consultar iptables:  iptables-save\niptables -t filter -L\niptables -t nat -L", 
            "title": "Enmascaramiento de ips"
        }, 
        {
            "location": "/networking/concepts/#pruebas-de-conectividad", 
            "text": "Comprabar respuestas protocolo ARP:  arping 192.168.100.1  Hacer ping indefinidamente (para terminar ctrl + C)  ping 8.8.8.8  Limitar n\u00famero de pings  ping -c 2 8.8.8.8  fping : Hacer pings a varios equipos de una red:  fping -g 192.168.100.100 192.168.100.110 -a -q\nfping -g 192.168.100.0/24 -a -q  nmap    Sondeo ping:  nmap -sP 192.168.100.0/24  Sondeo de puerto 22", 
            "title": "Pruebas de conectividad"
        }, 
        {
            "location": "/networking/concepts/#traceroute-y-geolocalizar-ips", 
            "text": "traceroute : Herramienta hist\u00f3rica para descubrir rutas  traceroute 8.8.8.8  mtr : Si quieres m\u00e1s informaci\u00f3n, estad\u00edsticas y detalle de geolocalizaci\u00f3n de ips  dnf -y install mtr GeoIP GeoIP-GeoLite-data GeoIP-GeoLite-data-extra\nmtr -n --report www.gencat.cat\ngeoiplookup 72.52.92.222", 
            "title": "Traceroute y geolocalizar ips"
        }, 
        {
            "location": "/networking/concepts/#capa-de-transporte", 
            "text": "", 
            "title": "Capa de transporte"
        }, 
        {
            "location": "/networking/concepts/#nmap-y-sondeo-de-puertos", 
            "text": "Sondeo de puertos TCP t\u00edpicos:  nmap -sS 192.168.100.1  Sondeo de un puerto concreto  nmap -sS -p 80 192.168.100.1  Sondeo de rango de puertos  nmap -sS -p 8080-8090 192.168.100.1  Sondeo de puertos aunque el ping no conteste, a\u00f1adiendo la opci\u00f3n -PN  nmap -sS -PN -p 80 192.168.100.1  Sondeo de puerto UDP:  nmap -sU -p 53 192.168.100.1", 
            "title": "nmap y sondeo de puertos"
        }, 
        {
            "location": "/networking/concepts/#netstat", 
            "text": "n\u00famero de puertos tcp y udp escuchando con el programa asociado:  netstat -utlnp  Las opciones se pueden combinar sin importar el orden en que van las letras,\nlas t\u00edpicas de netstat son:  -u =  puertos udp  -t =  puertos tcp  -p =  programa que est\u00e1 usando esa conexi\u00f3n y pid (s\u00f3lo lo puedes ver si eres root)  -n =  muestra el n\u00famero de puerto  -a =  todas las conexiones  -l =  puertos en escucha  Todas las conexiones tcp:  netstat -tanp", 
            "title": "netstat"
        }, 
        {
            "location": "/networking/concepts/#probar-puertos-con-netcat", 
            "text": "", 
            "title": "probar puertos con netcat"
        }, 
        {
            "location": "/networking/tools/", 
            "text": "", 
            "title": "Tools"
        }, 
        {
            "location": "/networking/bondings/", 
            "text": "", 
            "title": "Bondings"
        }, 
        {
            "location": "/networking/10g/", 
            "text": "", 
            "title": "TenGigabit"
        }, 
        {
            "location": "/networking/40g/", 
            "text": "", 
            "title": "FortyGigabit"
        }, 
        {
            "location": "/clusters/pacemaker/", 
            "text": "Pacemaker\n\n\nConfiguring\n\n\nPacemaker is a cluster an resource manager\n\n\nTo install it on CentOS\n\n\nyum install corosync pacemaker pcs python-pycurl fence-agents-apc fence-agents-apc-snmp\n\n\n\n\n\nEnable and start services\n\n\nsystemctl enable pcsd\nsystemctl enable corosync\nsystemctl enable pacemaker\nsystemctl start pcsd\n\n\n\n\n\nWe need a new password to join the cluster. The default user is hacluster\n\n\npasswd hacluster\n\n\n\n\n\nAuthorize servers, set up cluster name and start all\n\n\npcs cluster auth cluster1 cluster2\npcs cluster setup --name backup_cluster cluster1 cluster2\npcs cluster start --all\n\n\n\n\n\nWe can disable some settings till we finish configuration\n\n\npcs property set stonith-enabled=false\npcs property set maintenance-mode=true\n\n\n\n\n\nThe first resource that should exist is a fencing device\n\n\npcs cluster cib stonith_cfg\npcs -f stonith_cfg stonith create stonith1 fence_apc_snmp ipaddr=10.1.79.11 pcmk_host_list=\ncluster1\n pcmk_host_map=\ncluster1:6\n pcmk_host_check=static-list power_wait=3 inet4_only=1 retry_on=10\npcs -f stonith_cfg stonith create stonith2 fence_apc_snmp ipaddr=10.1.79.12 pcmk_host_list=\ncluster2\n pcmk_host_map=\ncluster2:6\n pcmk_host_check=static-list power_wait=5 inet4_only=1 retry_on=10\npcs cluster cib-push stonith_cfg\n\n\n\n\n\nFilesystem resource\n\n\npcs resource create backup_ext4 ocf:heartbeat:Filesystem device=\n/dev/drbd104\n directory=\n/mnt/backup\n fstype=\next4\n \noptions=defaults,noatime\n op monitor interval=10s\npcs resource create isard_ext4 ocf:heartbeat:Filesystem device=\n/dev/drbd102\n directory=\n/mnt/isard\n fstype=\next4\n \noptions=defaults,noatime\n op monitor interval=10s\n\n\n\n\n\nFloating IPs\n\n\npcs resource create backup_ip ocf:heartbeat:IPaddr2 ip=10.1.90.10 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create isard_ip ocf:heartbeat:IPaddr2 ip=10.1.2.200 cidr_netmask=32 nic=isard:0 op monitor interval=30\n\n\n\n\n\nNFS v4 server\n\n\npcs cluster cib nfs_cfg\npcs -f nfs_cfg resource create nfs_daemon systemd:nfs-server op monitor interval=30s --group=nfs_server\npcs -f nfs_cfg resource create nfs_root ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 options=rw,crossmnt,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt fsid=0 --group=nfs_server\npcs cluster cib-push nfs_cfg\npcs resource clone nfs_server master-max=3 master-node-max=1 clone-max=3 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\n\n\n\n\nNFS v4 exports\n\n\npcs cluster cib exports_cfg\npcs -f exports_cfg resource create backup_nfs ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 wait_for_leasetime_on_stop=true options=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt/backup fsid=5 op monitor interval=30s\npcs -f exports_cfg resource create isard_nfs ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 wait_for_leasetime_on_stop=true options=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt/isard fsid=6 op monitor interval=30s\npcs cluster cib-push exports_cfg\n\n\n\n\n\nResource groups\n\n\npcs resource group add backup backup_ext4 backup_nfs backup_ip\npcs resource group add isard isard_ext4 isard_nfs isard_ip\n\n\n\n\n\nActivate cluster\n\n\npcs property set maintenance-mode=false\npcs property set stonith-enabled=true", 
            "title": "Pacemaker"
        }, 
        {
            "location": "/clusters/pacemaker/#pacemaker", 
            "text": "", 
            "title": "Pacemaker"
        }, 
        {
            "location": "/clusters/pacemaker/#configuring", 
            "text": "Pacemaker is a cluster an resource manager  To install it on CentOS  yum install corosync pacemaker pcs python-pycurl fence-agents-apc fence-agents-apc-snmp  Enable and start services  systemctl enable pcsd\nsystemctl enable corosync\nsystemctl enable pacemaker\nsystemctl start pcsd  We need a new password to join the cluster. The default user is hacluster  passwd hacluster  Authorize servers, set up cluster name and start all  pcs cluster auth cluster1 cluster2\npcs cluster setup --name backup_cluster cluster1 cluster2\npcs cluster start --all  We can disable some settings till we finish configuration  pcs property set stonith-enabled=false\npcs property set maintenance-mode=true  The first resource that should exist is a fencing device  pcs cluster cib stonith_cfg\npcs -f stonith_cfg stonith create stonith1 fence_apc_snmp ipaddr=10.1.79.11 pcmk_host_list= cluster1  pcmk_host_map= cluster1:6  pcmk_host_check=static-list power_wait=3 inet4_only=1 retry_on=10\npcs -f stonith_cfg stonith create stonith2 fence_apc_snmp ipaddr=10.1.79.12 pcmk_host_list= cluster2  pcmk_host_map= cluster2:6  pcmk_host_check=static-list power_wait=5 inet4_only=1 retry_on=10\npcs cluster cib-push stonith_cfg", 
            "title": "Configuring"
        }, 
        {
            "location": "/clusters/pacemaker/#filesystem-resource", 
            "text": "pcs resource create backup_ext4 ocf:heartbeat:Filesystem device= /dev/drbd104  directory= /mnt/backup  fstype= ext4   options=defaults,noatime  op monitor interval=10s\npcs resource create isard_ext4 ocf:heartbeat:Filesystem device= /dev/drbd102  directory= /mnt/isard  fstype= ext4   options=defaults,noatime  op monitor interval=10s", 
            "title": "Filesystem resource"
        }, 
        {
            "location": "/clusters/pacemaker/#floating-ips", 
            "text": "pcs resource create backup_ip ocf:heartbeat:IPaddr2 ip=10.1.90.10 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create isard_ip ocf:heartbeat:IPaddr2 ip=10.1.2.200 cidr_netmask=32 nic=isard:0 op monitor interval=30", 
            "title": "Floating IPs"
        }, 
        {
            "location": "/clusters/pacemaker/#nfs-v4-server", 
            "text": "pcs cluster cib nfs_cfg\npcs -f nfs_cfg resource create nfs_daemon systemd:nfs-server op monitor interval=30s --group=nfs_server\npcs -f nfs_cfg resource create nfs_root ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 options=rw,crossmnt,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt fsid=0 --group=nfs_server\npcs cluster cib-push nfs_cfg\npcs resource clone nfs_server master-max=3 master-node-max=1 clone-max=3 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0", 
            "title": "NFS v4 server"
        }, 
        {
            "location": "/clusters/pacemaker/#nfs-v4-exports", 
            "text": "pcs cluster cib exports_cfg\npcs -f exports_cfg resource create backup_nfs ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 wait_for_leasetime_on_stop=true options=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt/backup fsid=5 op monitor interval=30s\npcs -f exports_cfg resource create isard_nfs ocf:heartbeat:exportfs clientspec=10.1.0.0/255.255.0.0 wait_for_leasetime_on_stop=true options=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/mnt/isard fsid=6 op monitor interval=30s\npcs cluster cib-push exports_cfg", 
            "title": "NFS v4 exports"
        }, 
        {
            "location": "/clusters/pacemaker/#resource-groups", 
            "text": "pcs resource group add backup backup_ext4 backup_nfs backup_ip\npcs resource group add isard isard_ext4 isard_nfs isard_ip", 
            "title": "Resource groups"
        }, 
        {
            "location": "/clusters/pacemaker/#activate-cluster", 
            "text": "pcs property set maintenance-mode=false\npcs property set stonith-enabled=true", 
            "title": "Activate cluster"
        }, 
        {
            "location": "/clusters/stonith/", 
            "text": "", 
            "title": "Stonith"
        }, 
        {
            "location": "/clusters/ocf/eio/", 
            "text": "#!/bin/sh\n\n\n#\n\n\n#\n\n\n#   EIO OCF RA. Does nothing but wait a few seconds, can be\n\n\n#   configured to fail occassionally.\n\n\n#\n\n\n# Copyright (c) 2004 SUSE LINUX AG, Lars Marowsky-Br\ufffde\n\n\n#                    All Rights Reserved.\n\n\n#\n\n\n# This program is free software; you can redistribute it and/or modify\n\n\n# it under the terms of version 2 of the GNU General Public License as\n\n\n# published by the Free Software Foundation.\n\n\n#\n\n\n# This program is distributed in the hope that it would be useful, but\n\n\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n\n\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n#\n\n\n# Further, this software is distributed without any warranty that it is\n\n\n# free of the rightful claim of any third person regarding infringement\n\n\n# or the like.  Any license provided herein, whether implied or\n\n\n# otherwise, applies only to this software file.  Patent licenses, if\n\n\n# any, provided herein do not apply to combinations of this program with\n\n\n# other software, or any other product whatsoever.\n\n\n#\n\n\n# You should have received a copy of the GNU General Public License\n\n\n# along with this program; if not, write the Free Software Foundation,\n\n\n# Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.\n\n\n#\n\n\n\n#######################################################################\n\n\n# Initialization:\n\n\n: \n${\nOCF_FUNCTIONS\n=\n${\nOCF_ROOT\n}\n/resource.d/heartbeat/.ocf-shellfuncs\n}\n\n. \n${\nOCF_FUNCTIONS\n}\n\n: \n${\n__OCF_ACTION\n=\n$1\n}\n\n\n\n#######################################################################\n\n\nmeta_data\n()\n \n{\n\n    cat \nEND\n\n\n?xml version=\n1.0\n?\n\n\n!DOCTYPE resource-agent SYSTEM \nra-api-1.dtd\n\n\nresource-agent name=\neio4\n version=\n1.0\n\n\nversion\n1.0\n/version\n\n\n\nlongdesc lang=\nen\n\n\nEnhanceIO\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nEnhanceIO\n/shortdesc\n\n\n\nparameters\n\n\nparameter name=\nstate\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nLocation to store the resource state in.\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nState file\n/shortdesc\n\n\ncontent type=\nstring\n default=\n${HA_VARRUN}/EIO-{OCF_RESOURCE_INSTANCE}.state\n /\n\n\n/parameter\n\n\n\nparameter name=\npasswd\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nFake password field\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nPassword\n/shortdesc\n\n\ncontent type=\nstring\n default=\n /\n\n\n/parameter\n\n\n\nparameter name=\nfake\n unique=\n0\n\n\nlongdesc lang=\nen\n\n\nFake attribute that can be changed to cause a reload\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nFake attribute that can be changed to cause a reload\n/shortdesc\n\n\ncontent type=\nstring\n default=\neio\n /\n\n\n/parameter\n\n\n\nparameter name=\nop_sleep\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nNumber of seconds to sleep during operations.  This can be used to test how\n\n\nthe cluster reacts to operation timeouts.\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nOperation sleep duration in seconds.\n/shortdesc\n\n\ncontent type=\nstring\n default=\n0\n /\n\n\n/parameter\n\n\n\n/parameters\n\n\n\nactions\n\n\naction name=\nstart\n        timeout=\n20\n /\n\n\naction name=\nstop\n         timeout=\n1800\n /\n\n\naction name=\nmonitor\n      timeout=\n20\n interval=\n10\n depth=\n0\n/\n\n\naction name=\nreload\n       timeout=\n20\n /\n\n\naction name=\nmigrate_to\n   timeout=\n20\n /\n\n\naction name=\nmigrate_from\n timeout=\n20\n /\n\n\naction name=\nvalidate-all\n timeout=\n20\n /\n\n\naction name=\nmeta-data\n    timeout=\n5\n /\n\n\n/actions\n\n\n/resource-agent\n\n\nEND\n\n\n}\n\n\n\n#######################################################################\n\n\n\n# don\nt exit on TERM, to test that lrmd makes sure that we do exit\n\n\ntrap\n sigterm_handler TERM\nsigterm_handler\n()\n \n{\n\n    ocf_log info \nThey use TERM to bring us down. No such luck.\n\n    \nreturn\n\n\n}\n\n\neio_usage\n()\n \n{\n\n    cat \nEND\n\n\nusage: $0 {start|stop|monitor|migrate_to|migrate_from|validate-all|meta-data}\n\n\n\nExpects to have a fully populated OCF RA-compliant environment set.\n\n\nEND\n\n\n}\n\n\neio_start\n()\n \n{\n\n    /usr/local/bin/eio_drbd.sh start\n    touch \n${\nOCF_RESKEY_state\n}\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\neio_stop\n()\n \n{\n\n    /usr/local/bin/eio_drbd.sh stop\n    rm \n${\nOCF_RESKEY_state\n}\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\neio_monitor\n()\n \n{\n\n    \n# Monitor _MUST!_ differentiate correctly between running\n\n    \n# (SUCCESS), failed (ERROR) or _cleanly_ stopped (NOT RUNNING).\n\n    \n# That is THREE states, not just yes/no.\n\n\n    \nif\n \n[\n \n$OCF_RESKEY_op_sleep\n -ne \n0\n \n]\n;\n \nthen\n\n        \nif\n \n[\n -f \n${\nVERIFY_SERIALIZED_FILE\n}\n \n]\n;\n \nthen\n\n            \n# two monitor ops have occurred at the same time.\n\n            \n# this is to verify a condition in the lrmd regression tests.\n\n            ocf_log err \n$VERIFY_SERIALIZED_FILE\n exists already\n\n            \nreturn\n \n$OCF_ERR_GENERIC\n\n        \nfi\n\n\n        touch \n${\nVERIFY_SERIALIZED_FILE\n}\n\n        sleep \n${\nOCF_RESKEY_op_sleep\n}\n\n        rm \n${\nVERIFY_SERIALIZED_FILE\n}\n\n    \nfi\n\n\n    \nif\n \n[\n -f \n${\nOCF_RESKEY_state\n}\n \n]\n;\n \nthen\n\n        \nreturn\n \n$OCF_SUCCESS\n\n    \nfi\n\n    \nif\n \nfalse\n \n;\n \nthen\n\n        \nreturn\n \n$OCF_ERR_GENERIC\n\n    \nfi\n\n    \nreturn\n \n$OCF_NOT_RUNNING\n\n\n}\n\n\neio_validate\n()\n \n{\n\n\n    \n# Is the state directory writable? \n\n    \nstate_dir\n=\n`\ndirname \n$OCF_RESKEY_state\n`\n\n    touch \n$state_dir\n/\n$$\n\n    \nif\n \n[\n \n$?\n !\n=\n \n0\n \n]\n;\n \nthen\n\n    \nreturn\n \n$OCF_ERR_ARGS\n\n    \nfi\n\n    rm \n$state_dir\n/\n$$\n\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\n: \n${\nOCF_RESKEY_fake\n=eio\n}\n\n: \n${\nOCF_RESKEY_op_sleep\n=0\n}\n\n: \n${\nOCF_RESKEY_CRM_meta_interval\n=0\n}\n\n: \n${\nOCF_RESKEY_CRM_meta_globally_unique\n:=\ntrue\n}\n\n\n\nif\n \n[\n \nx\n$OCF_RESKEY_state\n \n=\n \nx\n \n]\n;\n \nthen\n\n    \nif\n \n[\n \n${\nOCF_RESKEY_CRM_meta_globally_unique\n}\n \n=\n \nfalse\n \n]\n;\n \nthen\n\n    \nstate\n=\n${\nHA_VARRUN\n}\n/EIO-\n${\nOCF_RESOURCE_INSTANCE\n}\n.state\n\n\n    \n# Strip off the trailing clone marker\n\n    \nOCF_RESKEY_state\n=\n`\necho\n \n$state\n \n|\n sed s/:\n[\n0\n-9\n][\n0\n-9\n]\n*\n\\.\nstate/.state/\n`\n\n    \nelse\n \n    \nOCF_RESKEY_state\n=\n${\nHA_VARRUN\n}\n/EIO-\n${\nOCF_RESOURCE_INSTANCE\n}\n.state\n\n    \nfi\n\n\nfi\n\n\nVERIFY_SERIALIZED_FILE\n=\n${\nOCF_RESKEY_state\n}\n.serialized\n\n\n\ncase\n \n$__OCF_ACTION\n in\nmeta-data\n)\n  meta_data\n        \nexit\n \n$OCF_SUCCESS\n\n        \n;;\n\nstart\n)\n      eio_start\n;;\n\nstop\n)\n       eio_stop\n;;\n\nmonitor\n)\n    eio_monitor\n;;\n\nmigrate_to\n)\n ocf_log info \nMigrating \n${\nOCF_RESOURCE_INSTANCE\n}\n to \n${\nOCF_RESKEY_CRM_meta_migrate_target\n}\n.\n\n            eio_stop\n        \n;;\n\nmigrate_from\n)\n   ocf_log info \nMigrating \n${\nOCF_RESOURCE_INSTANCE\n}\n to \n${\nOCF_RESKEY_CRM_meta_migrate_source\n}\n.\n\n            eio_start\n        \n;;\n\nreload\n)\n     ocf_log err \nReloading...\n\n            eio_start\n        \n;;\n\nvalidate-all\n)\n   eio_validate\n;;\n\nusage\n|\nhelp\n)\n eio_usage\n        \nexit\n \n$OCF_SUCCESS\n\n        \n;;\n\n*\n)\n      eio_usage\n        \nexit\n \n$OCF_ERR_UNIMPLEMENTED\n\n        \n;;\n\n\nesac\n\n\nrc\n=\n$?\n\nocf_log debug \n${\nOCF_RESOURCE_INSTANCE\n}\n \n$__OCF_ACTION\n : \n$rc\n\n\nexit\n \n$rc\n\n\n#!/bin/bash\n\n\n#\n\n\n# chkconfig: 35 90 12\n\n\n# description: Foo server\n\n\n#\n\n\n# Get function from functions library\n\n. /etc/init.d/functions\n\n# Start the service FOO\n\nstart\n()\n \n{\n\n        initlog -c \necho -n Starting EIO for samba_cache: \n\n    \n#/bin/cp -rf /opt/94/* /etc/udev/rules.d/\n\n    sleep 2s\n        eio_cli \nenable\n -d /dev/vgdrbd11/lvdrbd_samba -s /dev/vgdrbd10/lvdrbd_cache -p lru -m wb -c samba_cache\n        \n#eio_cli create -d /dev/vgdrbd11/lvdrbd_samba -s /dev/vgdrbd10/lvdrbd_cache -p lru -m wb -c samba_cache\n\n        \n### Create the lock file ###\n\n        touch /var/lock/subsys/eio_samba_cache\n        success \n$\nCache samba_cache started\n\n        \necho\n\n\n}\n\n\n# Restart the service FOO\n\nstop\n()\n \n{\n\n        initlog -c \necho -n Stopping EIO for samba_cache: \n\n    eio_cli edit -c samba_cache -m ro\n    grep \nnr_dirty                              0\n /proc/enhanceio/samba_cache/stats\n    \nwhile\n \n[\n \n$?\n -ne \n0\n]\n;\n \ndo\n\n        grep \nnr_dirty                              0\n /proc/enhanceio/samba_cache/stats\n    \ndone\n\n    eio_cli delete -c samba_cache\n        \n### Now, delete the lock file ###\n\n        rm -f /var/lock/subsys/eio_samba_cache\n        \necho\n\n\n}\n\n\n### main logic ###\n\n\ncase\n \n$1\n in\n  start\n)\n\n        start\n        \n;;\n\n  stop\n)\n\n        stop\n        \n;;\n\n  status\n)\n\n        status FOO\n        \n;;\n\n  restart\n|\nreload\n|\ncondrestart\n)\n\n        stop\n        start\n        \n;;\n\n  *\n)\n\n        \necho\n $\nUsage: \n$0\n {start|stop|restart|reload|status}\n\n        \nexit\n \n1\n\n\nesac\n\n\nexit\n \n0", 
            "title": "EnhanceIO"
        }, 
        {
            "location": "/clusters/ocf/writeboost/", 
            "text": "#!/bin/sh\n\n\n#\n\n\n#\n\n\n#   DM-WRITEBOOST OCF RA. Does nothing but wait a few seconds, can be\n\n\n#   configured to fail occassionally.\n\n\n#\n\n\n# Copyright (c) 2004 SUSE LINUX AG, Lars Marowsky-Br\ufffde\n\n\n#                    All Rights Reserved.\n\n\n#\n\n\n# This program is free software; you can redistribute it and/or modify\n\n\n# it under the terms of version 2 of the GNU General Public License as\n\n\n# published by the Free Software Foundation.\n\n\n#\n\n\n# This program is distributed in the hope that it would be useful, but\n\n\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n\n\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n#\n\n\n# Further, this software is distributed without any warranty that it is\n\n\n# free of the rightful claim of any third person regarding infringement\n\n\n# or the like.  Any license provided herein, whether implied or\n\n\n# otherwise, applies only to this software file.  Patent licenses, if\n\n\n# any, provided herein do not apply to combinations of this program with\n\n\n# other software, or any other product whatsoever.\n\n\n#\n\n\n# You should have received a copy of the GNU General Public License\n\n\n# along with this program; if not, write the Free Software Foundation,\n\n\n# Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307, USA.\n\n\n#\n\n\n\n#######################################################################\n\n\n# Initialization:\n\n\n: \n${\nOCF_FUNCTIONS\n=\n${\nOCF_ROOT\n}\n/resource.d/heartbeat/.ocf-shellfuncs\n}\n\n. \n${\nOCF_FUNCTIONS\n}\n\n: \n${\n__OCF_ACTION\n=\n$1\n}\n\n\n\n#######################################################################\n\n\nmeta_data\n()\n \n{\n\n    cat \nEND\n\n\n?xml version=\n1.0\n?\n\n\n!DOCTYPE resource-agent SYSTEM \nra-api-1.dtd\n\n\nresource-agent name=\nwriteboost\n version=\n1.0\n\n\nversion\n1.0\n/version\n\n\n\nlongdesc lang=\nen\n\n\ndm-writeboost cache\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nwriteboost\n/shortdesc\n\n\n\nparameters\n\n\nparameter name=\nstate\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nLocation to store the resource state in.\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nState file\n/shortdesc\n\n\ncontent type=\nstring\n default=\n${HA_VARRUN}/WRITEBOOST-{OCF_RESOURCE_INSTANCE}.state\n /\n\n\n/parameter\n\n\n\nparameter name=\npasswd\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nFake password field\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nPassword\n/shortdesc\n\n\ncontent type=\nstring\n default=\n /\n\n\n/parameter\n\n\n\nparameter name=\nfake\n unique=\n0\n\n\nlongdesc lang=\nen\n\n\nFake attribute that can be changed to cause a reload\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nFake attribute that can be changed to cause a reload\n/shortdesc\n\n\ncontent type=\nstring\n default=\nwriteboost\n /\n\n\n/parameter\n\n\n\nparameter name=\nop_sleep\n unique=\n1\n\n\nlongdesc lang=\nen\n\n\nNumber of seconds to sleep during operations.  This can be used to test how\n\n\nthe cluster reacts to operation timeouts.\n\n\n/longdesc\n\n\nshortdesc lang=\nen\nOperation sleep duration in seconds.\n/shortdesc\n\n\ncontent type=\nstring\n default=\n0\n /\n\n\n/parameter\n\n\n\n/parameters\n\n\n\nactions\n\n\naction name=\nstart\n        timeout=\n3600\n /\n\n\naction name=\nstop\n         timeout=\n1111\n /\n\n\naction name=\nmonitor\n      timeout=\n20\n interval=\n10\n depth=\n0\n/\n\n\naction name=\nreload\n       timeout=\n20\n /\n\n\naction name=\nmigrate_to\n   timeout=\n20\n /\n\n\naction name=\nmigrate_from\n timeout=\n20\n /\n\n\naction name=\nvalidate-all\n timeout=\n20\n /\n\n\naction name=\nmeta-data\n    timeout=\n5\n /\n\n\n/actions\n\n\n/resource-agent\n\n\nEND\n\n\n}\n\n\n\n#######################################################################\n\n\n\n# don\nt exit on TERM, to test that lrmd makes sure that we do exit\n\n\ntrap\n sigterm_handler TERM\nsigterm_handler\n()\n \n{\n\n    ocf_log info \nThey use TERM to bring us down. No such luck.\n\n    \nreturn\n\n\n}\n\n\nwriteboost_usage\n()\n \n{\n\n    cat \nEND\n\n\nusage: $0 {start|stop|monitor|migrate_to|migrate_from|validate-all|meta-data}\n\n\n\nExpects to have a fully populated OCF RA-compliant environment set.\n\n\nEND\n\n\n}\n\n\nwriteboost_start\n()\n \n{\n\n    /sbin/writeboost\n    touch \n${\nOCF_RESKEY_state\n}\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\nwriteboost_stop\n()\n \n{\n\n    /sbin/writeboost -u\n    rm \n${\nOCF_RESKEY_state\n}\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\nwriteboost_monitor\n()\n \n{\n\n    \n# Monitor _MUST!_ differentiate correctly between running\n\n    \n# (SUCCESS), failed (ERROR) or _cleanly_ stopped (NOT RUNNING).\n\n    \n# That is THREE states, not just yes/no.\n\n\n    \nif\n \n[\n \n$OCF_RESKEY_op_sleep\n -ne \n0\n \n]\n;\n \nthen\n\n        \nif\n \n[\n -f \n${\nVERIFY_SERIALIZED_FILE\n}\n \n]\n;\n \nthen\n\n            \n# two monitor ops have occurred at the same time.\n\n            \n# this is to verify a condition in the lrmd regression tests.\n\n            ocf_log err \n$VERIFY_SERIALIZED_FILE\n exists already\n\n            \nreturn\n \n$OCF_ERR_GENERIC\n\n        \nfi\n\n\n        touch \n${\nVERIFY_SERIALIZED_FILE\n}\n\n        sleep \n${\nOCF_RESKEY_op_sleep\n}\n\n        rm \n${\nVERIFY_SERIALIZED_FILE\n}\n\n    \nfi\n\n\n    \nif\n \n[\n -f \n${\nOCF_RESKEY_state\n}\n \n]\n;\n \nthen\n\n        \nreturn\n \n$OCF_SUCCESS\n\n    \nfi\n\n    \nif\n \nfalse\n \n;\n \nthen\n\n        \nreturn\n \n$OCF_ERR_GENERIC\n\n    \nfi\n\n    \nreturn\n \n$OCF_NOT_RUNNING\n\n\n}\n\n\nwriteboost_validate\n()\n \n{\n\n\n    \n# Is the state directory writable? \n\n    \nstate_dir\n=\n`\ndirname \n$OCF_RESKEY_state\n`\n\n    touch \n$state_dir\n/\n$$\n\n    \nif\n \n[\n \n$?\n !\n=\n \n0\n \n]\n;\n \nthen\n\n    \nreturn\n \n$OCF_ERR_ARGS\n\n    \nfi\n\n    rm \n$state_dir\n/\n$$\n\n\n    \nreturn\n \n$OCF_SUCCESS\n\n\n}\n\n\n: \n${\nOCF_RESKEY_fake\n=writeboost\n}\n\n: \n${\nOCF_RESKEY_op_sleep\n=0\n}\n\n: \n${\nOCF_RESKEY_CRM_meta_interval\n=0\n}\n\n: \n${\nOCF_RESKEY_CRM_meta_globally_unique\n:=\ntrue\n}\n\n\n\nif\n \n[\n \nx\n$OCF_RESKEY_state\n \n=\n \nx\n \n]\n;\n \nthen\n\n    \nif\n \n[\n \n${\nOCF_RESKEY_CRM_meta_globally_unique\n}\n \n=\n \nfalse\n \n]\n;\n \nthen\n\n    \nstate\n=\n${\nHA_VARRUN\n}\n/WRITEBOOST-\n${\nOCF_RESOURCE_INSTANCE\n}\n.state\n\n\n    \n# Strip off the trailing clone marker\n\n    \nOCF_RESKEY_state\n=\n`\necho\n \n$state\n \n|\n sed s/:\n[\n0\n-9\n][\n0\n-9\n]\n*\n\\.\nstate/.state/\n`\n\n    \nelse\n \n    \nOCF_RESKEY_state\n=\n${\nHA_VARRUN\n}\n/WRITEBOOST-\n${\nOCF_RESOURCE_INSTANCE\n}\n.state\n\n    \nfi\n\n\nfi\n\n\nVERIFY_SERIALIZED_FILE\n=\n${\nOCF_RESKEY_state\n}\n.serialized\n\n\n\ncase\n \n$__OCF_ACTION\n in\nmeta-data\n)\n  meta_data\n        \nexit\n \n$OCF_SUCCESS\n\n        \n;;\n\nstart\n)\n      writeboost_start\n;;\n\nstop\n)\n       writeboost_stop\n;;\n\nmonitor\n)\n    writeboost_monitor\n;;\n\nmigrate_to\n)\n ocf_log info \nMigrating \n${\nOCF_RESOURCE_INSTANCE\n}\n to \n${\nOCF_RESKEY_CRM_meta_migrate_target\n}\n.\n\n            writeboost_stop\n        \n;;\n\nmigrate_from\n)\n   ocf_log info \nMigrating \n${\nOCF_RESOURCE_INSTANCE\n}\n to \n${\nOCF_RESKEY_CRM_meta_migrate_source\n}\n.\n\n            writeboost_start\n        \n;;\n\nreload\n)\n     ocf_log err \nReloading...\n\n            writeboost_start\n        \n;;\n\nvalidate-all\n)\n   writeboost_validate\n;;\n\nusage\n|\nhelp\n)\n writeboost_usage\n        \nexit\n \n$OCF_SUCCESS\n\n        \n;;\n\n*\n)\n      writeboost_usage\n        \nexit\n \n$OCF_ERR_UNIMPLEMENTED\n\n        \n;;\n\n\nesac\n\n\nrc\n=\n$?\n\nocf_log debug \n${\nOCF_RESOURCE_INSTANCE\n}\n \n$__OCF_ACTION\n : \n$rc\n\n\nexit\n \n$rc", 
            "title": "Writeboost"
        }, 
        {
            "location": "/clusters/live-migration/", 
            "text": "Live Migration\n\n\nIn this page, you are going to see how VM live migration works. We have  an hypervisor cluster and a NAS cluster. The NAS cluster is sharing the VM virtual disk through an NFS server while the hypervisor cluster is running the VM. The connection that you can see in the video is done via RDP (directly to the guest). The guest is playing a youtube video to demonstrate how the migration is done and how much is the client experience affected.\n\n\n1- Hypervisor migration\n\n\nHere we will do a virtual domain resource move within the hypervisors cluster. The hypervisor cluster has a shared storage exported from nas cluster and during the move the guest os will almos not notice it has been moved from running in one hypervisor to another one.\n\n\n          HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|  (vm)  |          |        |               |        |           |  (vm)  |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n    |                    ^\n    |                    |\n    +--------------------+\n\n\n\n\n\n\n\n\n2- NAS migration\n\n\nHere we will do a nfs storage cluster resource move from one nas to another while we keep a guest os running on one hypervisor node. \n\n\n          HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|        |          |  (vm)  |               |        |           |  (vm)  |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n                                                 ^                    |\n                                                 |                    |\n                                                 +--------------------+\n\n\n\n\n\n\n\n\n3- Hypervisor and NAS migration\n\n\nPretty much impressive we do a live virtual domain resource move from running in one hypervisor to another while we move the shared nfs storage from one nas to another at the same time. Some freeze seconds can be seen on user guest os but the guest continues working as if nothing has happened.\n\n\n          HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|        |          |  (vm)  |               |  (vm)  |           |        |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n    ^                    |                       |                    ^\n    |                    |                       |                    |\n    +--------------------+                       +--------------------+\n\n\n\n\n\n\n\n\nConclusions\n\n\nWe used two clusters, both set up with pacemaker monitoring resources.\n\n\nOn the storage cluster we used drbd primary/secondary configuration with nfs4 server exports and floating ips. On the hypervisors cluster we have virtual domain resources that will be able to be live migrated. This 'magic' is done by committing the ram memory that will be copied to the new hypervisor while the guest is still running and when it finishes it just pauses the virtual domain for some cents of milliseconds while the ram increment is finally copied. The domain is then started on the other hypervisor with the same memory state and it can continue.\n\n\nThis will allow us to get a high availability cluster where both hypervisors and storage can allow a node to fail without even noticing on clients.", 
            "title": "Live Migration"
        }, 
        {
            "location": "/clusters/live-migration/#live-migration", 
            "text": "In this page, you are going to see how VM live migration works. We have  an hypervisor cluster and a NAS cluster. The NAS cluster is sharing the VM virtual disk through an NFS server while the hypervisor cluster is running the VM. The connection that you can see in the video is done via RDP (directly to the guest). The guest is playing a youtube video to demonstrate how the migration is done and how much is the client experience affected.", 
            "title": "Live Migration"
        }, 
        {
            "location": "/clusters/live-migration/#1-hypervisor-migration", 
            "text": "Here we will do a virtual domain resource move within the hypervisors cluster. The hypervisor cluster has a shared storage exported from nas cluster and during the move the guest os will almos not notice it has been moved from running in one hypervisor to another one.            HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|  (vm)  |          |        |               |        |           |  (vm)  |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n    |                    ^\n    |                    |\n    +--------------------+", 
            "title": "1- Hypervisor migration"
        }, 
        {
            "location": "/clusters/live-migration/#2-nas-migration", 
            "text": "Here we will do a nfs storage cluster resource move from one nas to another while we keep a guest os running on one hypervisor node.             HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|        |          |  (vm)  |               |        |           |  (vm)  |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n                                                 ^                    |\n                                                 |                    |\n                                                 +--------------------+", 
            "title": "2- NAS migration"
        }, 
        {
            "location": "/clusters/live-migration/#3-hypervisor-and-nas-migration", 
            "text": "Pretty much impressive we do a live virtual domain resource move from running in one hypervisor to another while we move the shared nfs storage from one nas to another at the same time. Some freeze seconds can be seen on user guest os but the guest continues working as if nothing has happened.            HYPERVISORS                                      NAS\n\n+--------+          +--------+               +--------+           +--------+\n|        |          |        |               |        |           |        |\n| vnode2 |          | vnode3 |               |  nas1  |           |  nas2  |\n|        |          |        |               |        |           |        |\n|        |          |        |               |        |           |        |\n|        |          |  (vm)  |               |  (vm)  |           |        |\n|        |          |        |               |        |           |        |\n+---+----+          +----+---+               +--------+           +--------+\n    ^                    |                       |                    ^\n    |                    |                       |                    |\n    +--------------------+                       +--------------------+", 
            "title": "3- Hypervisor and NAS migration"
        }, 
        {
            "location": "/clusters/live-migration/#conclusions", 
            "text": "We used two clusters, both set up with pacemaker monitoring resources.  On the storage cluster we used drbd primary/secondary configuration with nfs4 server exports and floating ips. On the hypervisors cluster we have virtual domain resources that will be able to be live migrated. This 'magic' is done by committing the ram memory that will be copied to the new hypervisor while the guest is still running and when it finishes it just pauses the virtual domain for some cents of milliseconds while the ram increment is finally copied. The domain is then started on the other hypervisor with the same memory state and it can continue.  This will allow us to get a high availability cluster where both hypervisors and storage can allow a node to fail without even noticing on clients.", 
            "title": "Conclusions"
        }, 
        {
            "location": "/virtualization/concepts/", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/virtualization/disks/", 
            "text": "", 
            "title": "Disk images"
        }, 
        {
            "location": "/virtualization/gpus/", 
            "text": "", 
            "title": "GPUs"
        }, 
        {
            "location": "/setups/ha/active_passive/", 
            "text": "Cluster Active-Pasive\n\n\nThis setup is based on two servers (NAS1 \n NAS2), both with similar \nhardware and Centos 7, that will provide high availability nfs storage \nusing software raids and drbd 8. We will make use of EnhanceIO disk\ncache with Intel NVME disk.\n\n\nNETWORK\n\n\nNAS1 network device names\n\n\n\n\nescola: access interface\n\n\nnas: nfs exported storage\n\n\ndrbd: storage synchronization\n\n\n\n\ncat /etc/udev/rules.d/70-persistent-net.rules\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\n40:8d:5c:1e:0e:0c\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\nescola\n\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\na0:36:9f:6e:18:c0\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\nnas\n\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\na0:36:9f:6e:18:c2\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\ndrbd\n\n\n\n\n\n\nNAS2 network device names\n\n\ncat /etc/udev/rules.d/70-persistent-net.rules\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\n40:8d:5c:1e:1c:54\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\nescola\n\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\na0:36:9f:34:0a:c4\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\nnas\n\nSUBSYSTEM==\nnet\n, ACTION==\nadd\n, DRIVERS==\n?*\n, ATTR{address}==\na0:36:9f:34:0a:c6\n, ATTR{type}==\n1\n, KERNEL==\ne*\n, NAME=\ndrbd\n\n\n[root@nas2 network-scripts]# cat ifcfg-escola\nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nyes\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nNAME=\nescola\n\nONBOOT=\nyes\n\nHWADDR=\n40:8d:5c:1e:1c:54\n\nPEERDNS=\nyes\n\nPEERROUTES=\nyes\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nIPADDR=\n10.1.1.32\n\nPREFIX=\n24\n\nGATEWAY=\n10.1.1.199\n\nDNS1=\n10.1.1.200\n\nDNS2=\n10.1.1.201\n\nDOMAIN=\nescoladeltreball.org\n\n\n[root@nas2 network-scripts]# cat ifcfg-nas\nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nno\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nNAME=\ndrbd\n\nONBOOT=\nyes\n\nHWADDR=A0:36:9F:34:0A:C4\nPEERDNS=\nno\n\nPEERROUTES=\nno\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nIPADDR=\n10.1.2.32\n\nPREFIX=\n24\n\nMTU=\n9000\n\n\n[root@nas2 network-scripts]# cat ifcfg-drbd\nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nno\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nNAME=\ndrbd\n\nONBOOT=\nyes\n\nHWADDR=A0:36:9F:34:0A:C6\nPEERDNS=\nno\n\nPEERROUTES=\nno\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nIPADDR=\n10.1.3.32\n\nPREFIX=\n24\n\nMTU=\n9000\n\n\n\n\n\n\nOS base installation\n\n\ndnf update -y\nvi /etc/hostname\ndnf install ethtool pciutils fio smartmontools wget unzip tar mdadm net-tools\n\ndnf install tuned\nsystemctl start tuned\nsystemctl enable tuned\ntuned-adm profile throughput-performance\n\n\n\n\n\nCommand line for intel NVMe \n s3700\n\n\nhttp://www.intel.com/support/ssdc/hpssd/sb/CS-035687.htm\nwget https://downloadcenter.intel.com/downloads/eula/23931/Intel-Solid-State-Drive-Data-Center-Tool?httpDown=https%3A%2F%2Fdownloadmirror.intel.com%2F23931%2Feng%2FDataCenterTool_2_3_0_Linux.zip\nunzip DataCenterTool_2_3_0_Linux.zip\nrpm -Uvh isdct-2.3.0.400-13.x86_64.rpm\n\n\n\n\n\nSample commands to check NVME\n\n\nisdct show -intelssd\nisdct show -sensor -intelssd 1\n\n\n\n\n\nInitialize disks\n\n\ndd if=/dev/zero of=/dev/sdb bs=4k count=1024\ndd if=/dev/zero of=/dev/sdc bs=4k count=1024\ndd if=/dev/zero of=/dev/sdd bs=4k count=1024\ndd if=/dev/zero of=/dev/sde bs=4k count=1024\ndd if=/dev/zero of=/dev/sdf bs=4k count=1024\ndd if=/dev/zero of=/dev/sdg bs=4k count=1024\ndd if=/dev/zero of=/dev/nvme0n1 bs=4k count=1024\npartprobe\n\nWe could have used also: wipefs -a /dev/xxx\n\n\n\n\n\nNTP Network time server\n\n\nyum install ntp -y\nsystemctl start ntpd\nsystemctl status ntpd\nsystemctl enable ntpd\n\n\n\n\n\nHARD DISKS:\n\n\nThis is lsblk:\n\n\nNAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda               8:0    0  37,3G  0 disk\n\u251c\u2500sda1            8:1    0   200M  0 part /boot/efi\n\u251c\u2500sda2            8:2    0   500M  0 part /boot\n\u2514\u2500sda3            8:3    0  36,6G  0 part\n  \u251c\u2500fedora-root 253:0    0  32,9G  0 lvm  /\n  \u2514\u2500fedora-swap 253:1    0   3,7G  0 lvm  [SWAP]\nsdb               8:16   0   2,7T  0 disk\nsdc               8:32   0   2,7T  0 disk\nsdd               8:48   0   1,8T  0 disk\nsde               8:64   0   1,8T  0 disk\nsdf               8:80   0 465,8G  0 disk\nsdg               8:96   0  93,2G  0 disk\nnvme0n1         259:0    0 372,6G  0 disk\n\n\n\n\n\nHAD DISKS Partitions\n\n\n\n\nsdd \n sde: 7200rpm, 1,8TB\n\n\nsdb \n sdc: 5400rpm, 2,7TB\n\n\n\n\nScheme that we will create with raids (md) lvms and drbd.\nNOTE: We have lvs under and over drbd. That allow us to resize underlaying\nscheme and also redistribute storage over drbd. We also split storage to\nallow moving resources on both nodes.\n\n\nsdd1---\\\n        md1--\\           /---lvgroups (md1)-------drbd30------vg_groups-------lv_groups\nsde1---/      \\         /\n               vgdata--x-----lvtemplates----------drbd20------vg_templates----lv_templates\nsdb1---\\      /         \\\n        md2--/           \\---lvbases--------------drbd10------vg_bases--------lv_bases\nsdc1---/                  \\\n                           \\-lvoldgroups----------drbd35------vg_oldgroups----lv_oldgroups\n\n\nnvme0n1p1-\\                 /--lv_cachegroups-----drbd31------vgcache_groups-----lvcache_groups\n           \\               /\nsdf1--------x----vgcache--x----lv_cachetemplates--drbd21------vgcache_templates--lvcache_templates\n           /               \\\nsdg1------/                 \\--lv_cachebases------drbd11------vgcache_bases------lvcache_bases\n\nAnd this is the cache over the previous layout:\n\nlv_groups----------\\\n                    EnhanceIO:groups---------\n/vimet/groups\nlvcache_groups-----/\n\nlv_templates-------\\\n                    EnhanceIO:templates------\n/vimet/templates\nlvcache_templates--/\n\nlv_bases-----------\\\n                    EnhanceIO:bases----------\n/vimet/bases\nlvcache_bases------/\n\n\n\n\n\nLet's create partitions\n\n\nWe let an small partition on the beginning of the disks to do io tests.\n\n\nWith parted we can check partition alignment:\n\n\nprint free\nalign-check opt 1\n\n\n\n\n\nWe do create the partitions. We check sectors to be sure they start on\nthe beginning of a disk sector. This will maximize io. You should check\nyour disks with previous commands.\n\n\nparted /dev/sdb mklabel gpt\nparted /dev/sdc mklabel gpt\nparted /dev/sdd mklabel gpt\nparted /dev/sde mklabel gpt\nparted /dev/sdf mklabel gpt\nparted /dev/sdg mklabel gpt\nparted /dev/nvme0n1 mklabel gpt\nparted -a optimal /dev/sdb mkpart primary ext4 4096s 2980G\nparted -a optimal /dev/sdb mkpart primary ext4 2980G 100%\nparted -a optimal /dev/sdc mkpart primary ext4 4096s 2980GB\nparted -a optimal /dev/sdc mkpart primary ext4 2980GB 100%\nparted -a optimal /dev/sdd mkpart primary ext4 4096s 1980GB\nparted -a optimal /dev/sdd mkpart primary ext4 1980GB 100%\nparted -a optimal /dev/sde mkpart primary ext4 4096s 1980GB\nparted -a optimal /dev/sde mkpart primary ext4 1980GB 100%\nparted -a optimal /dev/sdf mkpart primary ext4 2048s 495G\nparted -a optimal /dev/sdf mkpart primary ext4 495GB 100%\nparted -a optimal /dev/sdg mkpart primary ext4 512 94GB\nparted -a optimal /dev/sdg mkpart primary ext4 94GB 99GB\nparted -a optimal /dev/sdg mkpart primary ext4 99GB 100%\nparted -a optimal /dev/nvme0n1 mkpart primary ext4 512 395GB\nparted -a optimal /dev/nvme0n1 mkpart primary ext4 395GB 100%\n\nreboot recommended\n\n\n\n\n\nRAIDS\n\n\nRaids will allow the firsh redundancy on system. We create level 1 raids\nthat will provide 2x read io speed and data duplication.\n\n\nmdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1\nmdadm --create /dev/md2 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1\nmdadm --detail --scan \n /etc/mdadm.conf\n\n\n\n\n\nLVMs non-clustered (local)\n\n\nDevice filtering\n\n\nWe need to set the devices where lvm will look for lvm signatures.\n\n\nfilter = [\na|sd.*|\n, \na|nvme.*|\n, \na|md.*|\n, \na|drbd.*|\n, \nr|.*|\n]\n\n\n\n\n\nScheme pv/vg/lv (under drbd)\n\n\npvcreate /dev/md1\npvcreate /dev/md2\n\nvgcreate -cn vgdata /dev/md1 /dev/md2\nlvcreate -l 100%FREE -n lvgroups vgdata /dev/md1\nlvcreate -L 1700G -n lvoldgroups vgdata /dev/md2\nlvcreate -l 100%FREE -n lvtemplates vgdata /dev/md2\n\npvcreate /dev/nvme0n1p1\npvcreate /dev/sdf1\npvcreate /dev/sdg1\nvgcreate vgcache /dev/nvme0n1p1 /dev/sdf1 /dev/sdg1\nlvcreate -l 100%FREE -n lvcachebases vgcache /dev/sdg1\nlvcreate -l 100%FREE -n lvcachetemplates vgcache /dev/sdg1\nlvcreate -l 100%FREE -n lvcachegroups vgcache /dev/nvme0n1p1\n\n\n\n\n\nDRBD\n\n\nInstall drbd 8\n\n\nyum install drbd drbd-utils drbd-udev drbd-pacemaker -y\nmodprobe drbd\nsystemctl enable drbd\n\n\n\n\n\nCreate drbd.conf and resource files.\n\n\nDRBD config files\n\n\n[root@nas1 ~]# cat /etc/drbd.conf \n\n\n# You can find an example in  /usr/share/doc/drbd.../drbd.conf.example\n\ninclude \ndrbd.d/global_common.conf\n;\ninclude \ndrbd.d/*.res\n;\n\n\n\n\n\n[root@nas1 ~]# cat /etc/drbd.d/global_common.conf \n\n\n# DRBD is the result of over a decade of development by LINBIT.\n# In case you need professional services for DRBD or have\n# feature requests visit http://www.linbit.com\n\nglobal {\n}\n\ncommon {\n        handlers {\n                fence-peer \n/usr/lib/drbd/crm-fence-peer.sh\n;\n                after-resync-target \n/usr/lib/drbd/crm-unfence-peer.sh\n;\n        }\n\n        startup {\n        }\n\n        options {\n        }\n\n        disk {\n                #al-extents 3389;\n                #disk-barrier no;\n                #disk-flushes no;\n                #fencing resource-and-stonith;\n                fencing resource-only;\n        }\n\n        net {\n                max-buffers 16000;\n                max-epoch-size 16000;\n                #unplug-watermark 16;\n                #sndbuf-size 2048;\n                allow-two-primaries;\n                after-sb-0pri discard-zero-changes;\n                after-sb-1pri discard-secondary;\n                after-sb-2pri disconnect;\n        }\n\n        syncer {\n         }\n}\n\n\n\n\n\n[root@nas1 ~]# cat /etc/drbd.d/bases.res \n\n\nresource bases {\n         startup {\n                #become-primary-on both;\n         }\n        volume 0 {\n                device    /dev/drbd11;\n                disk      /dev/vgcache/lvcachebases;\n                meta-disk internal;\n        }\n\n        volume 1 {\n                device    /dev/drbd10;\n                disk      /dev/vgdata/lvbases;\n                meta-disk internal;\n        }\n    on nas1 {\n         address   10.1.3.31:7810;\n    }\n    on nas2 {\n         address   10.1.3.32:7810;\n    }\n}\n\n\n\n\n\n[root@nas1 ~]# cat /etc/drbd.d/templates.res \n\n\nresource templates {\n         startup {\n                #become-primary-on both;\n         }\n   volume 0 {\n         device    /dev/drbd21;\n         disk      /dev/vgcache/lvcachetemplates;\n         meta-disk internal;\n   }\n   volume 1 {\n         device    /dev/drbd20;\n         disk      /dev/vgdata/lvtemplates;\n         meta-disk internal;\n   }\n    on nas1 {\n         address   10.1.3.31:7820;\n    }\n    on nas2 {\n         address   10.1.3.32:7820;\n    }\n}\n\n\n\n\n\n[root@nas1 ~]# cat /etc/drbd.d/groups.res \n\n\nresource groups {\n         startup {\n                #become-primary-on both;\n         }\n        volume 0 {\n                device    /dev/drbd31;\n                disk      /dev/vgcache/lvcachegroups;\n                meta-disk internal;\n        }\n\n        volume 1 {\n                device    /dev/drbd30;\n                disk      /dev/vgdata/lvgroups;\n                meta-disk /dev/sdg3;\n        }\n\n        volume 2 {\n                device    /dev/drbd35;\n                disk      /dev/vgdata/lvoldgroups;\n                meta-disk internal;\n        }\n    on nas1 {\n         address   10.1.3.31:7830;\n    }\n    on nas2 {\n         address   10.1.3.32:7830;\n    }\n}\n\n\n\n\n\nNow we can create drbd resources.\n\n\ndrbdadm create-md bases\ndrbdadm create-md templates\ndrbdadm create-md groups\ndrbdadm up bases\ndrbdadm up templates\ndrbdadm up grups\ndrbdadm primary bases --force\ndrbdadm primary templates --force\ndrbdadm primary grups --force\n\ndrbdadm disk-options --resync-rate=400M \nresource\n\n\n\n\n\n\nLVM Nested volumes (now over drbd)\n\n\nThis lvm setup has to be done in only one node:\n\n\nGroups\n\n\npvcreate /dev/drbd30\npvcreate /dev/drbd31\nvgcreate vgcache_groups /dev/drbd31\nlvcreate -l 100%FREE -n lvcache_groups vgcache_groups\nlvcreate -l 100%FREE -n lvcache_templates vgcache_templates\nvgcreate vg_groups /dev/drbd20\nlvcreate -l 100%FREE -n lv_groups vg_groups\nvgchange -ay vg_groups\n\n\n\n\n\nTemplates\n\n\npvcreate /dev/drbd20\npvcreate /dev/drbd21\nvgcreate vgcache_templates /dev/drbd21\nlvcreate -l 100%FREE -n lvcache_templates vgcache_templates\nvgcreate vg_templates /dev/drbd20\nlvcreate -l 100%FREE -n lv_templates vg_templates\nvgchange -ay vg_templates\n\n\n\n\n\nBases\n\n\npvcreate /dev/drbd10\npvcreate /dev/drbd11\nvgcreate vgcache_bases /dev/drbd11\nlvcreate -l 100%FREE -n lvcache_bases vgcache_bases\nvgcreate vg_bases /dev/drbd10\nlvcreate -l 100%FREE -n lv_bases vg_bases\nvgchange -ay vg_templates\n\n\n\n\n\nOldGroups\n\n\npvcreate /dev/drbd35\nvgcreate vg_oldgroups /dev/drbd35\nlvcreate -l 100%FREE -n lv_oldgroups vg_oldgroups\n\n\n\n\n\nFilesystem\n\n\nmkfs.ext4 /dev/vg_bases/lv_bases\nmkfs.ext4 /dev/vg_templates/lv_templates\nmkfs.ext4 /dev/vg_groups/lv_groups\nmkfs.ext4 /dev/vg_oldgroups/lv_oldgroups\n\n\n\n\n\n** Now you can try to mount lvs to check everything was fine.\n\n\nEnhanceIO Disk cache\n\n\nFirst we install it:\n\n\ndnf install kernel-devel gcc\ngit clone https://github.com/stec-inc/EnhanceIO.git\ncd EnhanceIO/Driver/enhanceio\nmake \n make install\n\ncd /root/EnhanceIO/CLI\n\n\n\n\n\nApply patch for kernels \n3:\n\n\ndiff --git a/CLI/eio_cli b/CLI/eio_cli\nindex 3453b35..a0b60c5 100755\n--- a/CLI/eio_cli\n+++ b/CLI/eio_cli\n@@ -1,4 +1,4 @@\n-#!/usr/bin/python\n+#!/usr/bin/python2\n #\n # Copyright (C) 2012 STEC, Inc. All rights not specifically granted\n # under a license included herein are reserved\n@@ -305,10 +305,12 @@ class Cache_rec(Structure):\n\n        def do_eio_ioctl(self,IOC_TYPE):\n                #send ioctl to driver\n-               fd = open(EIODEV, \nr\n)\n+               libc = CDLL(\nlibc.so.6\n)\n+               fd = os.open (EIODEV, os.O_RDWR, 0400)\n                fmt = \n\n+               selfaddr = c_uint64(addressof(self))\n                try:\n-                       if ioctl(fd, IOC_TYPE, addressof(self)) == SUCCESS:\n+                       if libc.ioctl(fd, IOC_TYPE, selfaddr) == SUCCESS:\n                                return SUCCESS\n                except Exception as e:\n                        print e\n\n\n\n\n\nCopy binary to path folder and install modules in kernel:\n\n\ncp EnhanceIO/CLI/eio_cli /sbin\n\nmodprobe enhanceio\nmodprobe enhanceio_fifo  x\nmodprobe enhanceio_lru\nmodprobe enhanceio_rand  x\n\n\n\n\n\nPersistent modules install:\n\n\nvi /etc/modules-load.d/enhanceio.conf\n    enhanceio\n    enhanceio_lru\n\n\n\n\n\nTo stop cache device we have to put it in read only and wait till all\ndirty sectors from nvme are layered down to rotational hard disks.\n\n\neio_cli edit -c data -m ro\n\n\n\n\n\nAvoid kernel updates as it will break enhanceio and you could lose all\nyour data!.\n\n\nvi /etc/dnf/dnf.conf\n    exclude=kernel*\n\n\n\n\n\nWe should create caches ONLY on primary drbd node and copy udev rules\ncreated to the other nas. (/etc/udev/rules.d/94...):\n\n\neio_cli create -d /dev/vg_groups/lv_groups -s /dev/vgcache_groups/lvcache_groups -p lru -m wb -c groups\neio_cli create -d /dev/vg_templates/lv_templates -s /dev/vgcache_templates/lvcache_templates -p lru -m wb -c templates\neio_cli create -d /dev/vg_bases/lv_bases -s /dev/vgcache_bases/lvcache_bases -p lru -m wt -c bases\n\n\n\n\n\nCheck cache status:\n\n\neio_cli info\ncat /proc/enhanceio/\ncache_name\n/stats\n\n\n\n\n\nPACEMAKER Cluster\n\n\nPacemaker will keep our resources under monitoring and will restart it on\nthe other node in case of failure.\n\n\ndnf install corosync pacemaker pcs -y\nsystemctl enable pcsd; systemctl enable corosync; systemctl enable pacemaker;\nsystemctl start pcsd;\n#passwd hacluster  (habitual)\npcs cluster auth nas1 nas2  (hacluster/habitual)\npcs cluster setup --name vimet_cluster nas1 nas2\npcs cluster start --all\npcs status\n\npcs property set default-resource-stickiness=200\npcs property set no-quorum-policy=ignore\n\n\n\n\n\nTo remove an existing cluster:\n\n\npcs cluster stop --all\npcs cluster destroy \nrm -rf /var/lib/pcsd/*\nreboot\n\n\n\n\n\nFence agents\n\n\nInstall fence agents. A pacemaker setup CAN'T work without a fencing\ndevice. We used APC stonith device:\n\n\nyum install python-pycurl fence-agents-apc fence-agents-apc-snmp -y\n\n\n\n\n\nFence agent (stonith) pacemaker resource definition:\n\n\npcs cluster cib fence_cfg\npcs -f fence_cfg stonith create stonith fence_apc_snmp params ipaddr=10.1.1.4 pcmk_host_list=\nnas1,nas2\n pcmk_host_map=\nnas1:3;nas2:4\n pcmk_host_check=static-list power_wait=5 inet4only debug=/var/log/stonith.log retry_on=10\npcs cluster cib-push fence_cfg\n\npcs property set stonith-enabled=true\npcs property set start-failure-is-fatal=false\n\n\n\n\n\nDRBD resources\n\n\npcs cluster cib drbd_bases_cfg\npcs -f drbd_bases_cfg resource create drbd_bases ocf:linbit:drbd drbd_resource=bases op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_bases_cfg resource master drbd_bases-clone drbd_bases master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_bases_cfg\n\npcs cluster cib drbd_templates_cfg\npcs -f drbd_templates_cfg resource create drbd_templates ocf:linbit:drbd drbd_resource=templates op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_templates_cfg resource master drbd_templates-clone drbd_templates master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_templates_cfg\n\npcs cluster cib drbd_groups_cfg\npcs -f drbd_groups_cfg resource create drbd_groups ocf:linbit:drbd drbd_resource=groups op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_groups_cfg resource master drbd_groups-clone drbd_groups master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_groups_cfg\n\n\n\n\n\nLVM resources\n\n\npcs resource create lv_bases ocf:heartbeat:LVM volgrpname=vg_bases op monitor interval=30s\npcs resource create lv_templates ocf:heartbeat:LVM volgrpname=vg_templates op monitor interval=30s\npcs resource create lv_groups ocf:heartbeat:LVM volgrpname=vg_groups op monitor interval=30s\npcs resource create lv_oldgroups ocf:heartbeat:LVM volgrpname=vg_oldgroups op monitor interval=30s\npcs resource create lvcache_bases ocf:heartbeat:LVM volgrpname=vgcache_bases op monitor interval=30s\npcs resource create lvcache_templates ocf:heartbeat:LVM volgrpname=vgcache_templates op monitor interval=30s\npcs resource create lvcache_groups ocf:heartbeat:LVM volgrpname=vgcache_groups op monitor interval=30s\n\n\n\n\n\nFilesystem ext4 and mountpoints\n\n\npcs resource create ext4_bases Filesystem device=\n/dev/vg_bases/lv_bases\n directory=\n/vimet/bases\n fstype=\next4\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s\npcs resource create ext4_templates Filesystem device=\n/dev/vg_templates/lv_templates\n directory=\n/vimet/templates\n fstype=\next4\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s\npcs resource create ext4_groups Filesystem device=\n/dev/vg_groups/lv_groups\n directory=\n/vimet/groups\n fstype=\next4\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s\npcs resource create ext4_oldgroups Filesystem device=\n/dev/vg_oldgroups/lv_oldgroups\n directory=\n/vimet/oldgroups\n fstype=\next4\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s\n\n\n\n\n\nNFS server\n\n\nVersion 4\n\n\nNFS: server and root\n\n\npcs cluster cib nfsserver_cfg\npcs -f nfsserver_cfg resource create nfs-daemon systemd:nfs-server \\\nnfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true op monitor interval=30s \\\n--group nfs_server\npcs -f nfsserver_cfg resource create nfs-root exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,crossmnt,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash \\\ndirectory=/vimet \\\nfsid=0 \\\n--group nfs_server\npcs cluster cib-push nfsserver_cfg\npcs resource clone nfs_server master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\n\n\n\n\nNFS: Exports\n\n\npcs cluster cib exports_cfg\npcs -f exports_cfg resource create nfs_bases exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/bases \\\nfsid=11 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_templates exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/templates \\\nfsid=20 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_groups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,async,mountpoint,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/groups \\\nfsid=30 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_oldgroups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,async,mountpoint,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/oldgroups \\\nfsid=35 \\\nop monitor interval=30s\npcs cluster cib-push exports_cfg\n\n\n\n\n\nFloating IPs\n\n\nThose IPs will 'move' always with assigned resources.\n\n\npcs resource create IPbases ocf:heartbeat:IPaddr2 ip=10.1.2.210 cidr_netmask=32 nic=nas:0  op monitor interval=30 \npcs resource create IPtemplates ocf:heartbeat:IPaddr2 ip=10.1.2.211 cidr_netmask=32 nic=nas:0  op monitor interval=30 \npcs resource create IPgroups ocf:heartbeat:IPaddr2 ip=10.1.2.212 cidr_netmask=32 nic=nas:0 op monitor interval=30\npcs resource create IPvbases ocf:heartbeat:IPaddr2 ip=10.1.1.28 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create IPvtemplates ocf:heartbeat:IPaddr2 ip=10.1.1.29 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create IPvgroups ocf:heartbeat:IPaddr2 ip=10.1.1.30 cidr_netmask=32 nic=escola:0 op monitor interval=30\n\n\n\n\n\nGroups and constraints\n\n\nGroups will allow starting and stopping resources in order.\nConstraings will allow restriction definitions.\n\n\nbases\n\n\npcs resource group add bases \\\n    lvcache_bases lv_bases ext4_bases nfs_bases IPbases IPvbases \n\npcs constraint order \\\n    promote drbd_bases-clone then bases INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_bases\n\npcs constraint colocation add \\\n    bases with master drbd_bases-clone INFINITY \\\n    id=c_drbd_bases\n\npcs constraint order \\\n    nfs_server-clone then bases INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_bases\n\npcs constraint colocation add \\\n    bases with started nfs_server-clone INFINITY \\\n    id=c_nfs_bases\n\n\n\n\n\ntemplates\n\n\npcs resource group add templates \\\n    lvcache_templates lv_templates ext4_templates nfs_templates IPtemplates IPvtemplates \n\npcs constraint order \\\n    promote drbd_templates-clone then templates INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_templates\n\npcs constraint colocation add \\\n    templates with master drbd_templates-clone INFINITY \\\n    id=c_drbd_templates\n\npcs constraint order \\\n    nfs_server-clone then templates INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_templates\n\npcs constraint colocation add \\\n    templates with started nfs_server-clone INFINITY \\\n    id=c_nfs_templates\n\n\n\n\n\ngroups\n\n\npcs resource group add groups \\\n    lvcache_groups lv_groups lv_oldgroups ext4_groups ext4_oldgroups nfs_groups nfs_oldgroups IPgroups IPvgroups\n\npcs constraint order \\\n    promote drbd_groups-clone then groups INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_groups\n\npcs constraint colocation add \\\n    groups with master drbd_groups-clone INFINITY \\\n    id=c_drbd_groups\n\npcs constraint order \\\n    nfs_server-clone then groups INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_groups\n\npcs constraint colocation add \\\n    groups with started nfs_server-clone INFINITY \\\n    id=c_nfs_groups\n\n\n\n\n\nThis constraints will allow for resources to be put on a preferred node.\n\n\npcs constraint location lv_groups prefers nas2=50\npcs constraint location lv_oldgroups prefers nas2=50\npcs constraint location lv_bases prefers nas1=50\npcs constraint location lv_templates prefers nas1=50\n\n\n\n\n\nPacemaker utils\n\n\nManage/Unmanage resource\n\n\nUnmanage drbd resource.\n\n\npcs resource unmanage drbd_templates-clone\n\n\n\n\n\nLet the cluster control a drbd resource again:\n\n\npcs resource meta drbd_templates-clone is-managed=true\n\n\n\n\n\nResize volumes under drbd\n\n\nStop cluster and start drbd manually.\n\n\npcs cluster stop/standby --all\nmodprobe drbd\n\n\n\n\n\nCheck that drbd resources are not active (cat /proc/drbd)\n\n\nCACHE: lvcachebases (ro), lvcachetemplates (wb), lvcachegroups (wb)\n\n\n1.- Cache set to read only\n\n\neio_cli edit -c groups -m ro\n\n\n\n\n\n2.- Wait till there are no dirty sectors (till it is 0)\n\n\ngrep nr_dirty /proc/enhanceio/groups/stats\n\n\n\n\n\n3.- Remove cache device\n\n\neio_cli delete -c groups\n\n\n\n\n\n4.- For example we do reduce lvcachegroups (600G: nvme0n1p1+/dev/sdf1)\n    and will grow lvcachetemplates to the new free space.\n\n\nShrink nvme0n1p1:\n\n\n\n\n\n        lvreduce -L 100G /dev/vgcache/lvcachegroups\n\n\n\n\n\nGrowing nvme0n1p1\n\n\n\n\n\n        lvresize -l 100%FREE -n /dev/vgcache/lvcachegroups /dev/nvme0n1p1\n\n\n\n\n\nExtend template cache volume to the free space we just created.\n\n\n\n\n\n        lvextend -l +100%FREE /dev/vgcache/lvcachetemplates\n\n\n\n\n\nNow drbd21 (resource groups volume 0) will be in 'diskless' state. This\nis normal as we broked drbd filesystem inside resized volumes.\n\n\nNow drbd21 (resource templates volume 0) will be synchronizing, as we\ngrowed it, but it still needs to resize manually.\n\n\nThis will be applied on reduced volumes:\n\n\nwipefs -a /dev/vgcache/lvcachegroups\ndrbdadm create-md groups/0\ndrbdadm up groups\ndrbdadm invalidate-remote groups/0\n\n\n\n\n\nNow we can create EnhanceIO groups cache again.\n\n\nThis will be applied on growed volumes:\n\n\ndrbdadm resize templates/0\n\n\n\n\n\nNow we can create EnhanceIO templates cache again.\n\n\nFor filesystems to be resized use resizefs for ext4.\n\n\nCredits:\n\n\nhttps://github.com/stec-inc/EnhanceIO\nhttps://www.suse.com/documentation/sle_ha/singlehtml/book_sleha_techguides/book_sleha_techguides.html\nhttps://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Power_Management_Guide/ALPM.html\n\n\n_\n_\n_\n_\n_\n____-", 
            "title": "Active-Pasive"
        }, 
        {
            "location": "/setups/ha/active_passive/#cluster-active-pasive", 
            "text": "This setup is based on two servers (NAS1   NAS2), both with similar \nhardware and Centos 7, that will provide high availability nfs storage \nusing software raids and drbd 8. We will make use of EnhanceIO disk\ncache with Intel NVME disk.", 
            "title": "Cluster Active-Pasive"
        }, 
        {
            "location": "/setups/ha/active_passive/#network", 
            "text": "", 
            "title": "NETWORK"
        }, 
        {
            "location": "/setups/ha/active_passive/#nas1-network-device-names", 
            "text": "escola: access interface  nas: nfs exported storage  drbd: storage synchronization   cat /etc/udev/rules.d/70-persistent-net.rules\nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== 40:8d:5c:1e:0e:0c , ATTR{type}== 1 , KERNEL== e* , NAME= escola \nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== a0:36:9f:6e:18:c0 , ATTR{type}== 1 , KERNEL== e* , NAME= nas \nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== a0:36:9f:6e:18:c2 , ATTR{type}== 1 , KERNEL== e* , NAME= drbd", 
            "title": "NAS1 network device names"
        }, 
        {
            "location": "/setups/ha/active_passive/#nas2-network-device-names", 
            "text": "cat /etc/udev/rules.d/70-persistent-net.rules\nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== 40:8d:5c:1e:1c:54 , ATTR{type}== 1 , KERNEL== e* , NAME= escola \nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== a0:36:9f:34:0a:c4 , ATTR{type}== 1 , KERNEL== e* , NAME= nas \nSUBSYSTEM== net , ACTION== add , DRIVERS== ?* , ATTR{address}== a0:36:9f:34:0a:c6 , ATTR{type}== 1 , KERNEL== e* , NAME= drbd \n\n[root@nas2 network-scripts]# cat ifcfg-escola\nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= yes \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nNAME= escola \nONBOOT= yes \nHWADDR= 40:8d:5c:1e:1c:54 \nPEERDNS= yes \nPEERROUTES= yes \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nIPADDR= 10.1.1.32 \nPREFIX= 24 \nGATEWAY= 10.1.1.199 \nDNS1= 10.1.1.200 \nDNS2= 10.1.1.201 \nDOMAIN= escoladeltreball.org \n\n[root@nas2 network-scripts]# cat ifcfg-nas\nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= no \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nNAME= drbd \nONBOOT= yes \nHWADDR=A0:36:9F:34:0A:C4\nPEERDNS= no \nPEERROUTES= no \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nIPADDR= 10.1.2.32 \nPREFIX= 24 \nMTU= 9000 \n\n[root@nas2 network-scripts]# cat ifcfg-drbd\nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= no \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nNAME= drbd \nONBOOT= yes \nHWADDR=A0:36:9F:34:0A:C6\nPEERDNS= no \nPEERROUTES= no \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nIPADDR= 10.1.3.32 \nPREFIX= 24 \nMTU= 9000", 
            "title": "NAS2 network device names"
        }, 
        {
            "location": "/setups/ha/active_passive/#os-base-installation", 
            "text": "dnf update -y\nvi /etc/hostname\ndnf install ethtool pciutils fio smartmontools wget unzip tar mdadm net-tools\n\ndnf install tuned\nsystemctl start tuned\nsystemctl enable tuned\ntuned-adm profile throughput-performance", 
            "title": "OS base installation"
        }, 
        {
            "location": "/setups/ha/active_passive/#command-line-for-intel-nvme-s3700", 
            "text": "http://www.intel.com/support/ssdc/hpssd/sb/CS-035687.htm\nwget https://downloadcenter.intel.com/downloads/eula/23931/Intel-Solid-State-Drive-Data-Center-Tool?httpDown=https%3A%2F%2Fdownloadmirror.intel.com%2F23931%2Feng%2FDataCenterTool_2_3_0_Linux.zip\nunzip DataCenterTool_2_3_0_Linux.zip\nrpm -Uvh isdct-2.3.0.400-13.x86_64.rpm", 
            "title": "Command line for intel NVMe &amp; s3700"
        }, 
        {
            "location": "/setups/ha/active_passive/#sample-commands-to-check-nvme", 
            "text": "isdct show -intelssd\nisdct show -sensor -intelssd 1", 
            "title": "Sample commands to check NVME"
        }, 
        {
            "location": "/setups/ha/active_passive/#initialize-disks", 
            "text": "dd if=/dev/zero of=/dev/sdb bs=4k count=1024\ndd if=/dev/zero of=/dev/sdc bs=4k count=1024\ndd if=/dev/zero of=/dev/sdd bs=4k count=1024\ndd if=/dev/zero of=/dev/sde bs=4k count=1024\ndd if=/dev/zero of=/dev/sdf bs=4k count=1024\ndd if=/dev/zero of=/dev/sdg bs=4k count=1024\ndd if=/dev/zero of=/dev/nvme0n1 bs=4k count=1024\npartprobe\n\nWe could have used also: wipefs -a /dev/xxx", 
            "title": "Initialize disks"
        }, 
        {
            "location": "/setups/ha/active_passive/#ntp-network-time-server", 
            "text": "yum install ntp -y\nsystemctl start ntpd\nsystemctl status ntpd\nsystemctl enable ntpd", 
            "title": "NTP Network time server"
        }, 
        {
            "location": "/setups/ha/active_passive/#hard-disks", 
            "text": "This is lsblk:  NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda               8:0    0  37,3G  0 disk\n\u251c\u2500sda1            8:1    0   200M  0 part /boot/efi\n\u251c\u2500sda2            8:2    0   500M  0 part /boot\n\u2514\u2500sda3            8:3    0  36,6G  0 part\n  \u251c\u2500fedora-root 253:0    0  32,9G  0 lvm  /\n  \u2514\u2500fedora-swap 253:1    0   3,7G  0 lvm  [SWAP]\nsdb               8:16   0   2,7T  0 disk\nsdc               8:32   0   2,7T  0 disk\nsdd               8:48   0   1,8T  0 disk\nsde               8:64   0   1,8T  0 disk\nsdf               8:80   0 465,8G  0 disk\nsdg               8:96   0  93,2G  0 disk\nnvme0n1         259:0    0 372,6G  0 disk", 
            "title": "HARD DISKS:"
        }, 
        {
            "location": "/setups/ha/active_passive/#had-disks-partitions", 
            "text": "sdd   sde: 7200rpm, 1,8TB  sdb   sdc: 5400rpm, 2,7TB   Scheme that we will create with raids (md) lvms and drbd.\nNOTE: We have lvs under and over drbd. That allow us to resize underlaying\nscheme and also redistribute storage over drbd. We also split storage to\nallow moving resources on both nodes.  sdd1---\\\n        md1--\\           /---lvgroups (md1)-------drbd30------vg_groups-------lv_groups\nsde1---/      \\         /\n               vgdata--x-----lvtemplates----------drbd20------vg_templates----lv_templates\nsdb1---\\      /         \\\n        md2--/           \\---lvbases--------------drbd10------vg_bases--------lv_bases\nsdc1---/                  \\\n                           \\-lvoldgroups----------drbd35------vg_oldgroups----lv_oldgroups\n\n\nnvme0n1p1-\\                 /--lv_cachegroups-----drbd31------vgcache_groups-----lvcache_groups\n           \\               /\nsdf1--------x----vgcache--x----lv_cachetemplates--drbd21------vgcache_templates--lvcache_templates\n           /               \\\nsdg1------/                 \\--lv_cachebases------drbd11------vgcache_bases------lvcache_bases\n\nAnd this is the cache over the previous layout:\n\nlv_groups----------\\\n                    EnhanceIO:groups--------- /vimet/groups\nlvcache_groups-----/\n\nlv_templates-------\\\n                    EnhanceIO:templates------ /vimet/templates\nlvcache_templates--/\n\nlv_bases-----------\\\n                    EnhanceIO:bases---------- /vimet/bases\nlvcache_bases------/", 
            "title": "HAD DISKS Partitions"
        }, 
        {
            "location": "/setups/ha/active_passive/#lets-create-partitions", 
            "text": "We let an small partition on the beginning of the disks to do io tests.  With parted we can check partition alignment:  print free\nalign-check opt 1  We do create the partitions. We check sectors to be sure they start on\nthe beginning of a disk sector. This will maximize io. You should check\nyour disks with previous commands.  parted /dev/sdb mklabel gpt\nparted /dev/sdc mklabel gpt\nparted /dev/sdd mklabel gpt\nparted /dev/sde mklabel gpt\nparted /dev/sdf mklabel gpt\nparted /dev/sdg mklabel gpt\nparted /dev/nvme0n1 mklabel gpt\nparted -a optimal /dev/sdb mkpart primary ext4 4096s 2980G\nparted -a optimal /dev/sdb mkpart primary ext4 2980G 100%\nparted -a optimal /dev/sdc mkpart primary ext4 4096s 2980GB\nparted -a optimal /dev/sdc mkpart primary ext4 2980GB 100%\nparted -a optimal /dev/sdd mkpart primary ext4 4096s 1980GB\nparted -a optimal /dev/sdd mkpart primary ext4 1980GB 100%\nparted -a optimal /dev/sde mkpart primary ext4 4096s 1980GB\nparted -a optimal /dev/sde mkpart primary ext4 1980GB 100%\nparted -a optimal /dev/sdf mkpart primary ext4 2048s 495G\nparted -a optimal /dev/sdf mkpart primary ext4 495GB 100%\nparted -a optimal /dev/sdg mkpart primary ext4 512 94GB\nparted -a optimal /dev/sdg mkpart primary ext4 94GB 99GB\nparted -a optimal /dev/sdg mkpart primary ext4 99GB 100%\nparted -a optimal /dev/nvme0n1 mkpart primary ext4 512 395GB\nparted -a optimal /dev/nvme0n1 mkpart primary ext4 395GB 100%\n\nreboot recommended", 
            "title": "Let's create partitions"
        }, 
        {
            "location": "/setups/ha/active_passive/#raids", 
            "text": "Raids will allow the firsh redundancy on system. We create level 1 raids\nthat will provide 2x read io speed and data duplication.  mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1\nmdadm --create /dev/md2 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1\nmdadm --detail --scan   /etc/mdadm.conf", 
            "title": "RAIDS"
        }, 
        {
            "location": "/setups/ha/active_passive/#lvms-non-clustered-local", 
            "text": "", 
            "title": "LVMs non-clustered (local)"
        }, 
        {
            "location": "/setups/ha/active_passive/#device-filtering", 
            "text": "We need to set the devices where lvm will look for lvm signatures.  filter = [ a|sd.*| ,  a|nvme.*| ,  a|md.*| ,  a|drbd.*| ,  r|.*| ]", 
            "title": "Device filtering"
        }, 
        {
            "location": "/setups/ha/active_passive/#scheme-pvvglv-under-drbd", 
            "text": "pvcreate /dev/md1\npvcreate /dev/md2\n\nvgcreate -cn vgdata /dev/md1 /dev/md2\nlvcreate -l 100%FREE -n lvgroups vgdata /dev/md1\nlvcreate -L 1700G -n lvoldgroups vgdata /dev/md2\nlvcreate -l 100%FREE -n lvtemplates vgdata /dev/md2\n\npvcreate /dev/nvme0n1p1\npvcreate /dev/sdf1\npvcreate /dev/sdg1\nvgcreate vgcache /dev/nvme0n1p1 /dev/sdf1 /dev/sdg1\nlvcreate -l 100%FREE -n lvcachebases vgcache /dev/sdg1\nlvcreate -l 100%FREE -n lvcachetemplates vgcache /dev/sdg1\nlvcreate -l 100%FREE -n lvcachegroups vgcache /dev/nvme0n1p1", 
            "title": "Scheme pv/vg/lv (under drbd)"
        }, 
        {
            "location": "/setups/ha/active_passive/#drbd", 
            "text": "Install drbd 8  yum install drbd drbd-utils drbd-udev drbd-pacemaker -y\nmodprobe drbd\nsystemctl enable drbd  Create drbd.conf and resource files.", 
            "title": "DRBD"
        }, 
        {
            "location": "/setups/ha/active_passive/#drbd-config-files", 
            "text": "[root@nas1 ~]# cat /etc/drbd.conf   # You can find an example in  /usr/share/doc/drbd.../drbd.conf.example\n\ninclude  drbd.d/global_common.conf ;\ninclude  drbd.d/*.res ;  [root@nas1 ~]# cat /etc/drbd.d/global_common.conf   # DRBD is the result of over a decade of development by LINBIT.\n# In case you need professional services for DRBD or have\n# feature requests visit http://www.linbit.com\n\nglobal {\n}\n\ncommon {\n        handlers {\n                fence-peer  /usr/lib/drbd/crm-fence-peer.sh ;\n                after-resync-target  /usr/lib/drbd/crm-unfence-peer.sh ;\n        }\n\n        startup {\n        }\n\n        options {\n        }\n\n        disk {\n                #al-extents 3389;\n                #disk-barrier no;\n                #disk-flushes no;\n                #fencing resource-and-stonith;\n                fencing resource-only;\n        }\n\n        net {\n                max-buffers 16000;\n                max-epoch-size 16000;\n                #unplug-watermark 16;\n                #sndbuf-size 2048;\n                allow-two-primaries;\n                after-sb-0pri discard-zero-changes;\n                after-sb-1pri discard-secondary;\n                after-sb-2pri disconnect;\n        }\n\n        syncer {\n         }\n}  [root@nas1 ~]# cat /etc/drbd.d/bases.res   resource bases {\n         startup {\n                #become-primary-on both;\n         }\n        volume 0 {\n                device    /dev/drbd11;\n                disk      /dev/vgcache/lvcachebases;\n                meta-disk internal;\n        }\n\n        volume 1 {\n                device    /dev/drbd10;\n                disk      /dev/vgdata/lvbases;\n                meta-disk internal;\n        }\n    on nas1 {\n         address   10.1.3.31:7810;\n    }\n    on nas2 {\n         address   10.1.3.32:7810;\n    }\n}  [root@nas1 ~]# cat /etc/drbd.d/templates.res   resource templates {\n         startup {\n                #become-primary-on both;\n         }\n   volume 0 {\n         device    /dev/drbd21;\n         disk      /dev/vgcache/lvcachetemplates;\n         meta-disk internal;\n   }\n   volume 1 {\n         device    /dev/drbd20;\n         disk      /dev/vgdata/lvtemplates;\n         meta-disk internal;\n   }\n    on nas1 {\n         address   10.1.3.31:7820;\n    }\n    on nas2 {\n         address   10.1.3.32:7820;\n    }\n}  [root@nas1 ~]# cat /etc/drbd.d/groups.res   resource groups {\n         startup {\n                #become-primary-on both;\n         }\n        volume 0 {\n                device    /dev/drbd31;\n                disk      /dev/vgcache/lvcachegroups;\n                meta-disk internal;\n        }\n\n        volume 1 {\n                device    /dev/drbd30;\n                disk      /dev/vgdata/lvgroups;\n                meta-disk /dev/sdg3;\n        }\n\n        volume 2 {\n                device    /dev/drbd35;\n                disk      /dev/vgdata/lvoldgroups;\n                meta-disk internal;\n        }\n    on nas1 {\n         address   10.1.3.31:7830;\n    }\n    on nas2 {\n         address   10.1.3.32:7830;\n    }\n}  Now we can create drbd resources.  drbdadm create-md bases\ndrbdadm create-md templates\ndrbdadm create-md groups\ndrbdadm up bases\ndrbdadm up templates\ndrbdadm up grups\ndrbdadm primary bases --force\ndrbdadm primary templates --force\ndrbdadm primary grups --force\n\ndrbdadm disk-options --resync-rate=400M  resource", 
            "title": "DRBD config files"
        }, 
        {
            "location": "/setups/ha/active_passive/#lvm-nested-volumes-now-over-drbd", 
            "text": "This lvm setup has to be done in only one node:", 
            "title": "LVM Nested volumes (now over drbd)"
        }, 
        {
            "location": "/setups/ha/active_passive/#groups", 
            "text": "pvcreate /dev/drbd30\npvcreate /dev/drbd31\nvgcreate vgcache_groups /dev/drbd31\nlvcreate -l 100%FREE -n lvcache_groups vgcache_groups\nlvcreate -l 100%FREE -n lvcache_templates vgcache_templates\nvgcreate vg_groups /dev/drbd20\nlvcreate -l 100%FREE -n lv_groups vg_groups\nvgchange -ay vg_groups", 
            "title": "Groups"
        }, 
        {
            "location": "/setups/ha/active_passive/#templates", 
            "text": "pvcreate /dev/drbd20\npvcreate /dev/drbd21\nvgcreate vgcache_templates /dev/drbd21\nlvcreate -l 100%FREE -n lvcache_templates vgcache_templates\nvgcreate vg_templates /dev/drbd20\nlvcreate -l 100%FREE -n lv_templates vg_templates\nvgchange -ay vg_templates", 
            "title": "Templates"
        }, 
        {
            "location": "/setups/ha/active_passive/#bases", 
            "text": "pvcreate /dev/drbd10\npvcreate /dev/drbd11\nvgcreate vgcache_bases /dev/drbd11\nlvcreate -l 100%FREE -n lvcache_bases vgcache_bases\nvgcreate vg_bases /dev/drbd10\nlvcreate -l 100%FREE -n lv_bases vg_bases\nvgchange -ay vg_templates", 
            "title": "Bases"
        }, 
        {
            "location": "/setups/ha/active_passive/#oldgroups", 
            "text": "pvcreate /dev/drbd35\nvgcreate vg_oldgroups /dev/drbd35\nlvcreate -l 100%FREE -n lv_oldgroups vg_oldgroups", 
            "title": "OldGroups"
        }, 
        {
            "location": "/setups/ha/active_passive/#filesystem", 
            "text": "mkfs.ext4 /dev/vg_bases/lv_bases\nmkfs.ext4 /dev/vg_templates/lv_templates\nmkfs.ext4 /dev/vg_groups/lv_groups\nmkfs.ext4 /dev/vg_oldgroups/lv_oldgroups  ** Now you can try to mount lvs to check everything was fine.", 
            "title": "Filesystem"
        }, 
        {
            "location": "/setups/ha/active_passive/#enhanceio-disk-cache", 
            "text": "First we install it:  dnf install kernel-devel gcc\ngit clone https://github.com/stec-inc/EnhanceIO.git\ncd EnhanceIO/Driver/enhanceio\nmake   make install\n\ncd /root/EnhanceIO/CLI  Apply patch for kernels  3:  diff --git a/CLI/eio_cli b/CLI/eio_cli\nindex 3453b35..a0b60c5 100755\n--- a/CLI/eio_cli\n+++ b/CLI/eio_cli\n@@ -1,4 +1,4 @@\n-#!/usr/bin/python\n+#!/usr/bin/python2\n #\n # Copyright (C) 2012 STEC, Inc. All rights not specifically granted\n # under a license included herein are reserved\n@@ -305,10 +305,12 @@ class Cache_rec(Structure):\n\n        def do_eio_ioctl(self,IOC_TYPE):\n                #send ioctl to driver\n-               fd = open(EIODEV,  r )\n+               libc = CDLL( libc.so.6 )\n+               fd = os.open (EIODEV, os.O_RDWR, 0400)\n                fmt =  \n+               selfaddr = c_uint64(addressof(self))\n                try:\n-                       if ioctl(fd, IOC_TYPE, addressof(self)) == SUCCESS:\n+                       if libc.ioctl(fd, IOC_TYPE, selfaddr) == SUCCESS:\n                                return SUCCESS\n                except Exception as e:\n                        print e  Copy binary to path folder and install modules in kernel:  cp EnhanceIO/CLI/eio_cli /sbin\n\nmodprobe enhanceio\nmodprobe enhanceio_fifo  x\nmodprobe enhanceio_lru\nmodprobe enhanceio_rand  x  Persistent modules install:  vi /etc/modules-load.d/enhanceio.conf\n    enhanceio\n    enhanceio_lru  To stop cache device we have to put it in read only and wait till all\ndirty sectors from nvme are layered down to rotational hard disks.  eio_cli edit -c data -m ro  Avoid kernel updates as it will break enhanceio and you could lose all\nyour data!.  vi /etc/dnf/dnf.conf\n    exclude=kernel*  We should create caches ONLY on primary drbd node and copy udev rules\ncreated to the other nas. (/etc/udev/rules.d/94...):  eio_cli create -d /dev/vg_groups/lv_groups -s /dev/vgcache_groups/lvcache_groups -p lru -m wb -c groups\neio_cli create -d /dev/vg_templates/lv_templates -s /dev/vgcache_templates/lvcache_templates -p lru -m wb -c templates\neio_cli create -d /dev/vg_bases/lv_bases -s /dev/vgcache_bases/lvcache_bases -p lru -m wt -c bases  Check cache status:  eio_cli info\ncat /proc/enhanceio/ cache_name /stats", 
            "title": "EnhanceIO Disk cache"
        }, 
        {
            "location": "/setups/ha/active_passive/#pacemaker-cluster", 
            "text": "Pacemaker will keep our resources under monitoring and will restart it on\nthe other node in case of failure.  dnf install corosync pacemaker pcs -y\nsystemctl enable pcsd; systemctl enable corosync; systemctl enable pacemaker;\nsystemctl start pcsd;\n#passwd hacluster  (habitual)\npcs cluster auth nas1 nas2  (hacluster/habitual)\npcs cluster setup --name vimet_cluster nas1 nas2\npcs cluster start --all\npcs status\n\npcs property set default-resource-stickiness=200\npcs property set no-quorum-policy=ignore", 
            "title": "PACEMAKER Cluster"
        }, 
        {
            "location": "/setups/ha/active_passive/#to-remove-an-existing-cluster", 
            "text": "pcs cluster stop --all\npcs cluster destroy \nrm -rf /var/lib/pcsd/*\nreboot", 
            "title": "To remove an existing cluster:"
        }, 
        {
            "location": "/setups/ha/active_passive/#fence-agents", 
            "text": "Install fence agents. A pacemaker setup CAN'T work without a fencing\ndevice. We used APC stonith device:  yum install python-pycurl fence-agents-apc fence-agents-apc-snmp -y  Fence agent (stonith) pacemaker resource definition:  pcs cluster cib fence_cfg\npcs -f fence_cfg stonith create stonith fence_apc_snmp params ipaddr=10.1.1.4 pcmk_host_list= nas1,nas2  pcmk_host_map= nas1:3;nas2:4  pcmk_host_check=static-list power_wait=5 inet4only debug=/var/log/stonith.log retry_on=10\npcs cluster cib-push fence_cfg\n\npcs property set stonith-enabled=true\npcs property set start-failure-is-fatal=false", 
            "title": "Fence agents"
        }, 
        {
            "location": "/setups/ha/active_passive/#drbd-resources", 
            "text": "pcs cluster cib drbd_bases_cfg\npcs -f drbd_bases_cfg resource create drbd_bases ocf:linbit:drbd drbd_resource=bases op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_bases_cfg resource master drbd_bases-clone drbd_bases master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_bases_cfg\n\npcs cluster cib drbd_templates_cfg\npcs -f drbd_templates_cfg resource create drbd_templates ocf:linbit:drbd drbd_resource=templates op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_templates_cfg resource master drbd_templates-clone drbd_templates master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_templates_cfg\n\npcs cluster cib drbd_groups_cfg\npcs -f drbd_groups_cfg resource create drbd_groups ocf:linbit:drbd drbd_resource=groups op monitor interval=15s role=Master op monitor interval=30s role=Slave\npcs -f drbd_groups_cfg resource master drbd_groups-clone drbd_groups master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_groups_cfg", 
            "title": "DRBD resources"
        }, 
        {
            "location": "/setups/ha/active_passive/#lvm-resources", 
            "text": "pcs resource create lv_bases ocf:heartbeat:LVM volgrpname=vg_bases op monitor interval=30s\npcs resource create lv_templates ocf:heartbeat:LVM volgrpname=vg_templates op monitor interval=30s\npcs resource create lv_groups ocf:heartbeat:LVM volgrpname=vg_groups op monitor interval=30s\npcs resource create lv_oldgroups ocf:heartbeat:LVM volgrpname=vg_oldgroups op monitor interval=30s\npcs resource create lvcache_bases ocf:heartbeat:LVM volgrpname=vgcache_bases op monitor interval=30s\npcs resource create lvcache_templates ocf:heartbeat:LVM volgrpname=vgcache_templates op monitor interval=30s\npcs resource create lvcache_groups ocf:heartbeat:LVM volgrpname=vgcache_groups op monitor interval=30s", 
            "title": "LVM resources"
        }, 
        {
            "location": "/setups/ha/active_passive/#filesystem-ext4-and-mountpoints", 
            "text": "pcs resource create ext4_bases Filesystem device= /dev/vg_bases/lv_bases  directory= /vimet/bases  fstype= ext4   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s\npcs resource create ext4_templates Filesystem device= /dev/vg_templates/lv_templates  directory= /vimet/templates  fstype= ext4   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s\npcs resource create ext4_groups Filesystem device= /dev/vg_groups/lv_groups  directory= /vimet/groups  fstype= ext4   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s\npcs resource create ext4_oldgroups Filesystem device= /dev/vg_oldgroups/lv_oldgroups  directory= /vimet/oldgroups  fstype= ext4   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s", 
            "title": "Filesystem ext4 and mountpoints"
        }, 
        {
            "location": "/setups/ha/active_passive/#nfs-server", 
            "text": "Version 4", 
            "title": "NFS server"
        }, 
        {
            "location": "/setups/ha/active_passive/#nfs-server-and-root", 
            "text": "pcs cluster cib nfsserver_cfg\npcs -f nfsserver_cfg resource create nfs-daemon systemd:nfs-server \\\nnfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true op monitor interval=30s \\\n--group nfs_server\npcs -f nfsserver_cfg resource create nfs-root exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,crossmnt,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash \\\ndirectory=/vimet \\\nfsid=0 \\\n--group nfs_server\npcs cluster cib-push nfsserver_cfg\npcs resource clone nfs_server master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0", 
            "title": "NFS: server and root"
        }, 
        {
            "location": "/setups/ha/active_passive/#nfs-exports", 
            "text": "pcs cluster cib exports_cfg\npcs -f exports_cfg resource create nfs_bases exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/bases \\\nfsid=11 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_templates exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,mountpoint,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/templates \\\nfsid=20 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_groups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,async,mountpoint,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/groups \\\nfsid=30 \\\nop monitor interval=30s\n\npcs -f exports_cfg resource create nfs_oldgroups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\nwait_for_leasetime_on_stop=true \\\noptions=rw,async,mountpoint,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/oldgroups \\\nfsid=35 \\\nop monitor interval=30s\npcs cluster cib-push exports_cfg", 
            "title": "NFS: Exports"
        }, 
        {
            "location": "/setups/ha/active_passive/#floating-ips", 
            "text": "Those IPs will 'move' always with assigned resources.  pcs resource create IPbases ocf:heartbeat:IPaddr2 ip=10.1.2.210 cidr_netmask=32 nic=nas:0  op monitor interval=30 \npcs resource create IPtemplates ocf:heartbeat:IPaddr2 ip=10.1.2.211 cidr_netmask=32 nic=nas:0  op monitor interval=30 \npcs resource create IPgroups ocf:heartbeat:IPaddr2 ip=10.1.2.212 cidr_netmask=32 nic=nas:0 op monitor interval=30\npcs resource create IPvbases ocf:heartbeat:IPaddr2 ip=10.1.1.28 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create IPvtemplates ocf:heartbeat:IPaddr2 ip=10.1.1.29 cidr_netmask=32 nic=escola:0 op monitor interval=30\npcs resource create IPvgroups ocf:heartbeat:IPaddr2 ip=10.1.1.30 cidr_netmask=32 nic=escola:0 op monitor interval=30", 
            "title": "Floating IPs"
        }, 
        {
            "location": "/setups/ha/active_passive/#groups-and-constraints", 
            "text": "Groups will allow starting and stopping resources in order.\nConstraings will allow restriction definitions.", 
            "title": "Groups and constraints"
        }, 
        {
            "location": "/setups/ha/active_passive/#bases_1", 
            "text": "pcs resource group add bases \\\n    lvcache_bases lv_bases ext4_bases nfs_bases IPbases IPvbases \n\npcs constraint order \\\n    promote drbd_bases-clone then bases INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_bases\n\npcs constraint colocation add \\\n    bases with master drbd_bases-clone INFINITY \\\n    id=c_drbd_bases\n\npcs constraint order \\\n    nfs_server-clone then bases INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_bases\n\npcs constraint colocation add \\\n    bases with started nfs_server-clone INFINITY \\\n    id=c_nfs_bases", 
            "title": "bases"
        }, 
        {
            "location": "/setups/ha/active_passive/#templates_1", 
            "text": "pcs resource group add templates \\\n    lvcache_templates lv_templates ext4_templates nfs_templates IPtemplates IPvtemplates \n\npcs constraint order \\\n    promote drbd_templates-clone then templates INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_templates\n\npcs constraint colocation add \\\n    templates with master drbd_templates-clone INFINITY \\\n    id=c_drbd_templates\n\npcs constraint order \\\n    nfs_server-clone then templates INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_templates\n\npcs constraint colocation add \\\n    templates with started nfs_server-clone INFINITY \\\n    id=c_nfs_templates", 
            "title": "templates"
        }, 
        {
            "location": "/setups/ha/active_passive/#groups_1", 
            "text": "pcs resource group add groups \\\n    lvcache_groups lv_groups lv_oldgroups ext4_groups ext4_oldgroups nfs_groups nfs_oldgroups IPgroups IPvgroups\n\npcs constraint order \\\n    promote drbd_groups-clone then groups INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_drbd_groups\n\npcs constraint colocation add \\\n    groups with master drbd_groups-clone INFINITY \\\n    id=c_drbd_groups\n\npcs constraint order \\\n    nfs_server-clone then groups INFINITY \\\n    require-all=true symmetrical=true \\\n    setoptions kind=Mandatory \\\n    id=o_nfs_groups\n\npcs constraint colocation add \\\n    groups with started nfs_server-clone INFINITY \\\n    id=c_nfs_groups  This constraints will allow for resources to be put on a preferred node.  pcs constraint location lv_groups prefers nas2=50\npcs constraint location lv_oldgroups prefers nas2=50\npcs constraint location lv_bases prefers nas1=50\npcs constraint location lv_templates prefers nas1=50", 
            "title": "groups"
        }, 
        {
            "location": "/setups/ha/active_passive/#pacemaker-utils", 
            "text": "", 
            "title": "Pacemaker utils"
        }, 
        {
            "location": "/setups/ha/active_passive/#manageunmanage-resource", 
            "text": "Unmanage drbd resource.  pcs resource unmanage drbd_templates-clone  Let the cluster control a drbd resource again:  pcs resource meta drbd_templates-clone is-managed=true", 
            "title": "Manage/Unmanage resource"
        }, 
        {
            "location": "/setups/ha/active_passive/#resize-volumes-under-drbd", 
            "text": "Stop cluster and start drbd manually.  pcs cluster stop/standby --all\nmodprobe drbd  Check that drbd resources are not active (cat /proc/drbd)", 
            "title": "Resize volumes under drbd"
        }, 
        {
            "location": "/setups/ha/active_passive/#cache-lvcachebases-ro-lvcachetemplates-wb-lvcachegroups-wb", 
            "text": "1.- Cache set to read only  eio_cli edit -c groups -m ro  2.- Wait till there are no dirty sectors (till it is 0)  grep nr_dirty /proc/enhanceio/groups/stats  3.- Remove cache device  eio_cli delete -c groups  4.- For example we do reduce lvcachegroups (600G: nvme0n1p1+/dev/sdf1)\n    and will grow lvcachetemplates to the new free space.  Shrink nvme0n1p1:          lvreduce -L 100G /dev/vgcache/lvcachegroups  Growing nvme0n1p1          lvresize -l 100%FREE -n /dev/vgcache/lvcachegroups /dev/nvme0n1p1  Extend template cache volume to the free space we just created.          lvextend -l +100%FREE /dev/vgcache/lvcachetemplates  Now drbd21 (resource groups volume 0) will be in 'diskless' state. This\nis normal as we broked drbd filesystem inside resized volumes.  Now drbd21 (resource templates volume 0) will be synchronizing, as we\ngrowed it, but it still needs to resize manually.  This will be applied on reduced volumes:  wipefs -a /dev/vgcache/lvcachegroups\ndrbdadm create-md groups/0\ndrbdadm up groups\ndrbdadm invalidate-remote groups/0  Now we can create EnhanceIO groups cache again.  This will be applied on growed volumes:  drbdadm resize templates/0  Now we can create EnhanceIO templates cache again.  For filesystems to be resized use resizefs for ext4.", 
            "title": "CACHE: lvcachebases (ro), lvcachetemplates (wb), lvcachegroups (wb)"
        }, 
        {
            "location": "/setups/ha/active_passive/#credits", 
            "text": "https://github.com/stec-inc/EnhanceIO\nhttps://www.suse.com/documentation/sle_ha/singlehtml/book_sleha_techguides/book_sleha_techguides.html\nhttps://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Power_Management_Guide/ALPM.html  _ _ _ _ _ ____-", 
            "title": "Credits:"
        }, 
        {
            "location": "/setups/ha/active_active/", 
            "text": "Active - Active cluster\n\n\nIn this setup we will make use of two servers (vserver4 \n vserver5), both \nwith similar hardware and software. Software raids, drbd 8 dual primary \nand pacemaker cluster control with GFS2 cluster filesystem.\nThe OS used was Fedora 22.\n\n\nNetworking\n\n\nRename interfaces\n\n\nnew_name=escola; \nold_name=enp3s0; \necho SUBSYSTEM==\\\nnet\\\n, ACTION==\\\nadd\\\n, DRIVERS==\\\n?*\\\n, \\\nATTR{address}==\\\n$(cat /sys/class/net/$old_name/address)\\\n, \\\nATTR{type}==\\\n1\\\n, KERNEL==\\\ne*\\\n, \\\nNAME=\\\n$new_name\\\n \n /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=drbd; \nold_name=enp1s0; \necho SUBSYSTEM==\\\nnet\\\n, ACTION==\\\nadd\\\n, DRIVERS==\\\n?*\\\n, \\\nATTR{address}==\\\n$(cat /sys/class/net/$old_name/address)\\\n, \\\nATTR{type}==\\\n1\\\n, KERNEL==\\\ne*\\\n, \\\nNAME=\\\n$new_name\\\n \n /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=dual0; \nold_name=enp2s0f0; \necho SUBSYSTEM==\\\nnet\\\n, ACTION==\\\nadd\\\n, DRIVERS==\\\n?*\\\n, \\\nATTR{address}==\\\n$(cat /sys/class/net/$old_name/address)\\\n, \\\nATTR{type}==\\\n1\\\n, KERNEL==\\\ne*\\\n, \\\nNAME=\\\n$new_name\\\n \n /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=dual1; \nold_name=enp2s0f1; \necho SUBSYSTEM==\\\nnet\\\n, ACTION==\\\nadd\\\n, DRIVERS==\\\n?*\\\n, \\\nATTR{address}==\\\n$(cat /sys/class/net/$old_name/address)\\\n, \\\nATTR{type}==\\\n1\\\n, KERNEL==\\\ne*\\\n, \\\nNAME=\\\n$new_name\\\n \n /etc/udev/rules.d/70-persistent-net.rules\n\n\n\n\n\nNetwork interface configurations\n\n\n[root@vserver4 ~]# cat /etc/sysconfig/network-scripts/ifcfg-escola \nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nyes\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nONBOOT=\nyes\n\nPEERDNS=\nyes\n\nPEERROUTES=\nyes\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nNAME=\nescola\n\nIPADDR=\n10.1.1.24\n\nPREFIX=\n24\n\nGATEWAY=\n10.1.1.199\n\nDNS1=\n10.1.1.200\n\nDNS2=\n10.1.1.201\n\nDOMAIN=\nescoladeltreball.org\n\n\n[root@vserver5 ~]# cat /etc/sysconfig/network-scripts/ifcfg-escola \nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nyes\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nONBOOT=\nyes\n\nPEERDNS=\nyes\n\nPEERROUTES=\nyes\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nNAME=\nescola\n\nIPADDR=\n10.1.1.25\n\nPREFIX=\n24\n\nGATEWAY=\n10.1.1.199\n\nDNS1=\n10.1.1.200\n\nDNS2=\n10.1.1.201\n\nDOMAIN=\nescoladeltreball.org\n\n\n[root@vserver4 ~]# cat /etc/sysconfig/network-scripts/ifcfg-drbd \nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nno\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nONBOOT=\nyes\n\nPEERDNS=\nno\n\nPEERROUTES=\nno\n\nIPV6_PEERDNS=\nno\n\nIPV6_PEERROUTES=\nno\n\n\nNAME=\ndrbd\n\nIPADDR=\n10.1.3.24\n\nPREFIX=\n24\n\nMTU=\n9000\n\n\n[root@vserver5 ~]# cat /etc/sysconfig/network-scripts/ifcfg-drbd \nTYPE=\nEthernet\n\nBOOTPROTO=\nnone\n\nDEFROUTE=\nno\n\nIPV4_FAILURE_FATAL=\nno\n\nIPV6INIT=\nno\n\nIPV6_AUTOCONF=\nno\n\nIPV6_DEFROUTE=\nno\n\nIPV6_FAILURE_FATAL=\nno\n\nONBOOT=\nyes\n\nPEERDNS=\nno\n\nPEERROUTES=\nno\n\nIPV6_PEERDNS=\nno\nTuneando fedora\nIPV6_PEERROUTES=\nno\n\n\nNAME=\ndrbd\n\nIPADDR=\n10.1.3.25\n\nPREFIX=\n24\n\nMTU=\n9000\n\n\n\n\n\n\nPartitioning and raids\n\n\n\n\n1 SSD for OS\n\n\n1 Intel high end SSD 100GB disk to use as cache\n\n\n3 500GB hard disks (raid 1)\n\n\n\n\nFormat disks and create partitions\n\n\n    parted -a optimal -s /dev/sda mklabel msdos\n    parted -a optimal -s /dev/sdc mklabel msdos\n    parted -a optimal -s /dev/sdd mklabel msdos\n    parted -a optimal -s /dev/sde mklabel msdos\n\n    [root@vserver4 ~]# lsblk \n    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n    sda      8:0    0  93,2G  0 disk \n    sdb      8:16   0  55,9G  0 disk \n    \u251c\u2500sdb1   8:17   0   500M  0 part /boot\n    \u251c\u2500sdb2   8:18   0   5,6G  0 part [SWAP]\n    \u251c\u2500sdb3   8:19   0  33,5G  0 part /\n    \u251c\u2500sdb4   8:20   0     1K  0 part \n    \u2514\u2500sdb5   8:21   0  16,4G  0 part /home\n    sdc      8:32   0 931,5G  0 disk \n    sdd      8:48   0 931,5G  0 disk \n    sde      8:64   0 931,5G  0 disk \n\n\n    [root@vserver4 ~]# parted -s /dev/sdc print\n    Model: ATA TOSHIBA DT01ACA0 (scsi)\n    Disk /dev/sdc: 500GB\n    Sector size (logical/physical): 512B/4096B\n    Partition Table: msdos\n    Disk Flags: \n\n    Number  Start   End    Size   Type     File system  Flags\n     1      1049kB  500GB  500GB  primary               raid\n\n\n\n\n\nAdjust SSD disks parameters\n\n\nDisable swap and avoid writes on read (noatime):\n\n\n    echo \nvm.swappiness=1\n \n /etc/sysctl.d/99-sysctl.conf\n\n\n\n\n\nAnd in fstab:\n\n\n    noatime,nodiratime,discard\n\n\n\n\n\nCreate raid 1\n\n\n    mdadm --create /dev/md0 --level=mirror --raid-devices=2 /dev/sdc1 /dev/sdd1 --spare-devices=1 /dev/sde1 \n    cat /proc/mdstat \n\n\n\n\n\nCreate LVM cache (over raid)\n\n\nVolumes\n\n\n    pvcreate /dev/md0\n    vgcreate vg_data /dev/md0\n    pvcreate /dev/sdb1\n\n\n\n\n\nCache\n\n\n    vgextend vg_data /dev/sdb1\n    lvcreate -L 2G -n lv_cache_meta vg_data /dev/sdb1\n    lvcreate -L 88G -n lv_cache_data vg_data /dev/sdb1\n    lvcreate -l 100%FREE -n lv_data vg_data /dev/md0\n    lvconvert --yes --type cache-pool --cachemode writeback --poolmetadata vg_data/lv_cache_meta vg_data/lv_cache_data\n    lvconvert --type cache --cachepool vg_data/lv_cache_data vg_data/lv_data\n\n    lsblk \n    lvdisplay \n    lvdisplay -a\n\n\n\n\n\nFedora tuning\n\n\nFirewall\n\n\n    systemctl stop firewalld\n    systemctl disable firewalld\n\n\n\n\n\nSelinux\n\n\n    setenforce 0\n    sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/sysconfig/selinux\n    sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/selinux/config\n    sestatus \n\n\n\n\n\nPackage utilities\n\n\n    yum -y install vim git tmux\n    yum -y update\n\n\n\n\n\nNetwork Time Protocol\n\n\nyum -y install ntp\nsystemctl start ntpd\nsystemctl status ntpd\nsystemctl enable ntpd\ndate\n\n\n\n\n\nBash history tuning\n\n\n    cat \n .bashrc \n \nEOF\n\n\n    # bash_history infinite\n    export HISTFILESIZE=\n    export HISTSIZE=\n    export HISTTIMEFORMAT=\n[%F %T] \n\n\n    # Avoid duplicates\n    export HISTCONTROL=ignoredups:erasedups  \n    # When the shell exits, append to the history file instead of overwriting it\n    shopt -s histappend\n\n    # After each command, append to the history file and reread it\n    export PROMPT_COMMAND=\n${PROMPT_COMMAND:+$PROMPT_COMMAND$\n\\n\n}history -a; history -c; history -r\n\n\n    alias history_cleaned=\ncat .bash_history |grep -a -v ^\n#\n\n\n    export TMOUT=3600\n\n    EOF\n\n\n\n\n\nDRBD 8\n\n\nInstall packages\n\n\n    yum -y install drbd drbd-bash-completion drbd-utils \n\n\n\n\n\nConfiguration files\n\n\nGet the samples from installed packages:\n\n\n    cp -a /etc/drbd.conf /root/drbd.conf.dist.f22\n    cp -a /etc/drbd.d/global_common.conf /root/drbd_global_common.conf.dist.f22\n\n\n\n\n\ndrbd.conf:\n\n\n    global {\n            usage-count yes;\n    }\n\n    common {\n            handlers {\n            }\n\n            startup {\n            }\n\n            options {\n            }\n\n            disk {\n            }\n\n            net {\n                    protocol C;\n\n                    allow-two-primaries;\n                    after-sb-0pri discard-zero-changes;\n                    after-sb-1pri discard-secondary;\n                    after-sb-2pri disconnect;\n            }\n    }\n\n\n\n\n\nResources: /etc/drbd.d/vdisks.res\n\n\n    resource vdisks {\n         device    /dev/drbd0;\n         disk      /dev/vg_data/lv_data;\n         meta-disk internal;\n        on vserver4 {\n         address   10.1.3.24:7789;\n        }\n        on vserver5 {\n         address   10.1.3.25:7789;\n        }\n    }\n\n\n\n\n\nWe create drbdmetadata\n\n\n    drbdadm create-md vdisks\n\n    [...]\n    Writing meta data...\n    New drbd meta data block successfully created.\n    success\n\n\n\n\n\nWe can 'dry-run' adjust to check config files:\n\n\n    drbdadm -d adjust all\n\n\n\n\n\nThe we can execute:\n\n\n    drbdadm adjust all\n\n\n\n\n\nVerify on both servers:\n\n\n    [root@vserver4 ~]# cat /proc/drbd \n    version: 8.4.5 (api:1/proto:86-101)\n    srcversion: 5A4F43804B37BB28FCB1F47 \n     0: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----\n        ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:488236452\n\n\n\n\n\nWe can force primary on one server (vserver4):\n\n\n    drbdadm primary --force vdisks\n\n\n\n\n\nAnd we do it again on the other server (vserver5) as we want a dual\nprimary configuration:\n\n\n    drbdadm primary vdisks\n\n    [root@vserver4 ~]# cat /proc/drbd \n    version: 8.4.5 (api:1/proto:86-101)\n    srcversion: 5A4F43804B37BB28FCB1F47 \n     0: cs:SyncSource ro:Primary/Primary ds:UpToDate/Inconsistent C r-----\n        ns:17322104 nr:0 dw:0 dr:17323016 al:0 bm:0 lo:2 pe:2 ua:2 ap:0 ep:1 wo:f oos:470916516\n        [\n....................] sync\ned:  3.6% (459876/476792)M\n        finish: 3:03:06 speed: 42,844 (36,616) K/sec\n\n\n\n\n\nCluster\n\n\nInstall pacemaker packages\n\n\nFence agents\n\n\n    yum -y install fence-agents-apc fence-agents-apc-snmp\n    fence_apc --help\n    fence_apc_snmp --help\n\n\n\n\n\nPacemaker\n\n\n    dnf -y install corosync pcs pacemaker pacemaker-doc\n\n\n\n\n\nPacemaker drbd resource\n\n\n    dnf -y install drbd-pacemaker \n\n\n\n\n\nPackages needed for gs2 filesystem (needs cluster lock control)    \n\n\n    dnf -y install gfs2-utils lvm2-cluster dlm\n\n\n\n\n\nStarting and configuring cluster\n\n\n    systemctl start pcsd\n    systemctl enable pcsd\n\n\n\n\n\n    passwd hacluster\n\n\n\n\n\nHost name resolution must be set in /etc/hosts\n\n\nvserver4:\n    echo \nvserver4\n \n /etc/hostname\n    echo \n10.1.1.24   vserver4\n \n /etc/hosts\n    echo \n10.1.1.25   vserver5\n \n /etc/hosts\n    exit\n\n\n\n\n\nvserver5: \n\n    echo \n10.1.1.24   vserver4\n \n /etc/hosts\n    echo \n10.1.1.25   vserver5\n \n /etc/hosts\n    echo \nvserver5\n \n /etc/hostname\n    exit\n\n\n\n\n\nIn one node only!:\n\n\n    server1=vserver4\n    server2=vserver5\n    cl_name=vservers\n\n    pcs cluster auth $server1 $server2\n    pcs cluster setup --name $cl_name $server1 $server2\n    pcs cluster start --all\n    pcs status\n\n    [root@vserver4 ~]#     pcs status\n    Cluster name: vservers\n    WARNING: no stonith devices and stonith-enabled is not false\n    WARNING: corosync and pacemaker node names do not match (IPs used in setup?)\n    Last updated: Sun Oct 25 23:37:34 2015      Last change: \n    Stack: unknown\n    Current DC: NONE\n    0 nodes and 0 resources configured\n\n\n    Full list of resources:\n\n\n    PCSD Status:\n      vserver4 member (vserver4): Online\n      vserver5 member (vserver5): Online\n\n    Daemon Status:\n      corosync: active/disabled\n      pacemaker: active/disabled\n      pcsd: active/enabled\n\n\n\n\n\nCheck that cluster config is loaded as expected\n\n\n    [root@vserver4 ~]# pcs cluster cib |grep vserver\n    \ncib crm_feature_set=\n3.0.10\n validate-with=\npacemaker-2.3\n epoch=\n5\n num_updates=\n8\n admin_epoch=\n0\n cib-last-written=\nSun Oct 25 23:52:24 2015\n update-origin=\nvserver5\n update-client=\ncrmd\n update-user=\nhacluster\n have-quorum=\n1\n dc-uuid=\n2\n\n            \nnvpair id=\ncib-bootstrap-options-cluster-name\n name=\ncluster-name\n value=\nvservers\n/\n\n          \nnode id=\n1\n uname=\nvserver4\n/\n\n          \nnode id=\n2\n uname=\nvserver5\n/\n\n        \nnode_state id=\n2\n uname=\nvserver5\n in_ccm=\ntrue\n crmd=\nonline\n crm-debug-origin=\ndo_state_transition\n join=\nmember\n expected=\nmember\n\n        \nnode_state id=\n1\n uname=\nvserver4\n in_ccm=\ntrue\n crmd=\nonline\n crm-debug-origin=\ndo_state_transition\n join=\nmember\n expected=\nmember\n\n    [root@vserver4 ~]# grep vserver /etc/corosync/corosync.conf \n    cluster_name: vservers\n            ring0_addr: vserver4\n            ring0_addr: vserver5\n\n\n\n\n\nFencing\n\n\n    [root@vserver4 ~]# pcs stonith list \n    fence_apc - Fence agent for APC over telnet/ssh\n    fence_apc_snmp - Fence agent for APC, Tripplite PDU over SNMP\n\n\n\n\n\n    pcs stonith describe fence_apc_snmp\n\n\n\n\n\nYou can check if your stonith is reacheable and working:\n\n\n    fence_apc_snmp --ip=stonith1 --action=monitor\n    fence_apc_snmp --ip=stonith1 --action=monitor --community=escola2015\n    fence_apc_snmp --ip=stonith1 --action=reboot --plug=6  --community=escola2015 --power-wait=5\n\n\n\n\n\nConfigure stonith resources as ssh (discarded as it is too slow)\n\n\n    #pwd1=$(cat /root/pwd1)\n    #pwd2=$(cat /root/pwd2)\n\n    pcs stonith delete stonith1\n\n    pcs cluster cib stonith_cfg\n\n\n    pcs -f stonith_cfg stonith create stonith1 fence_apc ipaddr=10.1.1.3 login=vservers passwd=$pwd1 pcmk_host_list=\nvserver4 vserver5\n pcmk_host_map=\nvserver4:4;vserver5:5\n\n    #pcs -f stonith_cfg stonith create  stonith2 fence_apc ipaddr=10.1.1.3 login=vserver5 passwd=$pwd2 pcmk_host_list=\nvserver5\n pcmk_host_map=\nvserver5:3\n\n    pcs -f stonith_cfg property set stonith-enabled=false\n\n    pcs cluster cib-push stonith_cfg\n\n\n\n\n\nConfigure stonith resource as snmp (we use this one)\n\n\n    pcs stonith delete stonith1\n    pcs cluster cib stonith_cfg\n    pcs -f stonith_cfg stonith create stonith1 fence_apc_snmp params ipaddr=10.1.1.3 pcmk_host_list=\nvserver4,vserver5\n pcmk_host_map=\nvserver4:4;verver5:5\n pcmk_host_check=static-list power_wait=5\n    pcs cluster cib-push stonith_cfg\n\n\n\n\n\nActivate stonith resource:\n\n\n    pcs property set stonith-enabled=true\n\n\n\n\n\nTests (warning, will reboot nodes!)    \n\n\n    pcs cluster stop vserver5\n    stonith_admin --reboot vserver5\n\n\n\n\n\n    pcs cluster start --all    \n    pcs cluster stop vserver4\n    stonith_admin --reboot vserver4\n\n\n\n\n\nWhile configuring cluster you may disable fencing:\n\n\n    pcs property set stonith-enabled=false\n\n\n\n\n\nCheck stonith resource definition\n\n\n    pcs stonith show --full\n\n\n\n\n\nYou'll find logs in:\n\n\n    tail -f /var/log/pacemaker.log \n\n\n\n\n\nPACEMAKER DRBD\n\n\n    echo drbd \n /etc/modules-load.d/drbd.conf\n    pcs resource create drbd-vdisks ocf:linbit:drbd drbd_resource=vdisks op monitor interval=60s\n    pcs resource master drbd-vdisks-clone drbd-opt master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\n\n\n\n\n\ndlm\n\n\nWe need cluster locking for gfs2 filesystem\n\n\n    pcs cluster cib dlm_cfg\n    pcs -f dlm_cfg resource create dlm ocf:pacemaker:controld op monitor interval=60s\n    pcs -f dlm_cfg resource clone dlm clone-max=2 clone-node-max=1\n    pcs cluster cib-push dlm_cfg\n\n\n\n\n\nCluster lvm\n\n\nSet up cluster lvms\n\n\n    systemctl disable lvm2-lvmetad.service\n    systemctl disable lvm2-lvmetad.socket\n    systemctl stop lvm2-lvmetad.service\n\n    lvmconf --enable-cluster\n    reboot\n\n\n\n\n\nYou should define the devices where lvm will look for lvm signatures in\nfile /etc/lvm/lvm.conf:\n\n\n    filter = [\na|sd.*|\n, \na|md.*|\n, \na|drbd.*|\n, \nr|.*|\n]\n\n\n\n\n\nSet up cluster lock lvms\n\n\n    pcs cluster cib clvmd_cfg\n    pcs -f clvmd_cfg resource create clvmd ocf:heartbeat:clvm params daemon_options=\ntimeout=30s\n op monitor interval=60s\n    pcs -f clvmd_cfg resource clone clvmd clone-max=2 clone-node-max=1\n    pcs cluster cib-push clvmd_cfg\n\n\n\n\n\nVerify cluster status:  \n\n\n    [root@vserver5 ~]# pcs status\n    Cluster name: vservers\n    WARNING: corosync and pacemaker node names do not match (IPs used in setup?)\n    Last updated: Thu Oct 29 13:14:58 2015      Last change: Thu Oct 29 13:14:42 2015 by root via cibadmin on vserver5\n    Stack: corosync\n    Current DC: vserver5 (version 1.1.13-3.fc22-44eb2dd) - partition with quorum\n    2 nodes and 7 resources configured\n\n    Online: [ vserver4 vserver5 ]\n\n    Full list of resources:\n\n     Master/Slave Set: drbd-vdisks-clone [drbd-vdisks]\n         Masters: [ vserver4 vserver5 ]\n     stonith1   (stonith:fence_apc_snmp):   Started vserver4\n     Clone Set: dlm-clone [dlm]\n         Started: [ vserver4 vserver5 ]\n     Clone Set: clvmd-clone [clvmd]\n         Started: [ vserver4 vserver5 ]\n\n    PCSD Status:\n      vserver4 member (vserver4): Online\n      vserver5 member (vserver5): Online\n\n    Daemon Status:\n      corosync: active/enabled\n      pacemaker: active/enabled\n      pcsd: active/enabled\n\n\n\n\n\nCreate volumes\n\n\nIn each server:\n\n\n    pvcreate /dev/drbd0\n\n\n\n\n\nIf we need to do cluster actions we should use -ci:\n\n\n    -cn ==\n local action\n    -cy ==\n cluster wide action\n\n\n\n\n\nNow we can continue with cluster wide commands:\n\n\n    vgcreate -cy vgcluster /dev/drbd0\n\n\n\n\n\nWe can check on the other node if the vg was created:\n\n\n    [root@vserver5 ~]# vgs\n      VG        #PV #LV #SN Attr   VSize   VFree  \n      vg_data     2   1   0 wz--n- 558,79g   1,16g\n      vgcluster   1   0   0 wz--nc 465,62g 465,62g\n\n\n\n\n\nWe keep some free space just in case we want to do io tests:\n\n\n    lvcreate -l 97%FREE -n lvcluster1 vgcluster /dev/drbd0\n\n\n\n\n\nAnd check it fromk the other server:    \n\n\n    [root@vserver4 ~]# lvs\n      LV         VG        Attr       LSize   Pool            Origin          Data%  Meta%  Move Log Cpy%Sync Convert\n      lv_data    vg_data   Cwi-aoC--- 465,63g [lv_cache_data] [lv_data_corig] 0,00   0,82            0,00            \n      lvcluster1 vgcluster -wi-a----- 451,65g    \n\n\n\n\n\nHow to run fencing from drbd itself\n\n\nIn 'handlers' section in /etc/drbd.d/global_common.conf:\n\n\n    fence-peer \n/usr/lib/drbd/crm-fence-peer.sh\n;\n    after-resync-target \n/usr/lib/drbd/crm-unfence-peer.sh\n;\n\n\n\n\n\nIn 'disk' section:\n\n\n    fencing resource-and-stonith;\n\n\n\n\n\nThis is the result:\n\n\n    [root@vserver4 ~]# cat /etc/drbd.d/global_common.conf \n    global {\n        usage-count yes;\n    }\n\n    common {\n        handlers {\n            split-brain \n/usr/lib/drbd/notify-split-brain.sh root\n;\n            fence-peer \n/usr/lib/drbd/crm-fence-peer.sh\n;\n            after-resync-target \n/usr/lib/drbd/crm-unfence-peer.sh\n;\n        }\n\n        startup {\n        }\n\n        options {\n        }\n\n        disk {\n            fencing resource-and-stonith;\n        }\n\n        net {\n            protocol C;\n\n            allow-two-primaries;\n            after-sb-0pri discard-zero-changes;\n            after-sb-1pri discard-secondary;\n            after-sb-2pri disconnect;\n        }\n    }\n\n\n\n\n\nCheck that pacemaker and stonith are working:\n\n\n    [root@vserver4 ~]# pcs property list\n    Cluster Properties:\n     cluster-infrastructure: corosync\n     cluster-name: vservers\n     dc-version: 1.1.13-3.fc22-44eb2dd\n     have-watchdog: false\n     stonith-enabled: true\n\n\n\n\n\nCheck again that stonith is enabled:\n\n\n    pcs property set stonith-enabled=true\n\n\n\n\n\nConstraints\n\n\ndlm and clvmd must be started in order:\n\n\npcs cluster cib cons/traints_cfg\npcs constraint order set drbd-vdisks-clone action=promote \\\nset dlm-clone clvmd-clone action=start \\\nsequential=true\npcs cluster cib-push constraints_cfg    \n\n\n\n\n\nDRBD\n\n\nSample config of drbd on gfs2 cluster.\n\n\nyum install drbd drbd-utils drbd-udev drbd-pacemaker -y\nmodprobe drbd\nsystemctl enable drbd\n\n\n\n\n\nWe create drbd resources (/etc/drbd.d/...)\n\n\ndrbdadm create-md bases\ndrbdadm create-md templates\ndrbdadm create-dm grups\ndrbdadm up bases\ndrbdadm up templates\ndrbdadm up grups\ndrbdadm primary bases --force\ndrbdadm primary templates --force\ndrbdadm primary grups --force\n\n\n\n\n\ndlm and clvm2 resources\n\n\nClusteres lvms\n\n\npcs cluster cib locks_cfg\npcs -f locks_cfg resource create dlm ocf:pacemaker:controld op monitor interval=60s --group cluster_lock\npcs -f locks_cfg resource create clvmd ocf:heartbeat:clvm params daemon_options=\ntimeout=30s\n op monitor interval=60s  --group cluster_lock\npcs -f locks_cfg resource clone cluster_lock clone-max=2 clone-node-max=1 on-fail=restart \npcs cluster cib-push locks_cfg\n\n\n\n\n\nWithout clustered lvms\n\n\npcs resource create dlm ocf:pacemaker:controld op monitor interval=60s \npcs resource clone dlm clone-max=2 clone-node-max=1 on-fail=restart\n\n\n\n\n\nDRBD Resources\n\n\npcs cluster cib drbd_bases_cfg\npcs -f drbd_bases_cfg resource create drbd_bases ocf:linbit:drbd drbd_resource=bases op monitor interval=60s\npcs -f drbd_bases_cfg resource master drbd_bases-clone drbd_bases master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_bases_cfg\n\npcs cluster cib drbd_templates_cfg\npcs -f drbd_templates_cfg resource create drbd_templates ocf:linbit:drbd drbd_resource=templates op monitor interval=60s\npcs -f drbd_templates_cfg resource master drbd_templates-clone drbd_templates master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_templates_cfg\n\npcs cluster cib drbd_grups_cfg\npcs -f drbd_grups_cfg resource create drbd_grups ocf:linbit:drbd drbd_resource=grups op monitor interval=60s\npcs -f drbd_grups_cfg resource master drbd_grups-clone drbd_grups master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_grups_cfg\n\n\n\n\n\nGFS2 filesystem\n\n\nmkfs.gfs2 -p lock_dlm -t vimet_cluster:bases -j 2 /dev/drbd10\nmkfs.gfs2 -p lock_dlm -t vimet_cluster:templates -j 2 /dev/drbd11\nmkfs.gfs2 -p lock_dlm -t vimet_cluster:grups -j 2 /dev/drbd30\n\n\n\n\n\nGFS2 resources\n\n\npcs resource create gfs2_bases Filesystem device=\n/dev/drbd10\n directory=\n/vimet/bases\n fstype=\ngfs2\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1\npcs resource create gfs2_templates Filesystem device=\n/dev/drbd11\n directory=\n/vimet/templates\n fstype=\ngfs2\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1\npcs resource create gfs2_grups Filesystem device=\n/dev/drbd30\n directory=\n/vimet/grups\n fstype=\ngfs2\n \noptions=defaults,noatime,nodiratime,noquota\n op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1\n\n\n\n\n\nNFS 4 server\n\n\npcs cluster cib nfsserver_cfg\npcs -f nfsserver_cfg resource create nfs-daemon systemd:nfs-server \\\nnfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true \\\n--group nfs_server\npcs -f nfsserver_cfg resource create nfs-root exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash \\\ndirectory=/vimet \\\nfsid=0 \\\n--group nfs_server\npcs cluster cib-push nfsserver_cfg\npcs resource clone nfs_server master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\n\n\n\n\nNFS 4 exports\n\n\npcs cluster cib exports_cfg\npcs -f exports_cfg resource create nfs_bases exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/bases \\\nfsid=11 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\npcs -f exports_cfg resource create nfs_templates exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/templates \\\nfsid=21 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\npcs -f exports_cfg resource create nfs_grups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/grups \\\nfsid=31 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\npcs cluster cib-push exports_cfg\n\n\n\n\n\nFloatin IPs\n\n\npcs resource create ClusterIPbases ocf:heartbeat:IPaddr2 ip=10.1.2.210 cidr_netmask=32 nic=nas:10 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPtemplates ocf:heartbeat:IPaddr2 ip=10.1.2.211 cidr_netmask=32 nic=nas:11 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPgrups ocf:heartbeat:IPaddr2 ip=10.1.2.212 cidr_netmask=32 nic=nas:30 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnasbases ocf:heartbeat:IPaddr2 ip=10.1.1.28 cidr_netmask=32 nic=nas:110 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnastemplates ocf:heartbeat:IPaddr2 ip=10.1.1.29 cidr_netmask=32 nic=nas:111 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnasgrups ocf:heartbeat:IPaddr2 ip=10.1.1.30 cidr_netmask=32 nic=nas:130 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\n\n\n\n\n\nConstraints\n\n\nStart and stop order and restrictions\n\n\npcs constraint order \\\n    set stonith action=start \\\n    set cluster_lock-clone action=start \\\n    set nfs_server-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=serveis\n\npcs constraint order \\\n    set drbd_bases-clone action=promote role=Master \\\n    set gfs2_bases-clone \\\n    set nfs_bases-clone \\\n    set ClusterIPcnasbases-clone action=start \\\n    set ClusterIPbases-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=bases\n\npcs constraint order \\\n    set drbd_templates-clone action=promote role=Master \\\n    set gfs2_templates-clone \\\n    set nfs_templates-clone \\\n    set ClusterIPcnastemplates-clone action=start \\\n    set ClusterIPtemplates-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=templates\n\npcs constraint order \\\n    set drbd_grups-clone action=promote role=Master \\\n    set gfs2_grups-clone \\\n    set nfs_grups-clone \\\n    set ClusterIPcnasgrups-clone action=start \\\n    set ClusterIPgrups-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=grups\n\n\n\n\n\nLocation constraints\n\n\npcs constraint colocation add \\\n    ClusterIPbases-clone with nfs_bases-clone INFINITY \\\n    id=colocate_bases\n\npcs constraint colocation add \\\n    ClusterIPtemplates-clone with nfs_templates-clone INFINITY \\\n    id=colocate_templates\n\npcs constraint colocation add \\\n    ClusterIPgrups-clone with nfs_grups-clone INFINITY \\\n    id=colocate_grups\n\npcs constraint colocation add \\\n    ClusterIPcnasbases-clone with nfs_bases-clone INFINITY \\\n    id=colocate_cnasbases\n\npcs constraint colocation add \\\n    ClusterIPcnastemplates-clone with nfs_templates-clone INFINITY \\\n    id=colocate_cnastemplates\n\npcs constraint colocation add \\\n    ClusterIPcnasgrups-clone with nfs_grups-clone INFINITY \\\n    id=colocate_cnasgrups", 
            "title": "Active-Active"
        }, 
        {
            "location": "/setups/ha/active_active/#active-active-cluster", 
            "text": "In this setup we will make use of two servers (vserver4   vserver5), both \nwith similar hardware and software. Software raids, drbd 8 dual primary \nand pacemaker cluster control with GFS2 cluster filesystem.\nThe OS used was Fedora 22.", 
            "title": "Active - Active cluster"
        }, 
        {
            "location": "/setups/ha/active_active/#networking", 
            "text": "Rename interfaces  new_name=escola; \nold_name=enp3s0; \necho SUBSYSTEM==\\ net\\ , ACTION==\\ add\\ , DRIVERS==\\ ?*\\ , \\\nATTR{address}==\\ $(cat /sys/class/net/$old_name/address)\\ , \\\nATTR{type}==\\ 1\\ , KERNEL==\\ e*\\ , \\\nNAME=\\ $new_name\\    /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=drbd; \nold_name=enp1s0; \necho SUBSYSTEM==\\ net\\ , ACTION==\\ add\\ , DRIVERS==\\ ?*\\ , \\\nATTR{address}==\\ $(cat /sys/class/net/$old_name/address)\\ , \\\nATTR{type}==\\ 1\\ , KERNEL==\\ e*\\ , \\\nNAME=\\ $new_name\\    /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=dual0; \nold_name=enp2s0f0; \necho SUBSYSTEM==\\ net\\ , ACTION==\\ add\\ , DRIVERS==\\ ?*\\ , \\\nATTR{address}==\\ $(cat /sys/class/net/$old_name/address)\\ , \\\nATTR{type}==\\ 1\\ , KERNEL==\\ e*\\ , \\\nNAME=\\ $new_name\\    /etc/udev/rules.d/70-persistent-net.rules\n\nnew_name=dual1; \nold_name=enp2s0f1; \necho SUBSYSTEM==\\ net\\ , ACTION==\\ add\\ , DRIVERS==\\ ?*\\ , \\\nATTR{address}==\\ $(cat /sys/class/net/$old_name/address)\\ , \\\nATTR{type}==\\ 1\\ , KERNEL==\\ e*\\ , \\\nNAME=\\ $new_name\\    /etc/udev/rules.d/70-persistent-net.rules  Network interface configurations  [root@vserver4 ~]# cat /etc/sysconfig/network-scripts/ifcfg-escola \nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= yes \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nONBOOT= yes \nPEERDNS= yes \nPEERROUTES= yes \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nNAME= escola \nIPADDR= 10.1.1.24 \nPREFIX= 24 \nGATEWAY= 10.1.1.199 \nDNS1= 10.1.1.200 \nDNS2= 10.1.1.201 \nDOMAIN= escoladeltreball.org \n\n[root@vserver5 ~]# cat /etc/sysconfig/network-scripts/ifcfg-escola \nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= yes \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nONBOOT= yes \nPEERDNS= yes \nPEERROUTES= yes \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nNAME= escola \nIPADDR= 10.1.1.25 \nPREFIX= 24 \nGATEWAY= 10.1.1.199 \nDNS1= 10.1.1.200 \nDNS2= 10.1.1.201 \nDOMAIN= escoladeltreball.org \n\n[root@vserver4 ~]# cat /etc/sysconfig/network-scripts/ifcfg-drbd \nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= no \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nONBOOT= yes \nPEERDNS= no \nPEERROUTES= no \nIPV6_PEERDNS= no \nIPV6_PEERROUTES= no \n\nNAME= drbd \nIPADDR= 10.1.3.24 \nPREFIX= 24 \nMTU= 9000 \n\n[root@vserver5 ~]# cat /etc/sysconfig/network-scripts/ifcfg-drbd \nTYPE= Ethernet \nBOOTPROTO= none \nDEFROUTE= no \nIPV4_FAILURE_FATAL= no \nIPV6INIT= no \nIPV6_AUTOCONF= no \nIPV6_DEFROUTE= no \nIPV6_FAILURE_FATAL= no \nONBOOT= yes \nPEERDNS= no \nPEERROUTES= no \nIPV6_PEERDNS= no Tuneando fedora\nIPV6_PEERROUTES= no \n\nNAME= drbd \nIPADDR= 10.1.3.25 \nPREFIX= 24 \nMTU= 9000", 
            "title": "Networking"
        }, 
        {
            "location": "/setups/ha/active_active/#partitioning-and-raids", 
            "text": "1 SSD for OS  1 Intel high end SSD 100GB disk to use as cache  3 500GB hard disks (raid 1)   Format disks and create partitions      parted -a optimal -s /dev/sda mklabel msdos\n    parted -a optimal -s /dev/sdc mklabel msdos\n    parted -a optimal -s /dev/sdd mklabel msdos\n    parted -a optimal -s /dev/sde mklabel msdos\n\n    [root@vserver4 ~]# lsblk \n    NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n    sda      8:0    0  93,2G  0 disk \n    sdb      8:16   0  55,9G  0 disk \n    \u251c\u2500sdb1   8:17   0   500M  0 part /boot\n    \u251c\u2500sdb2   8:18   0   5,6G  0 part [SWAP]\n    \u251c\u2500sdb3   8:19   0  33,5G  0 part /\n    \u251c\u2500sdb4   8:20   0     1K  0 part \n    \u2514\u2500sdb5   8:21   0  16,4G  0 part /home\n    sdc      8:32   0 931,5G  0 disk \n    sdd      8:48   0 931,5G  0 disk \n    sde      8:64   0 931,5G  0 disk \n\n\n    [root@vserver4 ~]# parted -s /dev/sdc print\n    Model: ATA TOSHIBA DT01ACA0 (scsi)\n    Disk /dev/sdc: 500GB\n    Sector size (logical/physical): 512B/4096B\n    Partition Table: msdos\n    Disk Flags: \n\n    Number  Start   End    Size   Type     File system  Flags\n     1      1049kB  500GB  500GB  primary               raid", 
            "title": "Partitioning and raids"
        }, 
        {
            "location": "/setups/ha/active_active/#adjust-ssd-disks-parameters", 
            "text": "Disable swap and avoid writes on read (noatime):      echo  vm.swappiness=1    /etc/sysctl.d/99-sysctl.conf  And in fstab:      noatime,nodiratime,discard", 
            "title": "Adjust SSD disks parameters"
        }, 
        {
            "location": "/setups/ha/active_active/#create-raid-1", 
            "text": "mdadm --create /dev/md0 --level=mirror --raid-devices=2 /dev/sdc1 /dev/sdd1 --spare-devices=1 /dev/sde1 \n    cat /proc/mdstat", 
            "title": "Create raid 1"
        }, 
        {
            "location": "/setups/ha/active_active/#create-lvm-cache-over-raid", 
            "text": "", 
            "title": "Create LVM cache (over raid)"
        }, 
        {
            "location": "/setups/ha/active_active/#volumes", 
            "text": "pvcreate /dev/md0\n    vgcreate vg_data /dev/md0\n    pvcreate /dev/sdb1", 
            "title": "Volumes"
        }, 
        {
            "location": "/setups/ha/active_active/#cache", 
            "text": "vgextend vg_data /dev/sdb1\n    lvcreate -L 2G -n lv_cache_meta vg_data /dev/sdb1\n    lvcreate -L 88G -n lv_cache_data vg_data /dev/sdb1\n    lvcreate -l 100%FREE -n lv_data vg_data /dev/md0\n    lvconvert --yes --type cache-pool --cachemode writeback --poolmetadata vg_data/lv_cache_meta vg_data/lv_cache_data\n    lvconvert --type cache --cachepool vg_data/lv_cache_data vg_data/lv_data\n\n    lsblk \n    lvdisplay \n    lvdisplay -a", 
            "title": "Cache"
        }, 
        {
            "location": "/setups/ha/active_active/#fedora-tuning", 
            "text": "", 
            "title": "Fedora tuning"
        }, 
        {
            "location": "/setups/ha/active_active/#firewall", 
            "text": "systemctl stop firewalld\n    systemctl disable firewalld", 
            "title": "Firewall"
        }, 
        {
            "location": "/setups/ha/active_active/#selinux", 
            "text": "setenforce 0\n    sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/sysconfig/selinux\n    sed -i s/SELINUX=enforcing/SELINUX=permissive/ /etc/selinux/config\n    sestatus", 
            "title": "Selinux"
        }, 
        {
            "location": "/setups/ha/active_active/#package-utilities", 
            "text": "yum -y install vim git tmux\n    yum -y update", 
            "title": "Package utilities"
        }, 
        {
            "location": "/setups/ha/active_active/#network-time-protocol", 
            "text": "yum -y install ntp\nsystemctl start ntpd\nsystemctl status ntpd\nsystemctl enable ntpd\ndate", 
            "title": "Network Time Protocol"
        }, 
        {
            "location": "/setups/ha/active_active/#bash-history-tuning", 
            "text": "cat   .bashrc    EOF \n\n    # bash_history infinite\n    export HISTFILESIZE=\n    export HISTSIZE=\n    export HISTTIMEFORMAT= [%F %T]  \n\n    # Avoid duplicates\n    export HISTCONTROL=ignoredups:erasedups  \n    # When the shell exits, append to the history file instead of overwriting it\n    shopt -s histappend\n\n    # After each command, append to the history file and reread it\n    export PROMPT_COMMAND= ${PROMPT_COMMAND:+$PROMPT_COMMAND$ \\n }history -a; history -c; history -r \n\n    alias history_cleaned= cat .bash_history |grep -a -v ^ # \n\n    export TMOUT=3600\n\n    EOF", 
            "title": "Bash history tuning"
        }, 
        {
            "location": "/setups/ha/active_active/#drbd-8", 
            "text": "", 
            "title": "DRBD 8"
        }, 
        {
            "location": "/setups/ha/active_active/#install-packages", 
            "text": "yum -y install drbd drbd-bash-completion drbd-utils", 
            "title": "Install packages"
        }, 
        {
            "location": "/setups/ha/active_active/#configuration-files", 
            "text": "Get the samples from installed packages:      cp -a /etc/drbd.conf /root/drbd.conf.dist.f22\n    cp -a /etc/drbd.d/global_common.conf /root/drbd_global_common.conf.dist.f22  drbd.conf:      global {\n            usage-count yes;\n    }\n\n    common {\n            handlers {\n            }\n\n            startup {\n            }\n\n            options {\n            }\n\n            disk {\n            }\n\n            net {\n                    protocol C;\n\n                    allow-two-primaries;\n                    after-sb-0pri discard-zero-changes;\n                    after-sb-1pri discard-secondary;\n                    after-sb-2pri disconnect;\n            }\n    }  Resources: /etc/drbd.d/vdisks.res      resource vdisks {\n         device    /dev/drbd0;\n         disk      /dev/vg_data/lv_data;\n         meta-disk internal;\n        on vserver4 {\n         address   10.1.3.24:7789;\n        }\n        on vserver5 {\n         address   10.1.3.25:7789;\n        }\n    }  We create drbdmetadata      drbdadm create-md vdisks\n\n    [...]\n    Writing meta data...\n    New drbd meta data block successfully created.\n    success  We can 'dry-run' adjust to check config files:      drbdadm -d adjust all  The we can execute:      drbdadm adjust all  Verify on both servers:      [root@vserver4 ~]# cat /proc/drbd \n    version: 8.4.5 (api:1/proto:86-101)\n    srcversion: 5A4F43804B37BB28FCB1F47 \n     0: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----\n        ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:488236452  We can force primary on one server (vserver4):      drbdadm primary --force vdisks  And we do it again on the other server (vserver5) as we want a dual\nprimary configuration:      drbdadm primary vdisks\n\n    [root@vserver4 ~]# cat /proc/drbd \n    version: 8.4.5 (api:1/proto:86-101)\n    srcversion: 5A4F43804B37BB28FCB1F47 \n     0: cs:SyncSource ro:Primary/Primary ds:UpToDate/Inconsistent C r-----\n        ns:17322104 nr:0 dw:0 dr:17323016 al:0 bm:0 lo:2 pe:2 ua:2 ap:0 ep:1 wo:f oos:470916516\n        [ ....................] sync ed:  3.6% (459876/476792)M\n        finish: 3:03:06 speed: 42,844 (36,616) K/sec", 
            "title": "Configuration files"
        }, 
        {
            "location": "/setups/ha/active_active/#cluster", 
            "text": "", 
            "title": "Cluster"
        }, 
        {
            "location": "/setups/ha/active_active/#install-pacemaker-packages", 
            "text": "Fence agents      yum -y install fence-agents-apc fence-agents-apc-snmp\n    fence_apc --help\n    fence_apc_snmp --help  Pacemaker      dnf -y install corosync pcs pacemaker pacemaker-doc  Pacemaker drbd resource      dnf -y install drbd-pacemaker   Packages needed for gs2 filesystem (needs cluster lock control)          dnf -y install gfs2-utils lvm2-cluster dlm", 
            "title": "Install pacemaker packages"
        }, 
        {
            "location": "/setups/ha/active_active/#starting-and-configuring-cluster", 
            "text": "systemctl start pcsd\n    systemctl enable pcsd      passwd hacluster  Host name resolution must be set in /etc/hosts  vserver4:\n    echo  vserver4    /etc/hostname\n    echo  10.1.1.24   vserver4    /etc/hosts\n    echo  10.1.1.25   vserver5    /etc/hosts\n    exit  vserver5: \n\n    echo  10.1.1.24   vserver4    /etc/hosts\n    echo  10.1.1.25   vserver5    /etc/hosts\n    echo  vserver5    /etc/hostname\n    exit  In one node only!:      server1=vserver4\n    server2=vserver5\n    cl_name=vservers\n\n    pcs cluster auth $server1 $server2\n    pcs cluster setup --name $cl_name $server1 $server2\n    pcs cluster start --all\n    pcs status\n\n    [root@vserver4 ~]#     pcs status\n    Cluster name: vservers\n    WARNING: no stonith devices and stonith-enabled is not false\n    WARNING: corosync and pacemaker node names do not match (IPs used in setup?)\n    Last updated: Sun Oct 25 23:37:34 2015      Last change: \n    Stack: unknown\n    Current DC: NONE\n    0 nodes and 0 resources configured\n\n\n    Full list of resources:\n\n\n    PCSD Status:\n      vserver4 member (vserver4): Online\n      vserver5 member (vserver5): Online\n\n    Daemon Status:\n      corosync: active/disabled\n      pacemaker: active/disabled\n      pcsd: active/enabled  Check that cluster config is loaded as expected      [root@vserver4 ~]# pcs cluster cib |grep vserver\n     cib crm_feature_set= 3.0.10  validate-with= pacemaker-2.3  epoch= 5  num_updates= 8  admin_epoch= 0  cib-last-written= Sun Oct 25 23:52:24 2015  update-origin= vserver5  update-client= crmd  update-user= hacluster  have-quorum= 1  dc-uuid= 2 \n             nvpair id= cib-bootstrap-options-cluster-name  name= cluster-name  value= vservers / \n           node id= 1  uname= vserver4 / \n           node id= 2  uname= vserver5 / \n         node_state id= 2  uname= vserver5  in_ccm= true  crmd= online  crm-debug-origin= do_state_transition  join= member  expected= member \n         node_state id= 1  uname= vserver4  in_ccm= true  crmd= online  crm-debug-origin= do_state_transition  join= member  expected= member \n    [root@vserver4 ~]# grep vserver /etc/corosync/corosync.conf \n    cluster_name: vservers\n            ring0_addr: vserver4\n            ring0_addr: vserver5", 
            "title": "Starting and configuring cluster"
        }, 
        {
            "location": "/setups/ha/active_active/#fencing", 
            "text": "[root@vserver4 ~]# pcs stonith list \n    fence_apc - Fence agent for APC over telnet/ssh\n    fence_apc_snmp - Fence agent for APC, Tripplite PDU over SNMP      pcs stonith describe fence_apc_snmp  You can check if your stonith is reacheable and working:      fence_apc_snmp --ip=stonith1 --action=monitor\n    fence_apc_snmp --ip=stonith1 --action=monitor --community=escola2015\n    fence_apc_snmp --ip=stonith1 --action=reboot --plug=6  --community=escola2015 --power-wait=5  Configure stonith resources as ssh (discarded as it is too slow)      #pwd1=$(cat /root/pwd1)\n    #pwd2=$(cat /root/pwd2)\n\n    pcs stonith delete stonith1\n\n    pcs cluster cib stonith_cfg\n\n\n    pcs -f stonith_cfg stonith create stonith1 fence_apc ipaddr=10.1.1.3 login=vservers passwd=$pwd1 pcmk_host_list= vserver4 vserver5  pcmk_host_map= vserver4:4;vserver5:5 \n    #pcs -f stonith_cfg stonith create  stonith2 fence_apc ipaddr=10.1.1.3 login=vserver5 passwd=$pwd2 pcmk_host_list= vserver5  pcmk_host_map= vserver5:3 \n    pcs -f stonith_cfg property set stonith-enabled=false\n\n    pcs cluster cib-push stonith_cfg  Configure stonith resource as snmp (we use this one)      pcs stonith delete stonith1\n    pcs cluster cib stonith_cfg\n    pcs -f stonith_cfg stonith create stonith1 fence_apc_snmp params ipaddr=10.1.1.3 pcmk_host_list= vserver4,vserver5  pcmk_host_map= vserver4:4;verver5:5  pcmk_host_check=static-list power_wait=5\n    pcs cluster cib-push stonith_cfg  Activate stonith resource:      pcs property set stonith-enabled=true  Tests (warning, will reboot nodes!)          pcs cluster stop vserver5\n    stonith_admin --reboot vserver5      pcs cluster start --all    \n    pcs cluster stop vserver4\n    stonith_admin --reboot vserver4  While configuring cluster you may disable fencing:      pcs property set stonith-enabled=false  Check stonith resource definition      pcs stonith show --full  You'll find logs in:      tail -f /var/log/pacemaker.log", 
            "title": "Fencing"
        }, 
        {
            "location": "/setups/ha/active_active/#pacemaker-drbd", 
            "text": "echo drbd   /etc/modules-load.d/drbd.conf\n    pcs resource create drbd-vdisks ocf:linbit:drbd drbd_resource=vdisks op monitor interval=60s\n    pcs resource master drbd-vdisks-clone drbd-opt master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true", 
            "title": "PACEMAKER DRBD"
        }, 
        {
            "location": "/setups/ha/active_active/#dlm", 
            "text": "We need cluster locking for gfs2 filesystem      pcs cluster cib dlm_cfg\n    pcs -f dlm_cfg resource create dlm ocf:pacemaker:controld op monitor interval=60s\n    pcs -f dlm_cfg resource clone dlm clone-max=2 clone-node-max=1\n    pcs cluster cib-push dlm_cfg", 
            "title": "dlm"
        }, 
        {
            "location": "/setups/ha/active_active/#cluster-lvm", 
            "text": "", 
            "title": "Cluster lvm"
        }, 
        {
            "location": "/setups/ha/active_active/#set-up-cluster-lvms", 
            "text": "systemctl disable lvm2-lvmetad.service\n    systemctl disable lvm2-lvmetad.socket\n    systemctl stop lvm2-lvmetad.service\n\n    lvmconf --enable-cluster\n    reboot  You should define the devices where lvm will look for lvm signatures in\nfile /etc/lvm/lvm.conf:      filter = [ a|sd.*| ,  a|md.*| ,  a|drbd.*| ,  r|.*| ]", 
            "title": "Set up cluster lvms"
        }, 
        {
            "location": "/setups/ha/active_active/#set-up-cluster-lock-lvms", 
            "text": "pcs cluster cib clvmd_cfg\n    pcs -f clvmd_cfg resource create clvmd ocf:heartbeat:clvm params daemon_options= timeout=30s  op monitor interval=60s\n    pcs -f clvmd_cfg resource clone clvmd clone-max=2 clone-node-max=1\n    pcs cluster cib-push clvmd_cfg  Verify cluster status:        [root@vserver5 ~]# pcs status\n    Cluster name: vservers\n    WARNING: corosync and pacemaker node names do not match (IPs used in setup?)\n    Last updated: Thu Oct 29 13:14:58 2015      Last change: Thu Oct 29 13:14:42 2015 by root via cibadmin on vserver5\n    Stack: corosync\n    Current DC: vserver5 (version 1.1.13-3.fc22-44eb2dd) - partition with quorum\n    2 nodes and 7 resources configured\n\n    Online: [ vserver4 vserver5 ]\n\n    Full list of resources:\n\n     Master/Slave Set: drbd-vdisks-clone [drbd-vdisks]\n         Masters: [ vserver4 vserver5 ]\n     stonith1   (stonith:fence_apc_snmp):   Started vserver4\n     Clone Set: dlm-clone [dlm]\n         Started: [ vserver4 vserver5 ]\n     Clone Set: clvmd-clone [clvmd]\n         Started: [ vserver4 vserver5 ]\n\n    PCSD Status:\n      vserver4 member (vserver4): Online\n      vserver5 member (vserver5): Online\n\n    Daemon Status:\n      corosync: active/enabled\n      pacemaker: active/enabled\n      pcsd: active/enabled", 
            "title": "Set up cluster lock lvms"
        }, 
        {
            "location": "/setups/ha/active_active/#create-volumes", 
            "text": "In each server:      pvcreate /dev/drbd0  If we need to do cluster actions we should use -ci:      -cn ==  local action\n    -cy ==  cluster wide action  Now we can continue with cluster wide commands:      vgcreate -cy vgcluster /dev/drbd0  We can check on the other node if the vg was created:      [root@vserver5 ~]# vgs\n      VG        #PV #LV #SN Attr   VSize   VFree  \n      vg_data     2   1   0 wz--n- 558,79g   1,16g\n      vgcluster   1   0   0 wz--nc 465,62g 465,62g  We keep some free space just in case we want to do io tests:      lvcreate -l 97%FREE -n lvcluster1 vgcluster /dev/drbd0  And check it fromk the other server:          [root@vserver4 ~]# lvs\n      LV         VG        Attr       LSize   Pool            Origin          Data%  Meta%  Move Log Cpy%Sync Convert\n      lv_data    vg_data   Cwi-aoC--- 465,63g [lv_cache_data] [lv_data_corig] 0,00   0,82            0,00            \n      lvcluster1 vgcluster -wi-a----- 451,65g", 
            "title": "Create volumes"
        }, 
        {
            "location": "/setups/ha/active_active/#how-to-run-fencing-from-drbd-itself", 
            "text": "In 'handlers' section in /etc/drbd.d/global_common.conf:      fence-peer  /usr/lib/drbd/crm-fence-peer.sh ;\n    after-resync-target  /usr/lib/drbd/crm-unfence-peer.sh ;  In 'disk' section:      fencing resource-and-stonith;  This is the result:      [root@vserver4 ~]# cat /etc/drbd.d/global_common.conf \n    global {\n        usage-count yes;\n    }\n\n    common {\n        handlers {\n            split-brain  /usr/lib/drbd/notify-split-brain.sh root ;\n            fence-peer  /usr/lib/drbd/crm-fence-peer.sh ;\n            after-resync-target  /usr/lib/drbd/crm-unfence-peer.sh ;\n        }\n\n        startup {\n        }\n\n        options {\n        }\n\n        disk {\n            fencing resource-and-stonith;\n        }\n\n        net {\n            protocol C;\n\n            allow-two-primaries;\n            after-sb-0pri discard-zero-changes;\n            after-sb-1pri discard-secondary;\n            after-sb-2pri disconnect;\n        }\n    }  Check that pacemaker and stonith are working:      [root@vserver4 ~]# pcs property list\n    Cluster Properties:\n     cluster-infrastructure: corosync\n     cluster-name: vservers\n     dc-version: 1.1.13-3.fc22-44eb2dd\n     have-watchdog: false\n     stonith-enabled: true  Check again that stonith is enabled:      pcs property set stonith-enabled=true", 
            "title": "How to run fencing from drbd itself"
        }, 
        {
            "location": "/setups/ha/active_active/#constraints", 
            "text": "dlm and clvmd must be started in order:  pcs cluster cib cons/traints_cfg\npcs constraint order set drbd-vdisks-clone action=promote \\\nset dlm-clone clvmd-clone action=start \\\nsequential=true\npcs cluster cib-push constraints_cfg", 
            "title": "Constraints"
        }, 
        {
            "location": "/setups/ha/active_active/#drbd", 
            "text": "Sample config of drbd on gfs2 cluster.  yum install drbd drbd-utils drbd-udev drbd-pacemaker -y\nmodprobe drbd\nsystemctl enable drbd  We create drbd resources (/etc/drbd.d/...)  drbdadm create-md bases\ndrbdadm create-md templates\ndrbdadm create-dm grups\ndrbdadm up bases\ndrbdadm up templates\ndrbdadm up grups\ndrbdadm primary bases --force\ndrbdadm primary templates --force\ndrbdadm primary grups --force", 
            "title": "DRBD"
        }, 
        {
            "location": "/setups/ha/active_active/#dlm-and-clvm2-resources", 
            "text": "Clusteres lvms  pcs cluster cib locks_cfg\npcs -f locks_cfg resource create dlm ocf:pacemaker:controld op monitor interval=60s --group cluster_lock\npcs -f locks_cfg resource create clvmd ocf:heartbeat:clvm params daemon_options= timeout=30s  op monitor interval=60s  --group cluster_lock\npcs -f locks_cfg resource clone cluster_lock clone-max=2 clone-node-max=1 on-fail=restart \npcs cluster cib-push locks_cfg  Without clustered lvms  pcs resource create dlm ocf:pacemaker:controld op monitor interval=60s \npcs resource clone dlm clone-max=2 clone-node-max=1 on-fail=restart", 
            "title": "dlm and clvm2 resources"
        }, 
        {
            "location": "/setups/ha/active_active/#drbd-resources", 
            "text": "pcs cluster cib drbd_bases_cfg\npcs -f drbd_bases_cfg resource create drbd_bases ocf:linbit:drbd drbd_resource=bases op monitor interval=60s\npcs -f drbd_bases_cfg resource master drbd_bases-clone drbd_bases master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_bases_cfg\n\npcs cluster cib drbd_templates_cfg\npcs -f drbd_templates_cfg resource create drbd_templates ocf:linbit:drbd drbd_resource=templates op monitor interval=60s\npcs -f drbd_templates_cfg resource master drbd_templates-clone drbd_templates master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_templates_cfg\n\npcs cluster cib drbd_grups_cfg\npcs -f drbd_grups_cfg resource create drbd_grups ocf:linbit:drbd drbd_resource=grups op monitor interval=60s\npcs -f drbd_grups_cfg resource master drbd_grups-clone drbd_grups master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 notify=true\npcs cluster cib-push drbd_grups_cfg", 
            "title": "DRBD Resources"
        }, 
        {
            "location": "/setups/ha/active_active/#gfs2-filesystem", 
            "text": "mkfs.gfs2 -p lock_dlm -t vimet_cluster:bases -j 2 /dev/drbd10\nmkfs.gfs2 -p lock_dlm -t vimet_cluster:templates -j 2 /dev/drbd11\nmkfs.gfs2 -p lock_dlm -t vimet_cluster:grups -j 2 /dev/drbd30", 
            "title": "GFS2 filesystem"
        }, 
        {
            "location": "/setups/ha/active_active/#gfs2-resources", 
            "text": "pcs resource create gfs2_bases Filesystem device= /dev/drbd10  directory= /vimet/bases  fstype= gfs2   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1\npcs resource create gfs2_templates Filesystem device= /dev/drbd11  directory= /vimet/templates  fstype= gfs2   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1\npcs resource create gfs2_grups Filesystem device= /dev/drbd30  directory= /vimet/grups  fstype= gfs2   options=defaults,noatime,nodiratime,noquota  op monitor interval=10s on-fail=restart clone clone-max=2 clone-node-max=1", 
            "title": "GFS2 resources"
        }, 
        {
            "location": "/setups/ha/active_active/#nfs-4-server", 
            "text": "pcs cluster cib nfsserver_cfg\npcs -f nfsserver_cfg resource create nfs-daemon systemd:nfs-server \\\nnfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true \\\n--group nfs_server\npcs -f nfsserver_cfg resource create nfs-root exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash \\\ndirectory=/vimet \\\nfsid=0 \\\n--group nfs_server\npcs cluster cib-push nfsserver_cfg\npcs resource clone nfs_server master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0", 
            "title": "NFS 4 server"
        }, 
        {
            "location": "/setups/ha/active_active/#nfs-4-exports", 
            "text": "pcs cluster cib exports_cfg\npcs -f exports_cfg resource create nfs_bases exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/bases \\\nfsid=11 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\npcs -f exports_cfg resource create nfs_templates exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/templates \\\nfsid=21 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\n\npcs -f exports_cfg resource create nfs_grups exportfs \\\nclientspec=10.1.0.0/255.255.0.0 \\\noptions=rw,async,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash directory=/vimet/grups \\\nfsid=31 \\\nclone master-max=2 master-node-max=1 clone-max=2 clone-node-max=1 on-fail=restart notify=true resource-stickiness=0\npcs cluster cib-push exports_cfg", 
            "title": "NFS 4 exports"
        }, 
        {
            "location": "/setups/ha/active_active/#floatin-ips", 
            "text": "pcs resource create ClusterIPbases ocf:heartbeat:IPaddr2 ip=10.1.2.210 cidr_netmask=32 nic=nas:10 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPtemplates ocf:heartbeat:IPaddr2 ip=10.1.2.211 cidr_netmask=32 nic=nas:11 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPgrups ocf:heartbeat:IPaddr2 ip=10.1.2.212 cidr_netmask=32 nic=nas:30 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnasbases ocf:heartbeat:IPaddr2 ip=10.1.1.28 cidr_netmask=32 nic=nas:110 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnastemplates ocf:heartbeat:IPaddr2 ip=10.1.1.29 cidr_netmask=32 nic=nas:111 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0\npcs resource create ClusterIPcnasgrups ocf:heartbeat:IPaddr2 ip=10.1.1.30 cidr_netmask=32 nic=nas:130 clusterip_hash=sourceip-sourceport-destport meta resource-stickiness=0 op monitor interval=5 clone globally-unique=true clone-max=2 clone-node-max=2 on-fail=restart resource-stickiness=0", 
            "title": "Floatin IPs"
        }, 
        {
            "location": "/setups/ha/active_active/#constraints_1", 
            "text": "Start and stop order and restrictions  pcs constraint order \\\n    set stonith action=start \\\n    set cluster_lock-clone action=start \\\n    set nfs_server-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=serveis\n\npcs constraint order \\\n    set drbd_bases-clone action=promote role=Master \\\n    set gfs2_bases-clone \\\n    set nfs_bases-clone \\\n    set ClusterIPcnasbases-clone action=start \\\n    set ClusterIPbases-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=bases\n\npcs constraint order \\\n    set drbd_templates-clone action=promote role=Master \\\n    set gfs2_templates-clone \\\n    set nfs_templates-clone \\\n    set ClusterIPcnastemplates-clone action=start \\\n    set ClusterIPtemplates-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=templates\n\npcs constraint order \\\n    set drbd_grups-clone action=promote role=Master \\\n    set gfs2_grups-clone \\\n    set nfs_grups-clone \\\n    set ClusterIPcnasgrups-clone action=start \\\n    set ClusterIPgrups-clone action=start \\\n    require-all=true sequential=true \\\n    setoptions kind=Mandatory id=grups  Location constraints  pcs constraint colocation add \\\n    ClusterIPbases-clone with nfs_bases-clone INFINITY \\\n    id=colocate_bases\n\npcs constraint colocation add \\\n    ClusterIPtemplates-clone with nfs_templates-clone INFINITY \\\n    id=colocate_templates\n\npcs constraint colocation add \\\n    ClusterIPgrups-clone with nfs_grups-clone INFINITY \\\n    id=colocate_grups\n\npcs constraint colocation add \\\n    ClusterIPcnasbases-clone with nfs_bases-clone INFINITY \\\n    id=colocate_cnasbases\n\npcs constraint colocation add \\\n    ClusterIPcnastemplates-clone with nfs_templates-clone INFINITY \\\n    id=colocate_cnastemplates\n\npcs constraint colocation add \\\n    ClusterIPcnasgrups-clone with nfs_grups-clone INFINITY \\\n    id=colocate_cnasgrups", 
            "title": "Constraints"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/", 
            "text": "GPU SRIOV with a FirePro S7150 graphics card\n\n\nIn order to archieve a GPU SRIOV with a FirePro S7150 card, we'll need to have a \nkernel 4.4\n of an \nUbuntu 16.04 server\n. During the installation process, we'll need to make sure that the virtualization option is checked on.  \n\n\nTODO\n:\n- Check if it can be made with other distros or kernels\n\n\n1- Modify the BIOS\n\n\nThe first step is going to be modify the BIOS. We'll need to reset it to defaults, change the boot method to \nlegacy\n, activate all the parameters related with \nvirtualization\n and, finally, enable \nSRIOV\n. If the motherboard doesn't have this last option, the whole thing is not going to work. We'll have to change the \nmain graphics\n card to the \nonboard\n one, since the FirePro S7150 doesn't have a video output. (\nIntelRCSetup \n Miscellanious Configuration \n Active Video [Onboard Device]\n)\n\n\n2- Kernel compilation\n\n\nIn order to use the graphics cards and all its cores, we'll have to recompile the Linux Kernel, modifying some values and applying some paches.\n\n\n2.1- Activate the \ndeb-src\n apt repositories to be able to download the Kernel source\n\n\nvim /etc/apt/sources.list\n----\n\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n\n\n# newer versions of the distribution.\n\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial main restricted\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial main restricted\n\n\n## Major bug fix updates produced after the final release of the\n\n\n## distribution.\n\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates main restricted\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates main restricted\n\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n\n\n## team. Also, please note that software in universe WILL NOT receive any\n\n\n## review or updates from the Ubuntu security team.\n\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial universe\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial universe\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates universe\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates universe\n\n\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu \n\n\n## team, and may not be under a free licence. Please satisfy yourself as to \n\n\n## your rights to use the software. Also, please note that software in \n\n\n## multiverse WILL NOT receive any review or updates from the Ubuntu\n\n\n## security team.\n\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates multiverse\n\n\n## N.B. software from this repository may not have been tested as\n\n\n## extensively as that contained in the main release, although it includes\n\n\n## newer versions of some applications which may provide useful features.\n\n\n## Also, please note that software in backports WILL NOT receive any review\n\n\n## or updates from the Ubuntu security team.\n\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\n\n\n## Uncomment the following two lines to add software from Canonical\ns\n\n\n## \npartner\n repository.\n\n\n## This software is not part of Ubuntu, but is offered by Canonical and the\n\n\n## respective vendors as a service to Ubuntu users.\n\n\n# deb http://archive.canonical.com/ubuntu xenial partner\n\n\n# deb-src http://archive.canonical.com/ubuntu xenial partner\n\n\ndeb http://security.ubuntu.com/ubuntu xenial-security main restricted\ndeb-src http://security.ubuntu.com/ubuntu xenial-security main restricted\ndeb http://security.ubuntu.com/ubuntu xenial-security universe\ndeb-src http://security.ubuntu.com/ubuntu xenial-security universe\ndeb http://security.ubuntu.com/ubuntu xenial-security multiverse\ndeb-src http://security.ubuntu.com/ubuntu xenial-security multiverse\n----\napt update \n apt upgrade -y\nreboot \n# !!!! THIS REBOOT IS NEEDED !!!!\n\n\n\n\n\n\n2.2- Now we need to download the required packages to be able to unpack the kernel. We need to download the repo that contains the \ngim\n module sources and the paches to apply\n\n\napt install -y dpkg-dev\napt \nsource\n linux-image-\n$(\nuname -r\n)\n\ngit clone https://github.com/GPUOpen-LibrariesAndSDKs/MxGPU-Virtualization\n\n\n\n\n\n2.3- Now we install all the packages required to compile the Kernel\n\n\napt build-dep linux-image-\n$(\nuname -r\n)\n\napt install libncurses5 libncurses5-dev\n\n\n\n\n\n2.4- Kernel preparation and patch installation\n\n\ncd\n linux-4.4.0\nmake oldconfig\nmake menuconfig\n\n Enable loadable module support ---\n\n\n \n[\n \n]\n Module signature verification \n# This has to be unchecked. After that, we can save and exit\n\npatch \n ../MxGPU-Virtualization/patch/0001-Added-legacy-endpoint-type-to-sriov-for-ubuntu-4.4.0-75-generic.diff\nFile to patch: ./drivers/pci/iov.c \n# We need to specify the path of the file we want to patch\n\npatch \n ../MxGPU-Virtualization/patch/0002-add-pci-io-access-cap-for-ubuntu-4.4.0-75-generic.diff\nFile to patch: ./drivers/vfio/pci/vfio_pci_config.c \n# We need to specify the path of the file we want to patch\n\n\n\n\n\n\n2.5- Kernel compilation\n\n\nmake -j \nnumber of threads that the CPU has\n deb-pkg \nLOCALVERSION\n=\n-vm-firepro\n\n\n\n\n\n2.6- New Kernel installation\n\n\ncd\n ..\ndpkg -i *.deb\n\n\n\n\n\n3- GRUB preparation\n\n\n3.1- Add the module \namdgpu\n to the blacklist (we'll use the \ngim\n module)\n\n\nvim /etc/modprobe.d/blacklist.conf\n----\n...\n\n# AMD GPU (we\nll use the gim module)\n\nblacklist amdgpu\n----\nupdate-initramfs -u\n\n\n\n\n\n3.2- Edit the default GRUB configuration in order to  enable  IOMMU\n\n\nvim /etc/default/grub\n----\n...\n\nGRUB_CMDLINE_LINUX_DEFAULT\n=\nquiet intel_iommu=on\n\n...\n----\nupdate-grub\n\n\n\n\n\n4- \ngim\n module compilation\n\n\nThe \ngim\n module is the module that we're going to use in order to control the graphics card. \n\n\ncd\n MxGPU-Virtualization\n./gim.sh\nmkdir /lib/modules/\nfirepro version\n/misc\nmv /lib/modules/\nfirepro version/GIM /lib/modules/\nfirepro version\n/misc/gim\ndepmod\nnano /etc/modules \n# Make the module load on boot\n\n----\n...\ngim\n...\n----\n\n\n\n\n\nOnce it's compiled, we need to execute the following:\n\n\nreboot \n# To apply all the GRUB configuration\n\nmodprobe gim\nmodprobe -r gim\nnano /etc/gim_config \n# It needs to be nano, the vi command isn\nt going to work\n\n----\n...\n\nvf_num\n=\n16\n \n# This is going to activate the 16 cores of the GPU\n\n...\n----\nmodprobe gim\ndmesg \n# Finally, we need to check that the gim module has successfully been loaded, without errors\n\nlspci -vvv \n# We can now see that the FirePro S7150 has all the 16 cores activated", 
            "title": "GPU SRIOV"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#gpu-sriov-with-a-firepro-s7150-graphics-card", 
            "text": "In order to archieve a GPU SRIOV with a FirePro S7150 card, we'll need to have a  kernel 4.4  of an  Ubuntu 16.04 server . During the installation process, we'll need to make sure that the virtualization option is checked on.    TODO :\n- Check if it can be made with other distros or kernels", 
            "title": "GPU SRIOV with a FirePro S7150 graphics card"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#1-modify-the-bios", 
            "text": "The first step is going to be modify the BIOS. We'll need to reset it to defaults, change the boot method to  legacy , activate all the parameters related with  virtualization  and, finally, enable  SRIOV . If the motherboard doesn't have this last option, the whole thing is not going to work. We'll have to change the  main graphics  card to the  onboard  one, since the FirePro S7150 doesn't have a video output. ( IntelRCSetup   Miscellanious Configuration   Active Video [Onboard Device] )", 
            "title": "1- Modify the BIOS"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#2-kernel-compilation", 
            "text": "In order to use the graphics cards and all its cores, we'll have to recompile the Linux Kernel, modifying some values and applying some paches.", 
            "title": "2- Kernel compilation"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#21-activate-the-deb-src-apt-repositories-to-be-able-to-download-the-kernel-source", 
            "text": "vim /etc/apt/sources.list\n---- # See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to  # newer versions of the distribution. \ndeb http://es.archive.ubuntu.com/ubuntu/ xenial main restricted\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial main restricted ## Major bug fix updates produced after the final release of the  ## distribution. \ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates main restricted\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates main restricted ## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu  ## team. Also, please note that software in universe WILL NOT receive any  ## review or updates from the Ubuntu security team. \ndeb http://es.archive.ubuntu.com/ubuntu/ xenial universe\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial universe\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates universe\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates universe ## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu   ## team, and may not be under a free licence. Please satisfy yourself as to   ## your rights to use the software. Also, please note that software in   ## multiverse WILL NOT receive any review or updates from the Ubuntu  ## security team. \ndeb http://es.archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial multiverse\ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-updates multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-updates multiverse ## N.B. software from this repository may not have been tested as  ## extensively as that contained in the main release, although it includes  ## newer versions of some applications which may provide useful features.  ## Also, please note that software in backports WILL NOT receive any review  ## or updates from the Ubuntu security team. \ndeb http://es.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse\ndeb-src http://es.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse ## Uncomment the following two lines to add software from Canonical s  ##  partner  repository.  ## This software is not part of Ubuntu, but is offered by Canonical and the  ## respective vendors as a service to Ubuntu users.  # deb http://archive.canonical.com/ubuntu xenial partner  # deb-src http://archive.canonical.com/ubuntu xenial partner \n\ndeb http://security.ubuntu.com/ubuntu xenial-security main restricted\ndeb-src http://security.ubuntu.com/ubuntu xenial-security main restricted\ndeb http://security.ubuntu.com/ubuntu xenial-security universe\ndeb-src http://security.ubuntu.com/ubuntu xenial-security universe\ndeb http://security.ubuntu.com/ubuntu xenial-security multiverse\ndeb-src http://security.ubuntu.com/ubuntu xenial-security multiverse\n----\napt update   apt upgrade -y\nreboot  # !!!! THIS REBOOT IS NEEDED !!!!", 
            "title": "2.1- Activate the deb-src apt repositories to be able to download the Kernel source"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#22-now-we-need-to-download-the-required-packages-to-be-able-to-unpack-the-kernel-we-need-to-download-the-repo-that-contains-the-gim-module-sources-and-the-paches-to-apply", 
            "text": "apt install -y dpkg-dev\napt  source  linux-image- $( uname -r ) \ngit clone https://github.com/GPUOpen-LibrariesAndSDKs/MxGPU-Virtualization", 
            "title": "2.2- Now we need to download the required packages to be able to unpack the kernel. We need to download the repo that contains the gim module sources and the paches to apply"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#23-now-we-install-all-the-packages-required-to-compile-the-kernel", 
            "text": "apt build-dep linux-image- $( uname -r ) \napt install libncurses5 libncurses5-dev", 
            "title": "2.3- Now we install all the packages required to compile the Kernel"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#24-kernel-preparation-and-patch-installation", 
            "text": "cd  linux-4.4.0\nmake oldconfig\nmake menuconfig  Enable loadable module support ---    [   ]  Module signature verification  # This has to be unchecked. After that, we can save and exit \npatch   ../MxGPU-Virtualization/patch/0001-Added-legacy-endpoint-type-to-sriov-for-ubuntu-4.4.0-75-generic.diff\nFile to patch: ./drivers/pci/iov.c  # We need to specify the path of the file we want to patch \npatch   ../MxGPU-Virtualization/patch/0002-add-pci-io-access-cap-for-ubuntu-4.4.0-75-generic.diff\nFile to patch: ./drivers/vfio/pci/vfio_pci_config.c  # We need to specify the path of the file we want to patch", 
            "title": "2.4- Kernel preparation and patch installation"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#25-kernel-compilation", 
            "text": "make -j  number of threads that the CPU has  deb-pkg  LOCALVERSION = -vm-firepro", 
            "title": "2.5- Kernel compilation"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#26-new-kernel-installation", 
            "text": "cd  ..\ndpkg -i *.deb", 
            "title": "2.6- New Kernel installation"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#3-grub-preparation", 
            "text": "", 
            "title": "3- GRUB preparation"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#31-add-the-module-amdgpu-to-the-blacklist-well-use-the-gim-module", 
            "text": "vim /etc/modprobe.d/blacklist.conf\n----\n... # AMD GPU (we ll use the gim module) \nblacklist amdgpu\n----\nupdate-initramfs -u", 
            "title": "3.1- Add the module amdgpu to the blacklist (we'll use the gim module)"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#32-edit-the-default-grub-configuration-in-order-to-enable-iommu", 
            "text": "vim /etc/default/grub\n----\n... GRUB_CMDLINE_LINUX_DEFAULT = quiet intel_iommu=on \n...\n----\nupdate-grub", 
            "title": "3.2- Edit the default GRUB configuration in order to  enable  IOMMU"
        }, 
        {
            "location": "/setups/virtualization/gpu_sriov/#4-gim-module-compilation", 
            "text": "The  gim  module is the module that we're going to use in order to control the graphics card.   cd  MxGPU-Virtualization\n./gim.sh\nmkdir /lib/modules/ firepro version /misc\nmv /lib/modules/ firepro version/GIM /lib/modules/ firepro version /misc/gim\ndepmod\nnano /etc/modules  # Make the module load on boot \n----\n...\ngim\n...\n----  Once it's compiled, we need to execute the following:  reboot  # To apply all the GRUB configuration \nmodprobe gim\nmodprobe -r gim\nnano /etc/gim_config  # It needs to be nano, the vi command isn t going to work \n----\n... vf_num = 16   # This is going to activate the 16 cores of the GPU \n...\n----\nmodprobe gim\ndmesg  # Finally, we need to check that the gim module has successfully been loaded, without errors \nlspci -vvv  # We can now see that the FirePro S7150 has all the 16 cores activated", 
            "title": "4- gim module compilation"
        }, 
        {
            "location": "/utilities/monitoring/", 
            "text": "Monitoring utils\n\n\nglances\n\n\nProvides real time full overview of system load in a beautiful interface:\n\n\n\n\nCPU\n\n\nMemory\n\n\nNetwork\n\n\nDisks\n\n\n\n\ndstat\n\n\nReal time short overview of system load:\n\n\n\n\nCPU\n\n\nDisks\n\n\nNetwork\n\n\n\n\niftop\n\n\nReal time network throughput\n\n\niftop -i eth0\n\n\n\n\n\nhtop\n\n\nReal time overview of the system processes and load, all in a nice interface:\n\n\n\n\nRAM\n\n\nCPU\n\n\nProcesses", 
            "title": "Monitoring"
        }, 
        {
            "location": "/utilities/monitoring/#monitoring-utils", 
            "text": "", 
            "title": "Monitoring utils"
        }, 
        {
            "location": "/utilities/monitoring/#glances", 
            "text": "Provides real time full overview of system load in a beautiful interface:   CPU  Memory  Network  Disks", 
            "title": "glances"
        }, 
        {
            "location": "/utilities/monitoring/#dstat", 
            "text": "Real time short overview of system load:   CPU  Disks  Network", 
            "title": "dstat"
        }, 
        {
            "location": "/utilities/monitoring/#iftop", 
            "text": "Real time network throughput  iftop -i eth0", 
            "title": "iftop"
        }, 
        {
            "location": "/utilities/monitoring/#htop", 
            "text": "Real time overview of the system processes and load, all in a nice interface:   RAM  CPU  Processes", 
            "title": "htop"
        }, 
        {
            "location": "/utilities/grafana/", 
            "text": "Monitoring with Grafana\n\n\nGrafana is a web ui with configurable panels that can graph data send\nto a carbon service listening on 2003/tcp for key/value timestamps.\n\n\n\n\nCARBON: Recevies key/value timestamps and inserts on Whisper database.\n\n\nWHISPER: Key/Value database.\n\n\nGRAPHITE: Api that brings timed series from data stored in whisper.\n\n\nGRAFANA: Web UI that allows configuring graphs from graphite data series.\n\n\n\n\nSet up Grafana environment\n\n\nUsing docker-compose (https://docs.docker.com/compose/install/) you can\nstart a complete grafana setup:\n\n\ngit clone https://github.com/kamon-io/docker-grafana-graphite.git\ncd docker-grafana-graphite\ndocker-compose up -d\n\n\n\n\n\nYou can access grafana web ui on port 80 (user admin, password admin)\n\n\nCarbon client script\n\n\nIn order to monitor services, disk IO, etc... you need a client script\nthat will run on client machines and will send key/value timestamps to\nyour grafana host on port 2003/tcp.\n\n\nDon't forget to install dstat on your linux distro.\n\n\nHere you have an example script:\n\n\nimport\n \ntime\n\n\nimport\n \nsubprocess\n,\nos\n\n\nimport\n \nsocket\n\n\nimport\n \nplatform\n\n\nimport\n \nstruct\n \n\nimport\n \nre\n\n\n\nHOSTNAME\n                \n=\n \nplatform\n.\nnode\n()\n.\nsplit\n(\n.\n)[\n0\n]\n\n\nSLEEP_SECONDS\n           \n=\n \n1\n\n\nWRITEBOOST_CACHE_DEVICE\n \n=\n \n                \n# Or empty string \n\n\nDISKS\n                   \n=\n \nsda,nvme0n1\n        \n# Comma separated list of block devices or empty string \n\n\nNETS\n                    \n=\n \neth0,eth1\n         \n# Comma separated list of net devices or empty string \n\n\nCARBON_SERVER\n           \n=\n \nmygrafana.mydomain.com\n\n\nCARBON_PORT\n             \n=\n \n2003\n\n\n\n\n#### SYSTEM INFO PARSER\n\n\ndef\n \nget_sys\n():\n\n    \nsysdata\n \n=\n \n{}\n\n    \nwith\n \nopen\n(\n/proc/loadavg\n,\n \nr\n)\n \nas\n \nf\n:\n\n        \nfor\n \nline\n \nin\n \nf\n:\n\n            \nsplitLine\n \n=\n \nline\n.\nsplit\n()\n\n    \nsysdata\n[\nload\n]\n=\nsplitLine\n[\n0\n]\n\n    \nwith\n \nopen\n(\n/proc/meminfo\n,\n \nr\n)\n \nas\n \nf\n:\n\n        \ni\n=\n0\n\n        \nfor\n \nline\n \nin\n \nf\n:\n\n            \nsplitLine\n \n=\n \nline\n.\nsplit\n()\n\n            \nif\n \ni\n==\n0\n:\n \nsysdata\n[\nmemTotal\n]\n=\nsplitLine\n[\n1\n]\n\n            \nif\n \ni\n==\n1\n:\n \nsysdata\n[\nmemFree\n]\n=\nsplitLine\n[\n1\n]\n\n            \nif\n \ni\n==\n2\n:\n \nsysdata\n[\nmemAvail\n]\n=\nsplitLine\n[\n1\n]\n\n            \nif\n \ni\n==\n3\n:\n \nsysdata\n[\nmemCached\n]\n=\nsplitLine\n[\n1\n]\n\n            \nif\n \ni\n==\n3\n:\n \nbreak\n\n            \ni\n+=\n1\n\n    \nreturn\n \nsysdata\n\n\n\n#### DM-WRITEBOOST CACHE PARSER\n\n\ndef\n \nget_writeboost\n():\n\n        \nif\n \nWRITEBOOST_CACHE_DEVICE\n \n==\n \n:\n \nreturn\n \nFalse\n\n        \noutput\n=\nsubprocess\n.\ncheck_output\n([\ndmsetup status \n+\nWRITEBOOST_CACHE_DEVICE\n],\n \nshell\n=\nTrue\n)\n.\ndecode\n(\nutf-8\n)\n.\nsplit\n(\n\\n\n)[\n0\n]\n.\nsplit\n()\n\n        \ndata\n=\n{\ncursor_pos\n:\noutput\n[\n3\n],\n\n              \nnr_cache_blocks\n:\noutput\n[\n4\n],\n\n              \nnr_segments\n:\noutput\n[\n5\n],\n\n              \ncurrent_id\n:\noutput\n[\n6\n],\n\n              \nlast_flushed_id\n:\noutput\n[\n7\n],\n\n              \nlast_writeback_id\n:\noutput\n[\n8\n],\n\n              \nnr_dirty_blocks\n:\noutput\n[\n9\n],\n\n              \nnr_partial_flushed\n:\noutput\n[\n10\n]}\n\n        \nreturn\n \ndata\n\n\n\n\n\n#### NFS Stats\n\n\ndef\n \nget_nfs\n():\n\n        \nraw_lines\n=\nsubprocess\n.\ncheck_output\n([\nnfsstat\n,\n-l\n])\n.\nsplit\n(\n\\n\n)\n\n        \ndata\n=\n{}\n\n        \nfor\n \nline\n \nin\n \nraw_lines\n:\n\n          \ntry\n:\n\n                \nif\n \n-\n \nin\n \nline\n[\n1\n]:\n \ncontinue\n\n                \nline\n=\nchunkstring\n(\nline\n,\n13\n)\n\n                \ndata\n[\nline\n[\n1\n]\n.\nstrip\n()]\n \n=\n \nline\n[\n2\n]\n.\nreplace\n(\n:\n,\n)\n.\nstrip\n()\n\n          \nexcept\n:\n\n                \ncontinue\n\n        \nreturn\n \ndata\n\n\n\ndef\n \nchunkstring\n(\nstring\n,\n \nlength\n):\n\n    \nreturn\n \nlist\n(\nstring\n[\n0\n+\ni\n:\nlength\n+\ni\n]\n.\nstrip\n()\n \nfor\n \ni\n \nin\n \nrange\n(\n0\n,\n \nlen\n(\nstring\n),\n \nlength\n))\n\n\n\n\n#### DSTAT disk and net metrics    \n\n\ndef\n \nexecute\n(\ncmd\n):\n\n    \npopen\n \n=\n \nsubprocess\n.\nPopen\n(\ncmd\n,\n \nstdout\n=\nsubprocess\n.\nPIPE\n,\n \nuniversal_newlines\n=\nTrue\n)\n\n    \nfor\n \nstdout_line\n \nin\n \niter\n(\npopen\n.\nstdout\n.\nreadline\n,\n \n):\n\n        \nyield\n \nstdout_line\n \n    \npopen\n.\nstdout\n.\nclose\n()\n\n    \nreturn_code\n \n=\n \npopen\n.\nwait\n()\n\n    \nif\n \nreturn_code\n:\n\n        \nraise\n \nsubprocess\n.\nCalledProcessError\n(\nreturn_code\n,\n \ncmd\n)\n\n\n\ndef\n \ncu\n(\nvalue\n):\n\n    \ntry\n:\n\n        \nif\n \nB\n \nin\n \nvalue\n:\n \nreturn\n \nvalue\n.\nsplit\n(\nB\n)[\n0\n]\n\n        \nif\n \nk\n \nin\n \nvalue\n:\n\n            \nreturn\n \nstr\n(\n1024\n*\nint\n(\nvalue\n.\nsplit\n(\nk\n)[\n0\n]))\n\n        \nif\n \nM\n \nin\n \nvalue\n:\n\n            \nreturn\n \nstr\n(\n1048576\n*\nint\n(\nvalue\n.\nsplit\n(\nM\n)[\n0\n]))\n\n        \nif\n \nG\n \nin\n \nvalue\n:\n\n            \nreturn\n \nstr\n(\n1073741824\n*\nint\n(\nvalue\n.\nsplit\n(\nM\n)[\n0\n]))\n\n    \nexcept\n \nException\n \nas\n \ne\n:\n\n        \nreturn\n \n0\n\n    \nreturn\n \nvalue\n\n\n\ndef\n \nget_dstat\n(\nline\n):\n\n    \nline\n=\nline\n.\nstrip\n()\n\n    \nk1\n=\nline\n.\nsplit\n(\n|\n)\n\n    \ndata\n=\n{}\n\n    \ni\n=\n0\n\n    \nif\n \nDISKS\n \nis\n \nnot\n \n:\n\n        \ndisks_data\n=\nk1\n[\n0\n]\n.\nsplit\n(\n:\n)\n\n        \nfor\n \nd\n \nin\n \nDISKS\n.\nsplit\n(\n,\n):\n\n            \nvalues\n=\ndisks_data\n[\ni\n]\n.\nsplit\n()\n\n            \ndata\n[\ndisks.\n+\nd\n+\n.read\n]\n=\ncu\n(\nvalues\n[\n0\n])\n\n            \ndata\n[\ndisks.\n+\nd\n+\n.write\n]\n=\ncu\n(\nvalues\n[\n1\n])\n\n            \ni\n=\ni\n+\n1\n\n    \ni\n=\n0\n\n    \nif\n \nNETS\n \nis\n \nnot\n \n:\n\n        \nnets_data\n=\nk1\n[\nlen\n(\nk1\n)\n-\n1\n]\n.\nsplit\n(\n:\n)\n\n        \nfor\n \nn\n \nin\n \nNETS\n.\nsplit\n(\n,\n):\n\n            \nvalues\n=\nnets_data\n[\ni\n]\n.\nsplit\n()\n\n            \ndata\n[\nnets.\n+\nn\n+\n.recv\n]\n=\ncu\n(\nvalues\n[\n0\n])\n\n            \ndata\n[\nnets.\n+\nn\n+\n.send\n]\n=\ncu\n(\nvalues\n[\n1\n])\n\n            \ni\n=\ni\n+\n1\n\n    \nreturn\n \ndata\n\n\n\n#### \n\n\ndef\n \nsend_stats2carbon\n(\nsysdata\n,\ndstat\n,\ncache\n):\n\n    \nsock\n=\nFalse\n\n    \nwhile\n \nsock\n \nis\n \nFalse\n:\n\n        \ntry\n:\n\n            \nsock\n \n=\n \nsocket\n.\nsocket\n()\n\n            \nsock\n.\nconnect\n((\nCARBON_SERVER\n,\n \nCARBON_PORT\n))\n\n        \nexcept\n \nException\n \nas\n \ne\n:\n\n            \nprint\n(\nCarbon server \n+\nCARBON_SERVER\n+\n or carbon port \n+\nstr\n(\nCARBON_PORT\n)\n+\n not reacheable!\n)\n\n            \nsock\n=\nFalse\n\n            \ntime\n.\nsleep\n(\n5\n)\n\n    \ntimestamp\n \n=\n \nint\n(\ntime\n.\ntime\n())\n\n    \nmsg\n=\n\n    \nfor\n \nkey\n,\nvalue\n \nin\n \nsysdata\n.\nitems\n():\n\n        \nmsg\n+=\n%s\n.sysdata.\n%s\n \n%s\n \n%d\n\\n\n \n%\n \n(\nHOSTNAME\n,\n \nkey\n,\n \nvalue\n,\n \ntimestamp\n)\n\n    \nif\n \ndstat\n \nis\n \nnot\n \nFalse\n:\n\n        \nfor\n \nkey\n,\nvalue\n \nin\n \ndstat\n.\nitems\n():\n\n            \nmsg\n+=\n%s\n.dstat.\n%s\n \n%s\n \n%d\n\\n\n \n%\n \n(\nHOSTNAME\n,\n \nkey\n,\n \nvalue\n,\n \ntimestamp\n)\n\n    \nif\n \ncache\n \nis\n \nnot\n \nFalse\n:\n\n        \nif\n \ncache\n \nis\n \nnot\n \nFalse\n:\n\n            \nfor\n \nkey\n,\nvalue\n \nin\n \ncache\n.\nitems\n():\n\n                \nmsg\n+=\n%s\n.writeboost.\n%s\n \n%s\n \n%d\n\\n\n \n%\n \n(\nHOSTNAME\n,\n \nkey\n,\n \nvalue\n,\n \ntimestamp\n)\n\n    \nsock\n.\nsendall\n(\nmsg\n.\nencode\n(\nutf-8\n))\n\n    \nsock\n.\nclose\n()\n\n\n\ndef\n \nescape_ansi\n(\nline\n):\n\n    \nansi_escape\n \n=\n \nre\n.\ncompile\n(\nr\n(\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~]\n)\n\n    \nreturn\n \nansi_escape\n.\nsub\n(\n,\n \nline\n)\n\n\n\n\n## Main\n\n\nif\n \nDISKS\n \nis\n \nnot\n \n \nor\n \nNETS\n \nis\n \nnot\n \n:\n\n    \ncmd\n=\n[\ndstat\n,\n-dnD\n,\nDISKS\n,\n-N\n,\nNETS\n,\n--nocolor\n,\n--noheaders\n]\n\n    \nif\n \nDISKS\n \n==\n \n:\n \ncmd\n=\n[\ndstat\n,\n-nN\n,\nNETS\n,\n--nocolor\n,\n--noheaders\n]\n\n    \nif\n \nNETS\n  \n==\n \n:\n \ncmd\n=\n[\ndstat\n,\n-dD\n,\nDISKS\n,\n--nocolor\n,\n--noheaders\n]\n\n    \nfor\n \nline\n \nin\n \nexecute\n(\ncmd\n):\n\n        \nline\n=\nescape_ansi\n(\nline\n)\n\n        \nif\n \n-\n \nin\n \nline\n \nor\n \nread\n \nin\n \nline\n:\n \n            \ncontinue\n\n        \nsend_stats2carbon\n(\nget_sys\n(),\nget_dstat\n(\nline\n),\nget_writeboost\n())\n\n\n\nelse\n:\n\n    \nwhile\n \nTrue\n:\n\n        \nsend_stats2carbon\n(\nget_sys\n(),\nFalse\n,\nget_writeboost\n())\n\n\n\n\n\n\nCarbon client systemd service\n\n\nFirst try your carbon-client.py script and when it runs with your configured variables then you can install systemd service and enable it:\n\n\nvi /etc/systemd/system/carbon.service \n\n[\nUnit\n]\n\n\nDescription\n=\nCarbon Service\n\nAfter\n=\nmulti-user.target\n\n\n[\nService\n]\n\n\nType\n=\nidle\n\nExecStart\n=\n/usr/bin/python3 /opt/carbon/carbon-client.py\n\n\n[\nInstall\n]\n\n\nWantedBy\n=\nmulti-user.target\n\n\n\n\n\nsystemctl daemon-reload\nsystemctl start carbon\nsystemctl \nenable\n carbon", 
            "title": "Grafana"
        }, 
        {
            "location": "/utilities/grafana/#monitoring-with-grafana", 
            "text": "Grafana is a web ui with configurable panels that can graph data send\nto a carbon service listening on 2003/tcp for key/value timestamps.   CARBON: Recevies key/value timestamps and inserts on Whisper database.  WHISPER: Key/Value database.  GRAPHITE: Api that brings timed series from data stored in whisper.  GRAFANA: Web UI that allows configuring graphs from graphite data series.", 
            "title": "Monitoring with Grafana"
        }, 
        {
            "location": "/utilities/grafana/#set-up-grafana-environment", 
            "text": "Using docker-compose (https://docs.docker.com/compose/install/) you can\nstart a complete grafana setup:  git clone https://github.com/kamon-io/docker-grafana-graphite.git\ncd docker-grafana-graphite\ndocker-compose up -d  You can access grafana web ui on port 80 (user admin, password admin)", 
            "title": "Set up Grafana environment"
        }, 
        {
            "location": "/utilities/grafana/#carbon-client-script", 
            "text": "In order to monitor services, disk IO, etc... you need a client script\nthat will run on client machines and will send key/value timestamps to\nyour grafana host on port 2003/tcp.  Don't forget to install dstat on your linux distro.  Here you have an example script:  import   time  import   subprocess , os  import   socket  import   platform  import   struct   import   re  HOSTNAME                  =   platform . node () . split ( . )[ 0 ]  SLEEP_SECONDS             =   1  WRITEBOOST_CACHE_DEVICE   =                    # Or empty string   DISKS                     =   sda,nvme0n1          # Comma separated list of block devices or empty string   NETS                      =   eth0,eth1           # Comma separated list of net devices or empty string   CARBON_SERVER             =   mygrafana.mydomain.com  CARBON_PORT               =   2003  #### SYSTEM INFO PARSER  def   get_sys (): \n     sysdata   =   {} \n     with   open ( /proc/loadavg ,   r )   as   f : \n         for   line   in   f : \n             splitLine   =   line . split () \n     sysdata [ load ] = splitLine [ 0 ] \n     with   open ( /proc/meminfo ,   r )   as   f : \n         i = 0 \n         for   line   in   f : \n             splitLine   =   line . split () \n             if   i == 0 :   sysdata [ memTotal ] = splitLine [ 1 ] \n             if   i == 1 :   sysdata [ memFree ] = splitLine [ 1 ] \n             if   i == 2 :   sysdata [ memAvail ] = splitLine [ 1 ] \n             if   i == 3 :   sysdata [ memCached ] = splitLine [ 1 ] \n             if   i == 3 :   break \n             i += 1 \n     return   sysdata  #### DM-WRITEBOOST CACHE PARSER  def   get_writeboost (): \n         if   WRITEBOOST_CACHE_DEVICE   ==   :   return   False \n         output = subprocess . check_output ([ dmsetup status  + WRITEBOOST_CACHE_DEVICE ],   shell = True ) . decode ( utf-8 ) . split ( \\n )[ 0 ] . split () \n         data = { cursor_pos : output [ 3 ], \n               nr_cache_blocks : output [ 4 ], \n               nr_segments : output [ 5 ], \n               current_id : output [ 6 ], \n               last_flushed_id : output [ 7 ], \n               last_writeback_id : output [ 8 ], \n               nr_dirty_blocks : output [ 9 ], \n               nr_partial_flushed : output [ 10 ]} \n         return   data  #### NFS Stats  def   get_nfs (): \n         raw_lines = subprocess . check_output ([ nfsstat , -l ]) . split ( \\n ) \n         data = {} \n         for   line   in   raw_lines : \n           try : \n                 if   -   in   line [ 1 ]:   continue \n                 line = chunkstring ( line , 13 ) \n                 data [ line [ 1 ] . strip ()]   =   line [ 2 ] . replace ( : , ) . strip () \n           except : \n                 continue \n         return   data  def   chunkstring ( string ,   length ): \n     return   list ( string [ 0 + i : length + i ] . strip ()   for   i   in   range ( 0 ,   len ( string ),   length ))  #### DSTAT disk and net metrics      def   execute ( cmd ): \n     popen   =   subprocess . Popen ( cmd ,   stdout = subprocess . PIPE ,   universal_newlines = True ) \n     for   stdout_line   in   iter ( popen . stdout . readline ,   ): \n         yield   stdout_line  \n     popen . stdout . close () \n     return_code   =   popen . wait () \n     if   return_code : \n         raise   subprocess . CalledProcessError ( return_code ,   cmd )  def   cu ( value ): \n     try : \n         if   B   in   value :   return   value . split ( B )[ 0 ] \n         if   k   in   value : \n             return   str ( 1024 * int ( value . split ( k )[ 0 ])) \n         if   M   in   value : \n             return   str ( 1048576 * int ( value . split ( M )[ 0 ])) \n         if   G   in   value : \n             return   str ( 1073741824 * int ( value . split ( M )[ 0 ])) \n     except   Exception   as   e : \n         return   0 \n     return   value  def   get_dstat ( line ): \n     line = line . strip () \n     k1 = line . split ( | ) \n     data = {} \n     i = 0 \n     if   DISKS   is   not   : \n         disks_data = k1 [ 0 ] . split ( : ) \n         for   d   in   DISKS . split ( , ): \n             values = disks_data [ i ] . split () \n             data [ disks. + d + .read ] = cu ( values [ 0 ]) \n             data [ disks. + d + .write ] = cu ( values [ 1 ]) \n             i = i + 1 \n     i = 0 \n     if   NETS   is   not   : \n         nets_data = k1 [ len ( k1 ) - 1 ] . split ( : ) \n         for   n   in   NETS . split ( , ): \n             values = nets_data [ i ] . split () \n             data [ nets. + n + .recv ] = cu ( values [ 0 ]) \n             data [ nets. + n + .send ] = cu ( values [ 1 ]) \n             i = i + 1 \n     return   data  ####   def   send_stats2carbon ( sysdata , dstat , cache ): \n     sock = False \n     while   sock   is   False : \n         try : \n             sock   =   socket . socket () \n             sock . connect (( CARBON_SERVER ,   CARBON_PORT )) \n         except   Exception   as   e : \n             print ( Carbon server  + CARBON_SERVER +  or carbon port  + str ( CARBON_PORT ) +  not reacheable! ) \n             sock = False \n             time . sleep ( 5 ) \n     timestamp   =   int ( time . time ()) \n     msg = \n     for   key , value   in   sysdata . items (): \n         msg += %s .sysdata. %s   %s   %d \\n   %   ( HOSTNAME ,   key ,   value ,   timestamp ) \n     if   dstat   is   not   False : \n         for   key , value   in   dstat . items (): \n             msg += %s .dstat. %s   %s   %d \\n   %   ( HOSTNAME ,   key ,   value ,   timestamp ) \n     if   cache   is   not   False : \n         if   cache   is   not   False : \n             for   key , value   in   cache . items (): \n                 msg += %s .writeboost. %s   %s   %d \\n   %   ( HOSTNAME ,   key ,   value ,   timestamp ) \n     sock . sendall ( msg . encode ( utf-8 )) \n     sock . close ()  def   escape_ansi ( line ): \n     ansi_escape   =   re . compile ( r (\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~] ) \n     return   ansi_escape . sub ( ,   line )  ## Main  if   DISKS   is   not     or   NETS   is   not   : \n     cmd = [ dstat , -dnD , DISKS , -N , NETS , --nocolor , --noheaders ] \n     if   DISKS   ==   :   cmd = [ dstat , -nN , NETS , --nocolor , --noheaders ] \n     if   NETS    ==   :   cmd = [ dstat , -dD , DISKS , --nocolor , --noheaders ] \n     for   line   in   execute ( cmd ): \n         line = escape_ansi ( line ) \n         if   -   in   line   or   read   in   line :  \n             continue \n         send_stats2carbon ( get_sys (), get_dstat ( line ), get_writeboost ())  else : \n     while   True : \n         send_stats2carbon ( get_sys (), False , get_writeboost ())", 
            "title": "Carbon client script"
        }, 
        {
            "location": "/utilities/grafana/#carbon-client-systemd-service", 
            "text": "First try your carbon-client.py script and when it runs with your configured variables then you can install systemd service and enable it:  vi /etc/systemd/system/carbon.service  [ Unit ]  Description = Carbon Service After = multi-user.target [ Service ]  Type = idle ExecStart = /usr/bin/python3 /opt/carbon/carbon-client.py [ Install ]  WantedBy = multi-user.target  systemctl daemon-reload\nsystemctl start carbon\nsystemctl  enable  carbon", 
            "title": "Carbon client systemd service"
        }, 
        {
            "location": "/utilities/fio/", 
            "text": "fio\n\n\nUtility to test io performance on hard disks.\nhttp://fio.readthedocs.io/en/latest/fio_doc.html\n\n\nSample configuration\n\n\n[global]\ndirectory=/isard/2/groups/test\ndirect=1\nioengine=libaio\niodepth=32\nnumjobs=4\nsize=10G\nruntime=20\ngroup_reporting=1\nexec_prerun=echo 3 \n /proc/sys/vm/drop_caches\n\n[maxreadbw]\n# specs: 2.200MB/s real: 2.180MB/s\nbs=128k\nrw=read\nwait_for_previous=1\n\n[maxreadio]\n# 430.000 iops\nbs=4k\nrw=randread\nwait_for_previous=1\n\n[maxwritebw]\n# specs: 900MB/s real: 971MB/s\nbs=128k\nrw=write\nwait_for_previous=1\n\n[randwriteio]\n# 230.000 iops\nbs=4k\nrw=randwrite\nwait_for_previous=1\n\n[qcow2]\nbs=64k\nrw=randrw\nwait_for_previous=1\n\n\n\n\n\nAnd here you have a sample ouput and you can see that the results approximate to what the disk company says.\n\n\nmaxreadbw: (g=0): rw=read, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=32\n...\nmaxreadio: (g=1): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32\n...\nmaxwritebw: (g=2): rw=write, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=32\n...\nrandwriteio: (g=3): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32\n...\nqcow2: (g=4): rw=randrw, bs=64K-64K/64K-64K/64K-64K, ioengine=libaio, iodepth=32\n...\nfio-2.2.8\nStarting 20 processes\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt] [17.4K/0/0 iops] [eta 00m:20s]\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt /s] [424K/0/0 iops] [eta 00m:20s]\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txts] [0/7717/0 iops] [eta 00m:22s]        \nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt338.4MB/0KB /s] [0/86.7K/0 iops] [eta 01m:12s]\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nJobs: 4 (f=4): [_(16),m(4)] [90.2% done] [681.6MB/667.2MB/0KB /s] [10.1K/10.7K/0 iops] [eta 00m:11s]\nmaxreadbw: (groupid=0, jobs=4): err= 0: pid=11498: Sat Nov 14 17:41:50 2015\n  read : io=40960MB, bw=2177.2MB/s, iops=17416, runt= 18814msec\n    slat (usec): min=5, max=1158, avg=16.98, stdev= 5.34\n    clat (usec): min=313, max=18243, avg=7329.29, stdev=3387.42\n     lat (usec): min=328, max=18259, avg=7346.41, stdev=3387.40\n    clat percentiles (usec):\n     |  1.00th=[  700],  5.00th=[ 1320], 10.00th=[ 2352], 20.00th=[ 4576],\n     | 30.00th=[ 5664], 40.00th=[ 6624], 50.00th=[ 7328], 60.00th=[ 7968],\n     | 70.00th=[ 8896], 80.00th=[10176], 90.00th=[12352], 95.00th=[13376],\n     | 99.00th=[14144], 99.50th=[14400], 99.90th=[14912], 99.95th=[15040],\n     | 99.99th=[15552]\n    bw (KB  /s): min=548864, max=570112, per=25.00%, avg=557278.53, stdev=3994.50\n    lat (usec) : 500=0.18%, 750=1.12%, 1000=1.60%\n    lat (msec) : 2=5.68%, 4=8.45%, 10=62.19%, 20=20.78%\n  cpu          : usr=1.03%, sys=10.54%, ctx=294128, majf=0, minf=4171\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, \n=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \n=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \n=64=0.0%\n     issued    : total=r=327680/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nmaxreadio: (groupid=1, jobs=4): err= 0: pid=11523: Sat Nov 14 17:41:50 2015\n  read : io=29406MB, bw=1470.3MB/s, iops=376374, runt= 20001msec\n    slat (usec): min=1, max=1136, avg= 2.61, stdev= 2.80\n    clat (usec): min=39, max=3137, avg=336.87, stdev=208.64\n     lat (usec): min=42, max=3139, avg=339.54, stdev=208.70\n    clat percentiles (usec):\n     |  1.00th=[   78],  5.00th=[   92], 10.00th=[  102], 20.00th=[  126],\n     | 30.00th=[  171], 40.00th=[  274], 50.00th=[  342], 60.00th=[  378],\n     | 70.00th=[  418], 80.00th=[  474], 90.00th=[  588], 95.00th=[  708],\n     | 99.00th=[ 1012], 99.50th=[ 1144], 99.90th=[ 1432], 99.95th=[ 1560],\n     | 99.99th=[ 2064]\n    bw (KB  /s): min=324544, max=497128, per=25.00%, avg=376387.60, stdev=38308.47\n    lat (usec) : 50=0.01%, 100=8.77%, 250=29.14%, 500=45.15%, 750=12.98%\n    lat (usec) : 1000=2.90%\n    lat (msec) : 2=1.04%, 4=0.01%\n  cpu          : usr=10.42%, sys=36.06%, ctx=2972500, majf=0, minf=527\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, \n=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \n=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \n=64=0.0%\n     issued    : total=r=7527872/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nmaxwritebw: (groupid=2, jobs=4): err= 0: pid=11531: Sat Nov 14 17:41:50 2015\n  write: io=19572MB, bw=976.99MB/s, iops=7815, runt= 20033msec\n    slat (usec): min=9, max=1220, avg=38.16, stdev=11.93\n    clat (usec): min=34, max=64684, avg=16334.02, stdev=12762.63\n     lat (usec): min=68, max=64734, avg=16372.51, stdev=12762.33\n    clat percentiles (usec):\n     |  1.00th=[   66],  5.00th=[   96], 10.00th=[  310], 20.00th=[ 2160],\n     | 30.00th=[ 4832], 40.00th=[ 7392], 50.00th=[15680], 60.00th=[24960],\n     | 70.00th=[27776], 80.00th=[30336], 90.00th=[32384], 95.00th=[33024],\n     | 99.00th=[34560], 99.50th=[35072], 99.90th=[37632], 99.95th=[45824],\n     | 99.99th=[59136]\n    bw (KB  /s): min=240629, max=259321, per=25.00%, avg=250112.90, stdev=4088.96\n    lat (usec) : 50=0.02%, 100=5.62%, 250=4.00%, 500=2.33%, 750=1.79%\n    lat (usec) : 1000=1.47%\n    lat (msec) : 2=4.08%, 4=7.25%, 10=17.73%, 20=8.86%, 50=46.82%\n    lat (msec) : 100=0.03%\n  cpu          : usr=3.13%, sys=8.95%, ctx=152485, majf=0, minf=4170\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=99.9%, \n=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \n=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \n=64=0.0%\n     issued    : total=r=0/w=156572/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nrandwriteio: (groupid=3, jobs=4): err= 0: pid=11617: Sat Nov 14 17:41:50 2015\n  write: io=9133.5MB, bw=467610KB/s, iops=116902, runt= 20001msec\n    slat (usec): min=3, max=55410, avg=12.25, stdev=415.43\n    clat (usec): min=7, max=69546, avg=1081.96, stdev=3366.40\n     lat (usec): min=13, max=69556, avg=1094.26, stdev=3392.60\n    clat percentiles (usec):\n     |  1.00th=[   22],  5.00th=[  564], 10.00th=[  604], 20.00th=[  684],\n     | 30.00th=[  740], 40.00th=[  788], 50.00th=[  844], 60.00th=[  892],\n     | 70.00th=[  932], 80.00th=[  964], 90.00th=[ 1004], 95.00th=[ 1032],\n     | 99.00th=[ 1256], 99.50th=[40192], 99.90th=[46848], 99.95th=[48384],\n     | 99.99th=[58112]\n    bw (KB  /s): min=76648, max=228728, per=25.14%, avg=117577.33, stdev=42200.75\n    lat (usec) : 10=0.01%, 20=0.84%, 50=0.75%, 100=0.07%, 250=0.13%\n    lat (usec) : 500=0.60%, 750=29.72%, 1000=57.44%\n    lat (msec) : 2=9.59%, 4=0.12%, 10=0.13%, 20=0.01%, 50=0.55%\n    lat (msec) : 100=0.04%\n  cpu          : usr=3.77%, sys=29.50%, ctx=1820945, majf=0, minf=531\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, \n=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \n=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \n=64=0.0%\n     issued    : total=r=0/w=2338168/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nqcow2: (groupid=4, jobs=4): err= 0: pid=11724: Sat Nov 14 17:41:50 2015\n  read : io=13225MB, bw=676766KB/s, iops=10574, runt= 20010msec\n    slat (usec): min=3, max=10750, avg=12.63, stdev=23.91\n    clat (usec): min=200, max=52176, avg=6612.80, stdev=4262.64\n     lat (usec): min=207, max=52180, avg=6625.59, stdev=4262.60\n    clat percentiles (usec):\n     |  1.00th=[  338],  5.00th=[  692], 10.00th=[ 1128], 20.00th=[ 2024],\n     | 30.00th=[ 2896], 40.00th=[ 4448], 50.00th=[ 6688], 60.00th=[ 8896],\n     | 70.00th=[10176], 80.00th=[10944], 90.00th=[11840], 95.00th=[12480],\n     | 99.00th=[13632], 99.50th=[14528], 99.90th=[17280], 99.95th=[26496],\n     | 99.99th=[47360]\n    bw (KB  /s): min=125952, max=185216, per=25.00%, avg=169196.04, stdev=7218.28\n  write: io=13185MB, bw=674735KB/s, iops=10542, runt= 20010msec\n    slat (usec): min=4, max=11507, avg=15.31, stdev=42.32\n    clat (usec): min=18, max=49475, avg=5476.05, stdev=4190.25\n     lat (usec): min=34, max=56137, avg=5491.52, stdev=4191.28\n    clat percentiles (usec):\n     |  1.00th=[   35],  5.00th=[   46], 10.00th=[  143], 20.00th=[  700],\n     | 30.00th=[ 1592], 40.00th=[ 3248], 50.00th=[ 5600], 60.00th=[ 7840],\n     | 70.00th=[ 9152], 80.00th=[ 9920], 90.00th=[10560], 95.00th=[10944],\n     | 99.00th=[12096], 99.50th=[12864], 99.90th=[15808], 99.95th=[22144],\n     | 99.99th=[42752]\n    bw (KB  /s): min=126208, max=179072, per=25.00%, avg=168713.49, stdev=5997.84\n    lat (usec) : 20=0.01%, 50=2.82%, 100=1.68%, 250=1.74%, 500=3.59%\n    lat (usec) : 750=3.32%, 1000=3.05%\n    lat (msec) : 2=10.18%, 4=14.19%, 10=33.94%, 20=25.43%, 50=0.06%\n    lat (msec) : 100=0.01%\n  cpu          : usr=2.70%, sys=9.85%, ctx=368799, majf=0, minf=2143\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, \n=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \n=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, \n=64=0.0%\n     issued    : total=r=211595/w=210960/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\n\nRun status group 0 (all jobs):\n   READ: io=40960MB, aggrb=2177.2MB/s, minb=2177.2MB/s, maxb=2177.2MB/s, mint=18814msec, maxt=18814msec\n\nRun status group 1 (all jobs):\n   READ: io=29406MB, aggrb=1470.3MB/s, minb=1470.3MB/s, maxb=1470.3MB/s, mint=20001msec, maxt=20001msec\n\nRun status group 2 (all jobs):\n  WRITE: io=19572MB, aggrb=976.99MB/s, minb=976.99MB/s, maxb=976.99MB/s, mint=20033msec, maxt=20033msec\n\nRun status group 3 (all jobs):\n  WRITE: io=9133.5MB, aggrb=467610KB/s, minb=467610KB/s, maxb=467610KB/s, mint=20001msec, maxt=20001msec\n\nRun status group 4 (all jobs):\n   READ: io=13225MB, aggrb=676765KB/s, minb=676765KB/s, maxb=676765KB/s, mint=20010msec, maxt=20010msec\n  WRITE: io=13185MB, aggrb=674734KB/s, minb=674734KB/s, maxb=674734KB/s, mint=20010msec, maxt=20010msec\n\nDisk stats (read/write):\n  nvme0n1: ios=8065889/3774940, merge=0/0, ticks=6274124/10488887, in_queue=16780920, util=95.98%", 
            "title": "fio"
        }, 
        {
            "location": "/utilities/fio/#fio", 
            "text": "Utility to test io performance on hard disks.\nhttp://fio.readthedocs.io/en/latest/fio_doc.html", 
            "title": "fio"
        }, 
        {
            "location": "/utilities/fio/#sample-configuration", 
            "text": "[global]\ndirectory=/isard/2/groups/test\ndirect=1\nioengine=libaio\niodepth=32\nnumjobs=4\nsize=10G\nruntime=20\ngroup_reporting=1\nexec_prerun=echo 3   /proc/sys/vm/drop_caches\n\n[maxreadbw]\n# specs: 2.200MB/s real: 2.180MB/s\nbs=128k\nrw=read\nwait_for_previous=1\n\n[maxreadio]\n# 430.000 iops\nbs=4k\nrw=randread\nwait_for_previous=1\n\n[maxwritebw]\n# specs: 900MB/s real: 971MB/s\nbs=128k\nrw=write\nwait_for_previous=1\n\n[randwriteio]\n# 230.000 iops\nbs=4k\nrw=randwrite\nwait_for_previous=1\n\n[qcow2]\nbs=64k\nrw=randrw\nwait_for_previous=1  And here you have a sample ouput and you can see that the results approximate to what the disk company says.  maxreadbw: (g=0): rw=read, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=32\n...\nmaxreadio: (g=1): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32\n...\nmaxwritebw: (g=2): rw=write, bs=128K-128K/128K-128K/128K-128K, ioengine=libaio, iodepth=32\n...\nrandwriteio: (g=3): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=32\n...\nqcow2: (g=4): rw=randrw, bs=64K-64K/64K-64K/64K-64K, ioengine=libaio, iodepth=32\n...\nfio-2.2.8\nStarting 20 processes\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadio: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxwritebw: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nrandwriteio: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nqcow2: Laying out IO file(s) (1 file(s) / 10240MB)\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadbw : Saving output of prerun in maxreadbw.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt] [17.4K/0/0 iops] [eta 00m:20s]\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxreadio : Saving output of prerun in maxreadio.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt /s] [424K/0/0 iops] [eta 00m:20s]\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nmaxwritebw : Saving output of prerun in maxwritebw.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txts] [0/7717/0 iops] [eta 00m:22s]        \nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nrandwriteio : Saving output of prerun in randwriteio.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt338.4MB/0KB /s] [0/86.7K/0 iops] [eta 01m:12s]\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nqcow2 : Saving output of prerun in qcow2.prerun.txt\nJobs: 4 (f=4): [_(16),m(4)] [90.2% done] [681.6MB/667.2MB/0KB /s] [10.1K/10.7K/0 iops] [eta 00m:11s]\nmaxreadbw: (groupid=0, jobs=4): err= 0: pid=11498: Sat Nov 14 17:41:50 2015\n  read : io=40960MB, bw=2177.2MB/s, iops=17416, runt= 18814msec\n    slat (usec): min=5, max=1158, avg=16.98, stdev= 5.34\n    clat (usec): min=313, max=18243, avg=7329.29, stdev=3387.42\n     lat (usec): min=328, max=18259, avg=7346.41, stdev=3387.40\n    clat percentiles (usec):\n     |  1.00th=[  700],  5.00th=[ 1320], 10.00th=[ 2352], 20.00th=[ 4576],\n     | 30.00th=[ 5664], 40.00th=[ 6624], 50.00th=[ 7328], 60.00th=[ 7968],\n     | 70.00th=[ 8896], 80.00th=[10176], 90.00th=[12352], 95.00th=[13376],\n     | 99.00th=[14144], 99.50th=[14400], 99.90th=[14912], 99.95th=[15040],\n     | 99.99th=[15552]\n    bw (KB  /s): min=548864, max=570112, per=25.00%, avg=557278.53, stdev=3994.50\n    lat (usec) : 500=0.18%, 750=1.12%, 1000=1.60%\n    lat (msec) : 2=5.68%, 4=8.45%, 10=62.19%, 20=20.78%\n  cpu          : usr=1.03%, sys=10.54%, ctx=294128, majf=0, minf=4171\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%,  =64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%,  =64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%,  =64=0.0%\n     issued    : total=r=327680/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nmaxreadio: (groupid=1, jobs=4): err= 0: pid=11523: Sat Nov 14 17:41:50 2015\n  read : io=29406MB, bw=1470.3MB/s, iops=376374, runt= 20001msec\n    slat (usec): min=1, max=1136, avg= 2.61, stdev= 2.80\n    clat (usec): min=39, max=3137, avg=336.87, stdev=208.64\n     lat (usec): min=42, max=3139, avg=339.54, stdev=208.70\n    clat percentiles (usec):\n     |  1.00th=[   78],  5.00th=[   92], 10.00th=[  102], 20.00th=[  126],\n     | 30.00th=[  171], 40.00th=[  274], 50.00th=[  342], 60.00th=[  378],\n     | 70.00th=[  418], 80.00th=[  474], 90.00th=[  588], 95.00th=[  708],\n     | 99.00th=[ 1012], 99.50th=[ 1144], 99.90th=[ 1432], 99.95th=[ 1560],\n     | 99.99th=[ 2064]\n    bw (KB  /s): min=324544, max=497128, per=25.00%, avg=376387.60, stdev=38308.47\n    lat (usec) : 50=0.01%, 100=8.77%, 250=29.14%, 500=45.15%, 750=12.98%\n    lat (usec) : 1000=2.90%\n    lat (msec) : 2=1.04%, 4=0.01%\n  cpu          : usr=10.42%, sys=36.06%, ctx=2972500, majf=0, minf=527\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%,  =64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%,  =64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%,  =64=0.0%\n     issued    : total=r=7527872/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nmaxwritebw: (groupid=2, jobs=4): err= 0: pid=11531: Sat Nov 14 17:41:50 2015\n  write: io=19572MB, bw=976.99MB/s, iops=7815, runt= 20033msec\n    slat (usec): min=9, max=1220, avg=38.16, stdev=11.93\n    clat (usec): min=34, max=64684, avg=16334.02, stdev=12762.63\n     lat (usec): min=68, max=64734, avg=16372.51, stdev=12762.33\n    clat percentiles (usec):\n     |  1.00th=[   66],  5.00th=[   96], 10.00th=[  310], 20.00th=[ 2160],\n     | 30.00th=[ 4832], 40.00th=[ 7392], 50.00th=[15680], 60.00th=[24960],\n     | 70.00th=[27776], 80.00th=[30336], 90.00th=[32384], 95.00th=[33024],\n     | 99.00th=[34560], 99.50th=[35072], 99.90th=[37632], 99.95th=[45824],\n     | 99.99th=[59136]\n    bw (KB  /s): min=240629, max=259321, per=25.00%, avg=250112.90, stdev=4088.96\n    lat (usec) : 50=0.02%, 100=5.62%, 250=4.00%, 500=2.33%, 750=1.79%\n    lat (usec) : 1000=1.47%\n    lat (msec) : 2=4.08%, 4=7.25%, 10=17.73%, 20=8.86%, 50=46.82%\n    lat (msec) : 100=0.03%\n  cpu          : usr=3.13%, sys=8.95%, ctx=152485, majf=0, minf=4170\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=99.9%,  =64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%,  =64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%,  =64=0.0%\n     issued    : total=r=0/w=156572/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nrandwriteio: (groupid=3, jobs=4): err= 0: pid=11617: Sat Nov 14 17:41:50 2015\n  write: io=9133.5MB, bw=467610KB/s, iops=116902, runt= 20001msec\n    slat (usec): min=3, max=55410, avg=12.25, stdev=415.43\n    clat (usec): min=7, max=69546, avg=1081.96, stdev=3366.40\n     lat (usec): min=13, max=69556, avg=1094.26, stdev=3392.60\n    clat percentiles (usec):\n     |  1.00th=[   22],  5.00th=[  564], 10.00th=[  604], 20.00th=[  684],\n     | 30.00th=[  740], 40.00th=[  788], 50.00th=[  844], 60.00th=[  892],\n     | 70.00th=[  932], 80.00th=[  964], 90.00th=[ 1004], 95.00th=[ 1032],\n     | 99.00th=[ 1256], 99.50th=[40192], 99.90th=[46848], 99.95th=[48384],\n     | 99.99th=[58112]\n    bw (KB  /s): min=76648, max=228728, per=25.14%, avg=117577.33, stdev=42200.75\n    lat (usec) : 10=0.01%, 20=0.84%, 50=0.75%, 100=0.07%, 250=0.13%\n    lat (usec) : 500=0.60%, 750=29.72%, 1000=57.44%\n    lat (msec) : 2=9.59%, 4=0.12%, 10=0.13%, 20=0.01%, 50=0.55%\n    lat (msec) : 100=0.04%\n  cpu          : usr=3.77%, sys=29.50%, ctx=1820945, majf=0, minf=531\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%,  =64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%,  =64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%,  =64=0.0%\n     issued    : total=r=0/w=2338168/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\nqcow2: (groupid=4, jobs=4): err= 0: pid=11724: Sat Nov 14 17:41:50 2015\n  read : io=13225MB, bw=676766KB/s, iops=10574, runt= 20010msec\n    slat (usec): min=3, max=10750, avg=12.63, stdev=23.91\n    clat (usec): min=200, max=52176, avg=6612.80, stdev=4262.64\n     lat (usec): min=207, max=52180, avg=6625.59, stdev=4262.60\n    clat percentiles (usec):\n     |  1.00th=[  338],  5.00th=[  692], 10.00th=[ 1128], 20.00th=[ 2024],\n     | 30.00th=[ 2896], 40.00th=[ 4448], 50.00th=[ 6688], 60.00th=[ 8896],\n     | 70.00th=[10176], 80.00th=[10944], 90.00th=[11840], 95.00th=[12480],\n     | 99.00th=[13632], 99.50th=[14528], 99.90th=[17280], 99.95th=[26496],\n     | 99.99th=[47360]\n    bw (KB  /s): min=125952, max=185216, per=25.00%, avg=169196.04, stdev=7218.28\n  write: io=13185MB, bw=674735KB/s, iops=10542, runt= 20010msec\n    slat (usec): min=4, max=11507, avg=15.31, stdev=42.32\n    clat (usec): min=18, max=49475, avg=5476.05, stdev=4190.25\n     lat (usec): min=34, max=56137, avg=5491.52, stdev=4191.28\n    clat percentiles (usec):\n     |  1.00th=[   35],  5.00th=[   46], 10.00th=[  143], 20.00th=[  700],\n     | 30.00th=[ 1592], 40.00th=[ 3248], 50.00th=[ 5600], 60.00th=[ 7840],\n     | 70.00th=[ 9152], 80.00th=[ 9920], 90.00th=[10560], 95.00th=[10944],\n     | 99.00th=[12096], 99.50th=[12864], 99.90th=[15808], 99.95th=[22144],\n     | 99.99th=[42752]\n    bw (KB  /s): min=126208, max=179072, per=25.00%, avg=168713.49, stdev=5997.84\n    lat (usec) : 20=0.01%, 50=2.82%, 100=1.68%, 250=1.74%, 500=3.59%\n    lat (usec) : 750=3.32%, 1000=3.05%\n    lat (msec) : 2=10.18%, 4=14.19%, 10=33.94%, 20=25.43%, 50=0.06%\n    lat (msec) : 100=0.01%\n  cpu          : usr=2.70%, sys=9.85%, ctx=368799, majf=0, minf=2143\n  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%,  =64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%,  =64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%,  =64=0.0%\n     issued    : total=r=211595/w=210960/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=32\n\nRun status group 0 (all jobs):\n   READ: io=40960MB, aggrb=2177.2MB/s, minb=2177.2MB/s, maxb=2177.2MB/s, mint=18814msec, maxt=18814msec\n\nRun status group 1 (all jobs):\n   READ: io=29406MB, aggrb=1470.3MB/s, minb=1470.3MB/s, maxb=1470.3MB/s, mint=20001msec, maxt=20001msec\n\nRun status group 2 (all jobs):\n  WRITE: io=19572MB, aggrb=976.99MB/s, minb=976.99MB/s, maxb=976.99MB/s, mint=20033msec, maxt=20033msec\n\nRun status group 3 (all jobs):\n  WRITE: io=9133.5MB, aggrb=467610KB/s, minb=467610KB/s, maxb=467610KB/s, mint=20001msec, maxt=20001msec\n\nRun status group 4 (all jobs):\n   READ: io=13225MB, aggrb=676765KB/s, minb=676765KB/s, maxb=676765KB/s, mint=20010msec, maxt=20010msec\n  WRITE: io=13185MB, aggrb=674734KB/s, minb=674734KB/s, maxb=674734KB/s, mint=20010msec, maxt=20010msec\n\nDisk stats (read/write):\n  nvme0n1: ios=8065889/3774940, merge=0/0, ticks=6274124/10488887, in_queue=16780920, util=95.98%", 
            "title": "Sample configuration"
        }, 
        {
            "location": "/about/license/", 
            "text": "", 
            "title": "About"
        }
    ]
}